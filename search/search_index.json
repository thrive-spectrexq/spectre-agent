{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models"},{"location":"#introduction","title":"Introduction","text":"<p>Youtu-Agent is a powerful and modular framework for building, running, and evaluating autonomous agents. It is designed with flexibility and extensibility in mind, allowing developers to easily create custom agents, tools, and environments.</p> <p>The framework's design is centered on a clear separation of concerns, enabling robust and scalable agent development.</p> <p>Note: Youtu-Agent is abbreviated as <code>utu</code> in the code.</p>"},{"location":"#core-architecture","title":"Core Architecture","text":"<p>At a high level, the framework's components interact as follows: An <code>AgentConfig</code> defines an <code>Agent</code>, which operates within an <code>Environment</code> and uses <code>Toolkits</code> to perform actions. The entire system can be benchmarked using the <code>Evaluation Framework</code>.</p> <pre><code>graph TD\n    subgraph Configuration\n        A[AgentConfig] --&gt;|defines| B(Agent);\n    end\n\n    subgraph Execution\n        B --&gt;|uses| C[Toolkit];\n        B --&gt;|operates in| D[Environment];\n    end\n\n    subgraph Benchmarking\n        E[Evaluation Framework] --&gt;|evaluates| B;\n    end</code></pre>"},{"location":"#key-modules","title":"Key Modules","text":"<p>The framework is divided into several key modules, each with a distinct responsibility.</p>"},{"location":"#configuration-configmd","title":"Configuration (<code>config.md</code>)","text":"<p>The entire framework is driven by a configuration system built on <code>pydantic</code> and <code>hydra</code>. It uses YAML files to define the behavior of agents, experiments, and their components.</p>"},{"location":"#agent-paradigms-agentsmd","title":"Agent Paradigms (<code>agents.md</code>)","text":"<p>The core logic of the agent. The framework supports two primary paradigms: - <code>SimpleAgent</code>: A classic single-agent model that reasons and acts in a loop (ReAct-style). - <code>OrchestraAgent</code>: A multi-agent system that uses a Plan-and-Execute strategy, coordinating a Planner, Workers, and a Reporter to solve complex tasks.</p>"},{"location":"#environments-envmd","title":"Environments (<code>env.md</code>)","text":"<p>Environments represent the world in which an agent operates. They provide state and context to the agent. The framework includes several environments, such as a <code>ShellLocalEnv</code> for filesystem access and a <code>BrowserEnv</code> for web interaction.</p>"},{"location":"#toolkits-toolsmd","title":"Toolkits (<code>tools.md</code>)","text":"<p>Toolkits are collections of tools that grant agents their capabilities. The framework comes with a rich set of pre-built toolkits for web search, file manipulation, code execution, document analysis, and more.</p>"},{"location":"#evaluation-framework-evalmd","title":"Evaluation Framework (<code>eval.md</code>)","text":"<p>A comprehensive framework for benchmarking agent performance. It provides a standardized pipeline for: 1. Data Management: Persisting and tracking evaluation data. 2. Processing: Standardizing benchmark-specific logic. 3. Execution: Running the agent and judging its performance automatically.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>For a step-by-step guide on how to install dependencies, set up your environment, and run your first agent, please see our Quickstart Guide.</p>"},{"location":"agents/","title":"Agent Paradigms","text":"<p>The framework provides two distinct agent paradigms to handle tasks of varying complexity: a straightforward <code>SimpleAgent</code> and a more advanced <code>OrchestraAgent</code>. The choice of paradigm is determined by the <code>type</code> field in the <code>AgentConfig</code>.</p> <p>The <code>get_agent</code> factory function is the primary entry point for creating an agent instance based on the specified configuration.</p> <pre><code>from utu.agents import get_agent\nfrom utu.config import ConfigLoader\n\n# Load a config file where type is set to \"simple\" or \"orchestra\"\nconfig = ConfigLoader.load_agent_config(\"my_agent_config\")\n\n# The factory returns the correct agent instance\nagent = get_agent(config)\n</code></pre>"},{"location":"agents/#simpleagent-single-agent-paradigm","title":"<code>SimpleAgent</code> (Single-Agent Paradigm)","text":"<p><code>SimpleAgent</code> implements a classic, single-agent approach based on the Reason-Act (ReAct) framework.</p>"},{"location":"agents/#how-it-works","title":"How it Works","text":"<p>A single LLM is responsible for the entire task-solving process. It operates in a loop:</p> <ol> <li>Reason: The agent analyzes the current task and its context.</li> <li>Act: Based on its reasoning, it selects and invokes an appropriate tool.</li> <li>Observe: It observes the result from the tool and incorporates the new information into its context.</li> </ol> <p>This loop continues until the agent determines that the task is complete and generates a final answer.</p> <pre><code>graph TD\n    A[Start] --&gt; B{Reason about Task};\n    B --&gt; C{Select Tool};\n    C --&gt; D[Execute Tool];\n    D --&gt; E[Observe Result];\n    E --&gt; B;\n    B --&gt; F[Final Answer];</code></pre>"},{"location":"agents/#use-case","title":"Use Case","text":"<p>This paradigm is best suited for straightforward tasks that can be solved by a linear sequence of tool calls and do not require complex, long-term planning or the coordination of different specialized skills.</p>"},{"location":"agents/#usage-examples","title":"Usage Examples","text":"<p>Here are a few examples of how to use <code>SimpleAgent</code>.</p>"},{"location":"agents/#1-basic-usage","title":"1. Basic Usage","text":"<p>You can instantiate <code>SimpleAgent</code> directly and override its configuration, such as the instructions, in the constructor.</p> <pre><code>from utu.agents import SimpleAgent\n\nasync with SimpleAgent(instructions=\"Always answer with prefix `Aloha!`\") as agent:\n    await agent.chat(\"What's the weather in Beijing today?\")\n</code></pre> <p>Besides <code>chat</code>, you can also use the <code>chat_streamed</code> method for streaming output.</p>"},{"location":"agents/#2-using-built-in-tools","title":"2. Using Built-in Tools","text":"<p>For more complex behavior, you can use a YAML configuration file to define the agent and its toolkits.</p> <p>Python Code:</p> <pre><code>from utu.agents import SimpleAgent\n\nasync with SimpleAgent(config=\"sample_tool.yaml\") as agent:\n    await agent.chat(\"What's the weather in Beijing today?\")\n</code></pre> <p>YAML Configuration (<code>configs/agents/sample_tool.yaml</code>):</p> <pre><code># configs/agents/sample_tool.yaml\ndefaults:\n  - /model/base\n  - /tools/search@toolkits.search # Loads the 'search' toolkit\n  - _self_\n\nagent:\n    name: simple-tool-agent\n    instructions: \"You are a helpful assistant that can search the web.\"\n</code></pre>"},{"location":"agents/#3-using-mcp-based-tools","title":"3. Using MCP-based Tools","text":"<p><code>SimpleAgent</code> can also connect to tools running as separate processes via the Multi-Component Protocol (MCP). This is configured by setting the toolkit <code>mode</code> to <code>mcp</code>.</p> <p>Python Code:</p> <pre><code>from utu.agents import SimpleAgent\n\nasync with SimpleAgent(config=\"sample_mcp.yaml\") as agent:\n    await agent.chat(\"What's the time now\")\n</code></pre> <p>YAML Configuration (<code>configs/agents/sample_mcp.yaml</code>):</p> <pre><code># configs/agents/sample_mcp.yaml\ndefaults:\n  - /model/base\n  - _self_\n\ntoolkits:\n  time:\n    name: time\n    mode: mcp\n    activated_tools: null\n    config:\n      command: uvx\n      args: [\"mcp-server-time\", \"--local-timezone=Asia/Shanghai\"]\n\nagent:\n    name: simple-mcp\n    instructions: \"Always answer with prefix `Aloha!`\"\n</code></pre>"},{"location":"agents/#orchestraagent-plan-and-execute-paradigm","title":"<code>OrchestraAgent</code> (Plan-and-Execute Paradigm)","text":"<p><code>OrchestraAgent</code> implements a more sophisticated, multi-agent paradigm based on the \"Plan-and-Execute\" model. It decomposes a complex task into smaller subtasks and orchestrates a team of specialized agents to solve them.</p>"},{"location":"agents/#architecture","title":"Architecture","text":"<p>The orchestra consists of three distinct roles:</p> <ol> <li> <p>Planner Agent: The \"brain\" of the operation. It receives the user's high-level goal and its sole responsibility is to create a detailed, step-by-step plan. Each step in the plan is a subtask assigned to a specific worker.</p> </li> <li> <p>Worker Agent(s): The \"hands\" of the operation. Each worker is a specialized <code>SimpleAgent</code> equipped with a specific set of tools (e.g., a <code>SearchWorker</code> with web search tools, a <code>CodeWorker</code> with file system and code execution tools). A worker receives a single subtask from the plan and executes it.</p> </li> <li> <p>Reporter Agent: The \"mouth\" of the operation. After all subtasks are completed by the workers, the reporter gathers all the results and synthesizes them into a single, coherent, final answer for the user.</p> </li> </ol>"},{"location":"agents/#workflow","title":"Workflow","text":"<p>The process is a clear, sequential flow:</p> <ol> <li>Plan: The Planner creates a multi-step plan.</li> <li>Work: The <code>OrchestraAgent</code> iterates through the plan, dispatching each subtask to the designated Worker and collecting the result.</li> <li>Report: The Reporter synthesizes the collected results into the final answer.</li> </ol> <pre><code>graph TD\n    A[User Task] --&gt; B[Planner Agent];\n    B --&gt; C(Plan: Subtask 1, 2, ...);\n    C --&gt; D[Worker Agents];\n    D --&gt; E{Execute Subtasks Sequentially};\n    E --&gt; F[Collected Results];\n    F --&gt; G[Reporter Agent];\n    G --&gt; H[Final Answer];</code></pre>"},{"location":"agents/#use-case_1","title":"Use Case","text":"<p>This paradigm excels at complex, multi-step tasks that require long-term planning, the use of different specialized tools, or the coordination of multiple skills. By decomposing the problem, it can tackle challenges that would be difficult for a single agent to manage.</p>"},{"location":"auto_generation/","title":"Automatic Generation of Agents and Tools","text":"<p>A key feature of the <code>Youtu-Agent</code> framework is its ability to automate the creation of both tools and agent configurations. This streamlines the development process and reduces the need for manual boilerplate code and configuration.</p>"},{"location":"auto_generation/#automatic-tool-generation","title":"Automatic Tool Generation","text":""},{"location":"auto_generation/#overview","title":"Overview","text":"<ul> <li>Tool deployment: isolated environment; MCP-based communication.</li> <li>Generation approach: implement the tool's capability atomically, test it, then wrap it as an MCP tool.</li> <li>Integration details: a <code>manifest.json</code> specifies how the tool integrates with the <code>Youtu-Agent</code> framework.</li> </ul>"},{"location":"auto_generation/#1-generate-test-the-tool","title":"1. Generate &amp; Test the Tool","text":"<p>Run the following command to start the tool generation process:</p> <pre><code>python scripts/gen_tool.py\n</code></pre> <p>This script will create a new directory (<code>configs/tools/generated/{name}</code>) and configuration file (<code>configs/tools/generated/{name}.yaml</code>). It will also automatically create a virtual environment for the new tool and install its dependencies by running the following commands in the new directory:</p> <pre><code>cd {output_directory}\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n</code></pre>"},{"location":"auto_generation/#2-integrate-the-tool-into-your-agent","title":"2. Integrate the Tool into Your Agent","text":"<p>For instance, if the generated tool is named <code>download_bilibili_video</code>, you can add it to your agent configuration (<code>cofnigs/agents/bilibili.yaml</code>) as follows:</p> <pre><code># @package _global_\ndefaults:\n  - /model/base@model\n  - /tools/generated/download_bilibili_video@toolkits.download_bilibili_video\n  - _self_\n\nagent:\n  name: utu-base\n  instructions: \"You are a helpful assistant.\"\n</code></pre> <p>Then, interact with your agent by running:</p> <pre><code>python scripts/cli_chat.py --config bilibili\n</code></pre>"},{"location":"auto_generation/#automatic-agent-generation","title":"Automatic Agent Generation","text":"<p><code>Youtu-Agent</code> can also automatically generate a configuration for a <code>SimpleAgent</code> based on your requirements. This is handled by an interactive \"meta-agent\" that asks you questions to define the agent's name, instructions, and desired tools.</p>"},{"location":"auto_generation/#1-generate-the-agent-configuration","title":"1. Generate the Agent Configuration","text":"<p>Start the interactive generation process by running:</p> <pre><code>python scripts/gen_simple_agent.py\n</code></pre> <p>The script will guide you through the setup process and save the resulting configuration file in the <code>configs/agents/generated/</code> directory.</p>"},{"location":"auto_generation/#2-run-the-generated-agent","title":"2. Run the Generated Agent","text":"<p>Once the configuration is created, you can run your new agent using the <code>cli_chat.py</code> script. Be sure to replace <code>xxx</code> with the name of your generated config file.</p> <pre><code>python scripts/cli_chat.py --config generated/xxx\n</code></pre>"},{"location":"config/","title":"Configuration System","text":"<p>The project's configuration is managed through a system based on <code>pydantic</code> for data validation and <code>hydra</code> for loading from YAML files. This provides a powerful and flexible way to define agents and experiments.</p> <p>All configurations are stored as <code>.yaml</code> files inside the <code>/configs</code> directory.</p>"},{"location":"config/#configloader","title":"<code>ConfigLoader</code>","text":"<p>The <code>ConfigLoader</code> is the main entry point for loading configurations from YAML files into <code>pydantic</code> models. It abstracts away the file paths and loading logic.</p> <p>Usage:</p> <pre><code>from utu.config import ConfigLoader\n\n# Load an agent configuration from /configs/agents/my_agent.yaml\nagent_config = ConfigLoader.load_agent_config(\"my_agent\")\n\n# Load an evaluation configuration from /configs/eval/my_eval.yaml\neval_config = ConfigLoader.load_eval_config(\"my_eval\")\n</code></pre>"},{"location":"config/#agentconfig","title":"<code>AgentConfig</code>","text":"<p><code>AgentConfig</code> is the central data structure for defining an agent. It specifies everything the agent needs to operate, including its model, tools, and personality.</p>"},{"location":"config/#key-components","title":"Key Components","text":"<ul> <li><code>type</code>: The agent's architecture. Can be:<ul> <li><code>simple</code>: A single agent that performs a task.</li> <li><code>orchestra</code>: A more complex, multi-agent system with a planner and workers.</li> </ul> </li> <li><code>model</code>: (<code>ModelConfigs</code>) Defines the primary LLM the agent will use, including the API provider, model name, and settings like temperature.</li> <li><code>agent</code>: (<code>ProfileConfig</code>) Defines the agent's profile, such as its <code>name</code> and system-level <code>instructions</code> (e.g., \"You are a helpful assistant.\").</li> <li><code>env</code>: (<code>EnvConfig</code>) Specifies the environment the agent operates in (e.g., <code>shell_local</code> or <code>browser_docker</code>). See Agent Environments for more details.</li> <li><code>toolkits</code>: (<code>dict[str, ToolkitConfig]</code>) A dictionary defining the tools available to the agent. Each toolkit can be loaded in <code>builtin</code> mode (running in the main process) or <code>mcp</code> mode (running as a separate process).</li> <li><code>max_turns</code>: The maximum number of conversational turns the agent can take before stopping.</li> </ul> <p>For the <code>orchestra</code> type, <code>AgentConfig</code> also includes fields for defining the planner, workers, and reporter agents.</p>"},{"location":"config/#evalconfig","title":"<code>EvalConfig</code>","text":"<p><code>EvalConfig</code> defines a complete evaluation experiment. It specifies the dataset to use, the agent to test, and how to judge the results.</p>"},{"location":"config/#key-components-for-evaluation","title":"Key Components for Evaluation","text":"<ul> <li><code>data</code>: (<code>DataConfig</code>) Defines the dataset to be used for the evaluation, including its name/path and the relevant fields (<code>question_field</code>, <code>gt_field</code>).</li> <li><code>rollout</code>: This section defines the execution phase of the evaluation.<ul> <li><code>agent</code>: (<code>AgentConfig</code>) A full <code>AgentConfig</code> for the agent being tested.</li> <li><code>concurrency</code>: The number of parallel processes to use when running the agent on the dataset.</li> </ul> </li> <li><code>judgement</code>: This section defines the judgment phase.<ul> <li><code>judge_model</code>: (<code>ModelConfigs</code>) The configuration for the LLM that will act as the judge.</li> <li><code>judge_concurrency</code>: The number of parallel processes to use for judging the results.</li> <li><code>eval_method</code>: The method used for evaluation (e.g., comparing against a ground truth answer).</li> </ul> </li> </ul>"},{"location":"docker/","title":"Quick Start with Docker","text":"<p>This guide walks you through deploying Youtu-agent using Docker containers. It covers the essential steps to get your first agent running with an interactive frontend interface.</p>"},{"location":"docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your system<ul> <li>Visit https://www.docker.com/ to download and install Docker if needed</li> <li>Verify your installation by running <code>docker --version</code></li> </ul> </li> </ul>"},{"location":"docker/#deployment-steps","title":"Deployment Steps","text":""},{"location":"docker/#step-1-build-docker-image","title":"Step 1. Build Docker Image","text":"<p>Build the Youtu-agent Docker image:</p> <pre><code>docker build -t youtu-agent .\n</code></pre>"},{"location":"docker/#step-2-configure-environment","title":"Step 2. Configure Environment","text":"<ol> <li>Create a configuration file by copying the template:</li> </ol> <pre><code>cp .env.docker.example .env\n</code></pre> <ol> <li>Configure the following required variables in <code>.env</code>:</li> </ol> <pre><code># LLM Configuration\nUTU_LLM_TYPE=chat.completions\nUTU_LLM_MODEL=deepseek-chat\nUTU_LLM_BASE_URL=https://api.deepseek.com/v1\nUTU_LLM_API_KEY=&lt;your-api-key&gt;\n\n# Serper API Configuration\n# Get your key from https://serper.dev/playground\nSERPER_API_KEY=&lt;your-serper-key&gt;\n\n# Frontend Configuration\n# Note: IP must be 0.0.0.0 for Docker container port forwarding\nUTU_WEBUI_PORT=8848\nUTU_WEBUI_IP=0.0.0.0\n</code></pre>"},{"location":"docker/#step-3-launch-the-service","title":"Step 3. Launch the Service","text":""},{"location":"docker/#option-1-run-the-default-web-search-agent-demo","title":"Option 1: Run the Default Web Search Agent Demo","text":"<p>By replacing the <code>/path/to/your/.env</code>, please run:</p> <pre><code>docker run -it \\\n    -p 8848:8848 \\\n    -v \"/path/to/your/.env:/youtu-agent/.env\" \\\n    youtu-agent\n</code></pre> <p>The service will be accessible at http://127.0.0.1:8848</p>"},{"location":"docker/#option-2-interactive-shell-access","title":"Option 2: Interactive Shell Access","text":"<p>To run other examples or custom configurations by replacing the <code>/path/to/your/.env</code>:</p> <pre><code>docker run -it \\\n    -p 8848:8848 \\\n    -v \"/path/to/your/.env:/youtu-agent/.env\" \\\n    youtu-agent \\\n    bash\n</code></pre>"},{"location":"docker/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Examples: Check the <code>/examples</code> directory for more detailed use cases and advanced scripts.</li> </ul>"},{"location":"env/","title":"Agent Environments","text":"<p>An Environment (<code>Env</code>) represents the world in which the agent operates. Its primary responsibilities are to provide the agent with a sense of its current state and a set of tools to interact with that world.</p> <p>The framework uses a factory function, <code>get_env</code>, to create the appropriate environment based on the agent's configuration file.</p>"},{"location":"env/#core-concepts","title":"Core Concepts","text":"<p>All environments inherit from the abstract base class <code>Env</code>, which defines the core interface:</p> <ul> <li><code>get_state() -&gt; str</code>: Returns a string describing the current state of the environment. This information is injected into the agent's prompt to provide context.</li> <li><code>get_tools() -&gt; list[Tool]</code>: Returns a list of <code>Tool</code> objects that the agent can use to interact with the environment.</li> <li><code>build()</code> / <code>cleanup()</code>: Methods to manage the lifecycle of the environment, such as starting services or cleaning up resources.</li> </ul>"},{"location":"env/#recommended-environments","title":"\u2b50 Recommended Environments","text":"<p>For production use and maximum security, we recommend using E2B-based environments:</p> <ul> <li>E2BEnv: For code execution and file manipulation tasks</li> <li>BrowserE2BEnv: For web automation and browser interaction tasks</li> </ul> <p>These cloud-based sandboxes provide enterprise-grade isolation, eliminating risks associated with running untrusted or AI-generated code on your local machine. They offer automatic resource cleanup, GUI monitoring capabilities (NoVNC), and seamless integration with modern development workflows. In practice, we use Tencent Cloud's Agent Sandbox service to host these environments securely.</p>"},{"location":"env/#available-environments","title":"Available Environments","text":"<p>Here are the currently available environment implementations.</p> Config Name Environment Class Description Recommendation <code>e2b</code> <code>E2BEnv</code> Cloud-based code interpreter \u2b50 Recommended for code execution <code>browser_e2b</code> <code>BrowserE2BEnv</code> Cloud-based browser automation \u2b50 Recommended for web tasks <code>shell_local</code> <code>ShellLocalEnv</code> Local shell with workspace Use for local development only <code>browser_docker</code> <code>BrowserEnv</code> Docker-based browser Use if E2B is unavailable <code>base</code> <code>BasicEnv</code> No environment For simple tasks without execution"},{"location":"env/#basicenv","title":"BasicEnv","text":"<p>This is the simplest and default environment. It can be considered a \"null\" environment.</p> <ul> <li>State: Provides no state information (returns an empty string).</li> <li>Tools: Provides no tools (returns an empty list).</li> <li>Use Case: Used when the agent's task does not require any specific environmental interaction.</li> </ul>"},{"location":"env/#shelllocalenv","title":"ShellLocalEnv","text":"<p>This environment provides the agent with an isolated workspace on the local filesystem.</p> <ul> <li>State: The state string includes the current time, the absolute path to the isolated workspace, and a crucial instruction for the agent: <code>You can only run bash commands in your workspace!!!</code>. This helps guide and constrain the agent's behavior.</li> <li>Tools: This environment does not provide tools directly. The agent must be configured separately with tools capable of executing shell commands (e.g., a <code>bash</code> tool). The environment's role is to provide the context and workspace for those tools.</li> <li>Isolation: A unique workspace directory is created for each run session, preventing interference between different tasks.</li> </ul>"},{"location":"env/#browserenv","title":"BrowserEnv","text":"<p>This is an environment that gives the agent control over a fully-featured, interactive web browser.</p> <ul> <li>Architecture: <code>BrowserEnv</code> runs a browser automation service inside a Docker container. This ensures that each agent session is completely isolated and has a clean, predictable browser environment.</li> <li>State: The state represents the current content of the web page. It is updated after every action (e.g., clicking an element, navigating to a URL), giving the agent feedback on the result of its last action.</li> <li>Tools: Tools are provided dynamically by the browser service running in the container. <code>BrowserEnv</code> acts as a proxy: it discovers the available tools (e.g., <code>go_to_url</code>, <code>click_element</code>, <code>input_text</code>) and makes them available to the agent. When the agent calls a tool, <code>BrowserEnv</code> forwards the request to the Docker container for execution.</li> </ul>"},{"location":"env/#e2benv","title":"E2BEnv","text":"<p><code>E2BEnv</code> is a cloud-based code execution environment powered by E2B, offering superior security and isolation compared to local environments.</p> <ul> <li>Architecture: Uses E2B's Code Interpreter sandbox (<code>code-interpreter-v1</code> template) running in a secure, isolated cloud environment. Each session creates a fresh AsyncSandbox instance.</li> <li>Tools: You can config python, bash, file editing tools with E2BEnv.</li> <li>Use Case: Recommended for all code execution tasks where security is a concern, especially when running untrusted or AI-generated code.</li> <li>Configuration: See <code>configs/agents/examples/e2b/e2b_python.yaml</code> for a complete setup example with Python, Bash, and file editing tools.</li> </ul> <pre><code># @package _global_\ndefaults:\n  - /tools/e2b/python_executor@toolkits.PythonTool\n  - /tools/e2b/bash@toolkits.BashTool\n  - /tools/e2b/file_edit@toolkits.FileTool\n  - _self_\n\nenv:\n  name: e2b\n  config:\n    request_timeout: 5  # Optional: timeout in seconds for file operations (default: 5)\n\nagent: ...\n</code></pre>"},{"location":"env/#browsere2benv","title":"BrowserE2BEnv","text":"<p>This is an advanced browser automation environment built on E2B, providing secure browser control with GUI access.</p> <ul> <li>Architecture: Leverages E2B's SDK combined with TencentCloud's Agent Sandbox service. Integrates Playwright MCP server for tool provisioning via Chrome DevTools Protocol (CDP).</li> <li>Tools: Dynamically provided by the Playwright MCP server (<code>@playwright/mcp</code>), including:</li> <li>Browser navigation and page interaction</li> <li>Element clicking, text input, and form filling</li> <li>Screenshot capture and page content extraction</li> <li>Multi-tab and window management</li> <li>Configuration: See <code>configs/agents/examples/e2b/e2b_browser.yaml</code> for a complete setup example. Note that browser tools are auto-discovered from the MCP server\u2014no manual tool configuration required.</li> </ul> <pre><code># @package _global_\ndefaults:\n  - /model/base@model\n  - _self_\n\nenv:\n  name: browser_e2b\n\nagent: ...\n</code></pre>"},{"location":"environment_variables/","title":"Environment Variables","text":"<p>This document outlines the configuration of key environment variables in Youtu-Agent.</p>"},{"location":"environment_variables/#overview","title":"Overview","text":"<p>Before running Youtu-Agent, you need to configure the necessary environment variables. A recommended practice is to start by copying the <code>.env.example</code> file from the project root:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Then, edit the <code>.env</code> file to fill in the required API Keys and other configurations. For a more comprehensive list of options, please refer to the <code>.env.full</code> file, which contains all available environment variables and their detailed descriptions.</p>"},{"location":"environment_variables/#llm-api-keys","title":"LLM API Keys","text":"<p>The core capabilities of Youtu-Agent rely on Large Language Models (LLMs). You need to configure the appropriate models for different functional modules.</p>"},{"location":"environment_variables/#core-llm","title":"Core LLM","text":"<p>This is the primary text generation LLM that the Agent relies on for its operations.</p> <pre><code># LLM type (e.g., chat.completions, responses)\nUTU_LLM_TYPE=chat.completions\n# LLM model name (e.g., deepseek-chat, gpt-4-turbo)\nUTU_LLM_MODEL=deepseek-chat\n# LLM provider's API base URL (e.g., https://api.deepseek.com/v1)\nUTU_LLM_BASE_URL=https://api.deepseek.com/v1\n# LLM provider's API Key\nUTU_LLM_API_KEY=YOUR_API_KEY\n</code></pre>"},{"location":"environment_variables/#vision-and-audio-llms","title":"Vision and Audio LLMs","text":"<p>When using the <code>ImageToolkit</code> or <code>AudioToolkit</code>, you need to configure LLMs that support multimodal capabilities.</p> <p>Image (Vision) LLM:</p> <pre><code># Image LLM type\nUTU_IMAGE_LLM_TYPE=\n# Image LLM model name (e.g., qwen-vl-plus)\nUTU_IMAGE_LLM_MODEL=\n# Image LLM API base URL\nUTU_IMAGE_LLM_BASE_URL=\n# Image LLM API Key\nUTU_IMAGE_LLM_API_KEY=\n</code></pre> <p>Audio LLM:</p> <pre><code># Audio LLM model name (e.g., whisper-1)\nUTU_AUDIO_LLM_MODEL=\n# Audio LLM API base URL\nUTU_AUDIO_LLM_BASE_URL=\n# Audio LLM API Key\nUTU_AUDIO_LLM_API_KEY=\n</code></pre>"},{"location":"environment_variables/#evaluation-llm","title":"Evaluation LLM","text":"<p>When running the evaluation framework (<code>/utu/eval</code>), a separate \"Judge\" LLM is required to score and assess the Agent's output.</p> <pre><code># Judge LLM type\nJUDGE_LLM_TYPE=\n# Judge LLM model name\nJUDGE_LLM_MODEL=\n# Judge LLM API base URL\nJUDGE_LLM_BASE_URL=\n# Judge LLM API Key\nJUDGE_LLM_API_KEY=\n</code></pre>"},{"location":"environment_variables/#tools","title":"Tools","text":"<p>Some Toolkits require their own API Keys or specific configurations.</p>"},{"location":"environment_variables/#search-toolkit-searchtoolkit","title":"Search Toolkit (<code>SearchToolkit</code>)","text":"<p>The search toolkit integrates the following two services by default:</p> <ol> <li>Web Search: Uses the efficient Google Search API provided by Serper. You will need to register and obtain an API Key.</li> <li>Web Content Extraction: Uses the Jina AI Reader to convert web page content into an LLM-friendly Markdown format. This also requires registration and an API Key.</li> </ol> <p>Configure them in your <code>.env</code> file as follows:</p> <pre><code># Get from https://serper.dev/\nSERPER_API_KEY=YOUR_SERPER_API_KEY\n# Get from https://jina.ai/reader/\nJINA_API_KEY=YOUR_JINA_API_KEY\n</code></pre>"},{"location":"environment_variables/#tracing-monitoring","title":"Tracing &amp; Monitoring","text":"<p>The framework integrates OpenTelemetry and Phoenix for tracing and monitoring the Agent's execution flow.</p> <pre><code># Phoenix/OpenTelemetry trace receiver endpoint\nPHOENIX_ENDPOINT=http://127.0.0.1:6006/v1/traces\n# Project name displayed in the Phoenix UI\nPHOENIX_PROJECT_NAME=Youtu-Agent\n</code></pre>"},{"location":"environment_variables/#miscellaneous","title":"Miscellaneous","text":""},{"location":"environment_variables/#hugging-face","title":"Hugging Face","text":"<p>Used to download datasets (e.g., GAIA) from the Hugging Face Hub.</p> <pre><code># Get from https://huggingface.co/settings/tokens\nHF_TOKEN=YOUR_HUGGINGFACE_TOKEN\n</code></pre>"},{"location":"environment_variables/#web-ui","title":"Web UI","text":"<p>Used to configure the address and port for the front-end interface.</p> <pre><code># Port for the frontend service to listen on\nUTU_WEBUI_PORT=8848\n# IP address for the frontend service to listen on\nUTU_WEBUI_IP=127.0.0.1\n# Whether to enable auto reload for tornado server\nUTU_WEBUI_AUTOLOAD=false\n</code></pre>"},{"location":"eval/","title":"Evaluation Framework","text":"<p>The <code>utu/eval/</code> module provides a standardized and extensible framework for benchmarking agents. It is designed to be modular, allowing for the easy addition of new datasets and evaluation methods. The entire workflow is orchestrated by <code>BaseBenchmark</code>, which ties together data management, benchmark-specific logic, and the agent being tested.</p> <p>Here are the three core components of the framework:</p>"},{"location":"eval/#1-data-management-dbdatamanager","title":"1. Data Management (<code>DBDataManager</code>)","text":"<p>The <code>DBDataManager</code> is the persistence layer of the evaluation framework. It is responsible for loading datasets and tracking the state of every sample throughout the evaluation process.</p> <ul> <li>Database Backend: It uses a database (defaulting to SQLite) to store all evaluation data. Each sample is stored as an <code>EvaluationSample</code> record.</li> <li>Experiment Tracking: All samples are associated with an <code>exp_id</code> (Experiment ID). This allows for easy tracking and resuming of experiments. If you run an evaluation with an existing <code>exp_id</code>, the system will pick up where it left off.</li> <li>Stateful Tracking: Each <code>EvaluationSample</code> has a <code>stage</code> field (<code>init</code>, <code>rollout</code>, <code>judged</code>) that tracks its progress through the pipeline. This ensures that each step of the evaluation only processes the relevant samples.</li> </ul>"},{"location":"eval/#2-dataset-standardization-processer","title":"2. Dataset Standardization (<code>Processer</code>)","text":"<p>A <code>Processer</code> handles all the logic that is specific to a particular benchmark (e.g., GAIA, BrowseComp). This design cleanly separates the generic evaluation flow from the details of each dataset.</p> <ul> <li><code>BaseProcesser</code> Interface: This abstract class defines the contract for all processers, which must implement:<ul> <li><code>preprocess_one()</code>: Prepares a raw data sample for the agent. This can involve reformatting the question, adding specific instructions, or attaching file paths.</li> <li><code>judge_one()</code>: Evaluates an agent's response for a single sample.</li> <li><code>calculate_metrics()</code>: Computes the final summary statistics for a benchmark.</li> </ul> </li> <li>Judging Strategies: The framework provides different base implementations for judging:<ul> <li><code>BaseLLMJudgeProcesser</code>: Uses a powerful LLM as a judge, guided by specialized prompt templates. This is suitable for complex, open-ended questions.</li> <li><code>BaseMatchProcesser</code>: Uses rule-based methods (e.g., exact string or number matching) for judging. This is faster and more suitable for questions with a single, precise answer.</li> </ul> </li> <li><code>PROCESSER_FACTORY</code>: A factory that automatically discovers and registers all available processers. The <code>BaseBenchmark</code> uses this factory to dynamically select the correct processer for each sample based on its <code>source</code> field.</li> </ul>"},{"location":"eval/#3-standardized-test-flow-basebenchmark","title":"3. Standardized Test Flow (<code>BaseBenchmark</code>)","text":"<p><code>BaseBenchmark</code> is the main orchestrator that drives the entire evaluation pipeline from start to finish. It provides a standardized, four-stage process.</p> <ul> <li><code>preprocess</code>: Loads all initial samples from the <code>DBDataManager</code> and uses the appropriate <code>Processer</code> to prepare them for the agent.</li> <li><code>rollout</code>: Runs the configured agent on all the preprocessed samples. The agent's response, trajectory, and other metadata are saved back to the database for each sample.</li> <li><code>judge</code>: Fetches the completed samples from the <code>rollout</code> stage and uses the <code>Processer</code> to evaluate whether the agent's response was correct.</li> <li><code>stat</code>: Gathers all judged samples, groups them by benchmark, and uses the <code>Processer</code> to calculate and log the final metrics (e.g., accuracy).</li> </ul> <p>This structured pipeline ensures that every evaluation is consistent, automated, and resilient, as it can be stopped and resumed at any stage.</p>"},{"location":"examples/","title":"Examples","text":"<p>Check out examples in the <code>examples</code> directory.</p> Example Core Architecture Implementation Key Tools Use Case / Features <code>research</code> Manual Multi-Agent Orchestration Orchestrates 3 independent <code>SimpleAgent</code>s in code to create a \"Plan-and-Execute\" workflow. <code>SearchToolkit</code> Demonstrates building a complex workflow from scratch using basic SimpleAgent blocks. <code>wide_research</code> \"Agent-as-Tool\" Pattern A single <code>SimpleAgent</code> makes decisions and calls a custom tool that encapsulates parallel sub-agents. <code>SearchToolkit</code> Shows how to encapsulate parallelism and complex logic within a tool, simplifying the main agent's logic. <code>paper_collector</code> Standard OrchestraAgent Configuration-driven; uses few-shot examples (<code>planner_examples_data.json</code>) to guide the Planner. <code>DocumentToolkit</code>, <code>SearchToolkit</code> A standard, \"out-of-the-box\" application of OrchestraAgent for multi-step document analysis. <code>file_manager</code> SimpleAgent + UI A configuration-driven <code>SimpleAgent</code> wrapped in an interactive <code>Gradio</code> web UI. <code>BashToolkit</code> A practical example of an agent interacting with the local file system, with a focus on UI and safety. <code>data_analysis</code> Customized OrchestraAgent Extends OrchestraAgent's core components: 1. <code>DAPlannerAgent</code>: Proactively inspects data schema before planning. 2. Reporter: Uses a custom template to generate a rich HTML report. <code>TabularDataToolkit</code> An advanced OrchestraAgent use case, showing deep customization for a complex, domain-specific problem. <code>ppt_gen</code> SimpleAgent A configuration-driven <code>SimpleAgent</code> that synthesizes content from a given document and generates a PowerPoint presentation based on the given json schema and content page by page. <code>SearchToolkit</code> An experimental demo of using SimpleAgent to generate a PowerPoint presentation."},{"location":"examples/#run-the-examples","title":"Run the Examples","text":"<p>In each example directory, you can run the <code>main.py</code> files to start the examples in command line. For some examples, you can also run the <code>main_web.py</code> files to start the examples with WebUI. Refer to corresponding <code>README.md</code> files in the example directories for more details.</p> <p>Note: To use the WebUI, you need to install the <code>utu_agent_ui</code> package. Refer to Installation for more details.</p>"},{"location":"faq/","title":"FAQ","text":"<p>This page addresses frequently asked questions and common issues.</p>"},{"location":"faq/#why-is-the-agents-output-truncated","title":"Why is the agent's output truncated?","text":"<p>If you observe that the agent's output is being cut off unexpectedly, the issue may be related to the <code>max_output_token</code> limit set by the LLM service you are using.</p> <p>For example, the DeepSeek API sets a default limit of 4,096 output tokens for the <code>deepseek-chat</code> model, but this can be manually extended up to 8,192 tokens.</p> <p>To resolve this, you can explicitly set the <code>max_tokens</code> parameter in your model's configuration (<code>ModelSettingsConfig</code>).</p> <pre><code>model:\n  # ... other provider settings\n  model_settings:\n    temperature: 0.3\n    top_p: 0.95\n    max_tokens: 8000\n</code></pre> <p>For more context, see this issue.</p>"},{"location":"faq/#how-can-i-resolve-llm-request-timeouts","title":"How can I resolve LLM request timeouts?","text":"<p>If you are encountering request timeouts, first ensure that the LLM service is operational. If the service is running correctly, you may need to increase the request timeout period.</p> <p>The default timeout for the <code>openai</code> Python package is 600 seconds. You can override this by setting the <code>timeout</code> value within <code>extra_args</code> in your <code>ModelSettingsConfig</code>.</p> <pre><code>model:\n  # ... other provider settings\n  model_settings:\n    extra_args:\n      timeout: 1200 # Sets the timeout to 1200 seconds\n</code></pre>"},{"location":"faq/#how-to-use-litellm-or-azure-model","title":"How to use LiteLLM (or Azure) model?","text":"<p>Method 1: If the LiteLLM service is compatible with the openai chat.completions API, you can simply set basic environment variables in your <code>.env</code> file:</p> <pre><code>UTU_LLM_TYPE=chat.completions  # use the default llm calling method\n# basic openai configs, see `.env.full` if you're not familiar with these configs\nUTU_LLM_MODEL=\nUTU_LLM_BASE_URL=\nUTU_LLM_API_KEY=\n</code></pre> <p>Method 2: If the service need to be used by the <code>litellm</code> package, you should install the additional package and config the following environment variables:</p> <pre><code>UTU_LLM_TYPE=litellm  # set the llm type as litellm\n# set the litellm model name. e.g. azure/gpt-5\nUTU_LLM_MODEL=\n# add other necessary litellm configs bellow, see https://docs.litellm.ai/docs/providers/\n</code></pre> <p>e.g. for Azure support, you need to set:</p> <pre><code>AZURE_API_BASE=https://&lt;YOUR-RESOURCE-NAME&gt;.azure.com/\nAZURE_API_KEY=&lt;AZURE_OPENAI_API_KEY&gt;\n</code></pre> <p>For more context, ref this issue</p>"},{"location":"frontend/","title":"WebUI","text":"<p>We provide a simple WebUI for visualizing the conversation.</p>"},{"location":"frontend/#basic-usage","title":"Basic Usage","text":"<p>It is required to install the <code>utu_agent_ui</code> package before using the WebUI. Refer to Installation for more details.</p> <p>After installing <code>utu_agent_ui</code>, The <code>WebUIAgents</code> class (from <code>utu.ui.webui_agents</code>) should work. Here is a simple usage example:</p> <pre><code>from utu.ui.webui_agents import WebUIAgents\n\nwebui = WebUIAgents(default_config=\"simple/base.yaml\")\nprint(f\"Server started at http://127.0.0.1:8848/\")\nwebui.launch(ip=\"127.0.0.1\", port=8848)\n</code></pre>"},{"location":"frontend/#examples-with-webui-integration","title":"Examples with WebUI Integration","text":"<p>The <code>examples</code> directory contains some examples for you to play with. You can run the <code>main_web.py</code> files to start the WebUI. For example:</p> <pre><code>python examples/data_analysis/main_web.py\n</code></pre> <p>By default, the WebUI uses port <code>8848</code> and IP <code>127.0.0.1</code>. You can customize these settings by adding the following configuration to your <code>.env</code> file:</p> <pre><code># =============================================\n# frontend\n# =============================================\nUTU_WEBUI_PORT=8848\nUTU_WEBUI_IP=127.0.0.1\n</code></pre> <p>If you change the default port or IP, make sure to update the WebSocket URL in your frontend configuration to match the new settings. The default WebSocket URL is <code>ws://localhost:8848/ws</code>, and you can find and modify this setting in the right top corner of the frontend page.</p>"},{"location":"frontend/#webuichatbot-deprecated","title":"WebUIChatbot (Deprecated)","text":"<p>Warning</p> <p>The <code>WebUIChatbot</code> class (from <code>utu.ui.webui_chatbot</code>) is deprecated. Please use <code>WebUIAgents</code> instead.</p> <p>Here is a simple usage example for <code>WebUIChatbot</code>:</p> <pre><code>from utu.ui.webui_chatbot import WebUIChatbot\nfrom utu.agents import SimpleAgent\n\nsimple_agent = SimpleAgent(name=\"demo\")\n\nchatbot = WebUIChatbot(\n    simple_agent,\n    example_query=\"Hello, how are you?\",\n)\nchatbot.launch(port=8848, ip=\"127.0.0.1\")\n</code></pre> <p>It also works with an <code>OrchestraAgent</code>. The following pseudo-code shows the basic usage:</p> <pre><code>from utu.agents import OrchestraAgent\nfrom utu.ui.webui_chatbot import WebUIChatbot\n\n# ... load the config ...\n\norchestra_agent = OrchestraAgent(config=config)\nchatbot = WebUIChatbot(\n    orchestra_agent,\n    example_query=\"Hello, how are you?\",\n)\nchatbot.launch(port=8848, ip=\"127.0.0.1\")\n</code></pre>"},{"location":"frontend/#installation","title":"Installation","text":"<p>We ship the static web pages in the <code>utu_agent_ui</code> package, which can be installed by either <code>pip install</code>ing the prebuilt wheel file or compiling from source.</p>"},{"location":"frontend/#installing-the-prebuilt-whl-file","title":"Installing the Prebuilt <code>*.whl</code> file","text":"<p>Download prebuilt wheel file (<code>utu_agent_ui-0.3.0-py3-none-any.whl</code>) from releases and run the following command:</p> <pre><code>uv pip install utu_agent_ui-0.3.0-py3-none-any.whl\n</code></pre>"},{"location":"frontend/#compiling-from-source","title":"Compiling from Source","text":"<p>Before compiling, make sure you have <code>npm</code> installed.</p> <p><code>cd</code> into the <code>utu/ui/frontend</code> directory and run the following commands:</p> <pre><code>npm install\nuv pip install build\nbash ./build.sh\n</code></pre> <p>The wheel file will be in the <code>build</code> directory. Refer to Installing the Prebuilt <code>*.whl</code> file to install it.</p>"},{"location":"frontend/#implementation-details","title":"Implementation Details","text":"<p>The <code>WebUIAgents</code> class is basically a tornado based WebSocket server, which translate the model responses to JSON events and send them through a WebSocket connection to the frontend.</p> <p>The frontend (in <code>utu/ui/frontend</code>, or installed as <code>utu_agent_ui</code> package) is a React application, which visualizes the events, and provides a simple UI for users to interact with the agent.</p> <p>You can customize the port and IP of <code>WebUIAgents</code> by setting the <code>UTU_WEBUI_PORT</code> and <code>UTU_WEBUI_IP</code> environment variables (default: <code>127.0.0.1:8848</code>). The default WebSocket URL is <code>ws://localhost:8848/ws</code>, and you can find and modify this setting in the right top corner of the frontend page.</p> <p>For development, you can set <code>UTU_WEBUI_AUTOLOAD=true</code> to enable auto reload for tornado server.</p>"},{"location":"practice/","title":"Agent Practice with Training-Free GRPO","text":"<p>This guide covers the agent practice functionality in Youtu-Agent, powered by Training-Free Group Relative Policy Optimization (GRPO). Training-Free GRPO is a cost-effective solution that enhances agent performance without LLM parameter updates by leveraging group relative semantic advantages and iteratively distilling high-quality experiential knowledge.</p>"},{"location":"practice/#overview","title":"Overview","text":"<p>The practice module provides core functionality for:</p> <ul> <li>Training-Free Learning: Improve agent performance without fine-tuning model parameters</li> <li>Experience Distillation: Extract and integrate high-quality experiential knowledge</li> <li>Flexible Evaluation: Configurable reward calculation through custom verification functions</li> <li>Domain Adaptation: Support for diverse tasks from math reasoning to web search</li> </ul>"},{"location":"practice/#module-structure","title":"Module Structure","text":"<pre><code>utu/practice/\n\u251c\u2500\u2500 __init__.py                 # Module exports\n\u251c\u2500\u2500 training_free_grpo.py       # Main orchestrator\n\u251c\u2500\u2500 rollout_manager.py          # Rollout execution and batch processing\n\u251c\u2500\u2500 experience_updater.py       # Experience processing and integration\n\u251c\u2500\u2500 data_manager.py             # Dataset management\n\u251c\u2500\u2500 utils.py                    # Configuration parsing and utilities\n\u251c\u2500\u2500 dataset/                    # Dataset storage directory\n\u2514\u2500\u2500 verify/                     # Verification functions\n    \u251c\u2500\u2500 math.py                 # Math verification\n    \u2514\u2500\u2500 webwalker.py            # Web search verification\n</code></pre>"},{"location":"practice/#quick-start","title":"Quick Start","text":""},{"location":"practice/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ol> <li>Completed the QuickStart guide for environment setup</li> <li>Installed all dependencies: <code>uv sync --all-extras</code></li> <li>Activated the virtual environment: <code>source .venv/bin/activate</code></li> <li>Configured API keys in <code>.env</code> file</li> </ol>"},{"location":"practice/#basic-workflow","title":"Basic Workflow","text":"<p>The practice process follows these steps:</p> <ol> <li>Data Preparation: Upload datasets for practice and evaluation</li> <li>Verification Setup: Configure domain-specific verification functions</li> <li>Configuration: Prepare agent, evaluation, and practice configs</li> <li>Baseline Evaluation: Evaluate initial agent performance</li> <li>Run Training-Free GRPO: Execute the practice process</li> <li>Evaluate Enhanced Agent: Assess improved performance</li> </ol>"},{"location":"practice/#configuration-system","title":"Configuration System","text":"<p>The practice module uses a hierarchical configuration approach:</p>"},{"location":"practice/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<pre><code>configs/\n\u251c\u2500\u2500 agents/practice/              # Agent configurations\n\u2502   \u251c\u2500\u2500 math_agent.yaml\n\u2502   \u251c\u2500\u2500 math_practice_agent.yaml\n\u2502   \u251c\u2500\u2500 web_agent.yaml\n\u2502   \u2514\u2500\u2500 web_practice_agent.yaml\n\u251c\u2500\u2500 eval/                         # Evaluation configurations\n\u2502   \u251c\u2500\u2500 math/\n\u2502   \u2502   \u251c\u2500\u2500 math_AIME24.yaml\n\u2502   \u2502   \u2514\u2500\u2500 math_AIME25.yaml\n\u2502   \u2514\u2500\u2500 web/\n\u2502       \u251c\u2500\u2500 web.yaml\n\u2502       \u2514\u2500\u2500 web_practice.yaml\n\u2514\u2500\u2500 practice/                     # Practice configurations\n    \u251c\u2500\u2500 math_reasoning.yaml\n    \u2514\u2500\u2500 web_search.yaml\n</code></pre>"},{"location":"practice/#configuration-components","title":"Configuration Components","text":"<p>TrainingFreeGRPOConfig: Unified configuration class with:</p> <ul> <li><code>exp_id</code>: Experiment identifier</li> <li><code>PracticeArguments</code>: Practice-specific parameters (epochs, batch size, GRPO settings)</li> <li><code>DataArguments</code>: Data processing parameters</li> <li><code>EvalConfig</code>: Evaluation configuration reference</li> </ul> <p>Utilities:</p> <ul> <li><code>TaskRecorder</code>: Records practice progress, experiences, and statistics</li> <li><code>parse_training_free_grpo_config()</code>: Configuration parser with YAML files and command-line overrides</li> </ul>"},{"location":"practice/#data-preparation","title":"Data Preparation","text":""},{"location":"practice/#upload-from-huggingface","title":"Upload from HuggingFace","text":"<p>Use the provided script to load built-in datasets:</p> <pre><code>python scripts/data/process_training_free_GRPO_data.py\n</code></pre> <p>Built-in datasets include:</p> <ul> <li>AIME24/AIME25: AIME competition problems</li> <li>DAPO-Math-17k: Math problems from DAPO dataset</li> <li>AFM_web_RL: Web agent reinforcement learning dataset</li> <li>WebWalkerQA: Web navigation question-answering dataset</li> </ul>"},{"location":"practice/#upload-custom-datasets","title":"Upload Custom Datasets","text":"<p>Upload your own datasets from local files:</p> <pre><code>python scripts/data/upload_dataset.py \\\n  --file_path path/to/your_dataset.jsonl \\\n  --dataset_name YourDataset\n</code></pre> <p>Required fields for each sample:</p> <pre><code>{\n    \"dataset\": \"YourDataset\",           # Dataset name\n    \"source\": \"training_free_grpo\",     # Must be \"training_free_grpo\"\n    \"question\": \"What is 2+2?\",         # The question/prompt\n    \"answer\": \"4\"                       # Expected answer (or None)\n}\n</code></pre>"},{"location":"practice/#verification-functions","title":"Verification Functions","text":"<p>Verification functions are the core of the reward calculation system, providing domain-specific evaluation criteria.</p>"},{"location":"practice/#function-interface","title":"Function Interface","text":"<p>Create verification functions in <code>utu/practice/verify/</code>:</p> <pre><code>from utu.db import EvaluationSample\n\ndef verify_func(sample: EvaluationSample, timeout_score: float = 0, **kwargs) -&gt; dict:\n    \"\"\"\n    Verify the correctness of an agent response.\n\n    Args:\n        sample: EvaluationSample containing:\n            - raw_question: Original question\n            - correct_answer: Ground truth answer\n            - response: Agent's final response\n            - other metadata fields\n        timeout_score: Score for timeout cases\n        **kwargs: Additional arguments including:\n            - llm: LLM client for verification requiring judgment\n\n    Returns:\n        dict: {\n          \"reward\": float,          # ranges from 0.0 to 1.0\n          \"reasoning\": str | None   # extra details for experience extraction\n        }\n    \"\"\"\n    # Your verification logic here\n    pass\n</code></pre>"},{"location":"practice/#built-in-verification-functions","title":"Built-in Verification Functions","text":"<p>Math Verification (<code>utu/practice/verify/math.py</code>):</p> <ul> <li>Uses symbolic math verification</li> <li>Compares extracted expressions with ground truth</li> <li>Requires <code>math-verify</code> package: <code>uv pip install math-verify</code></li> </ul> <p>Web Search Verification (<code>utu/practice/verify/webwalker.py</code>):</p> <ul> <li>LLM-based judgment for web search responses</li> <li>Compares agent response with ground truth using judge LLM</li> <li>Access judge via <code>kwargs['llm']</code></li> </ul>"},{"location":"practice/#custom-verification","title":"Custom Verification","text":"<p>Example for simple string matching:</p> <pre><code># utu/practice/verify/str_match.py\nfrom utu.db import EvaluationSample\n\ndef string_match_verify(sample: EvaluationSample, timeout_score: float = 0, **kwargs) -&gt; dict:\n    \"\"\"Simple string matching verification.\"\"\"\n    if sample.correct_answer.lower() == sample.response.lower():\n        return {\"reward\": 1.0, \"reasoning\": None}\n    return {\"reward\": 0.0, \"reasoning\": None}\n</code></pre>"},{"location":"practice/#configuration-files","title":"Configuration Files","text":""},{"location":"practice/#agent-configuration","title":"Agent Configuration","text":"<p>Create or use existing agent configs in <code>configs/agents/practice/</code>. See Agents for detailed configuration options.</p>"},{"location":"practice/#evaluation-configuration","title":"Evaluation Configuration","text":"<p>Create evaluation config in <code>configs/eval/</code>:</p> <pre><code># configs/eval/my_domain/my_eval.yaml\n# @package _global_\ndefaults:\n  - /agents/practice/my_agent@agent\n  - _self_\n\nexp_id: \"my_eval\"\n\n# Evaluation dataset\ndata:\n  dataset: \"MyEvalDataset\"\n  type: \"single\"\n\n# Evaluation settings\nconcurrency: 64\npass_k: 3\n\n# Verification function\nverify_filename: \"my_verify.py\"\nverify_func_name: \"my_verify_func\"\n\n# Optional: Judge model for LLM-based verification\njudge_model:\n  model_provider:\n    type: ${oc.env:JUDGE_LLM_TYPE}\n    model: ${oc.env:JUDGE_LLM_MODEL}\n    base_url: ${oc.env:JUDGE_LLM_BASE_URL}\n    api_key: ${oc.env:JUDGE_LLM_API_KEY}\n  model_params:\n    temperature: 0.5\n</code></pre>"},{"location":"practice/#practice-configuration","title":"Practice Configuration","text":"<p>Create practice config in <code>configs/practice/</code>:</p> <pre><code># configs/practice/my_practice.yaml\n# @package _global_\ndefaults:\n  - /eval/my_domain/my_eval@evaluation\n  - _self_\n\nexp_id: \"my_practice\"\n\n# Practice Arguments\npractice:\n  epochs: 5\n  batch_size: 32\n  grpo_n: 3\n  rollout_concurrency: 64\n  rollout_temperature: 0.7\n  task_timeout: 3600\n  do_eval: false\n  eval_strategy: \"epoch\"\n  restart_step: null\n  agent_objective: |\n    input: Description of input\n    output: Description of expected output\n  learning_objective: |\n    Description of learning goals and expected experiences\n  num_experiences_per_query: 1\n\n# Data Arguments\ndata:\n  practice_dataset_name: \"MyPracticeDataset\"\n</code></pre>"},{"location":"practice/#running-practice","title":"Running Practice","text":""},{"location":"practice/#evaluate-baseline","title":"Evaluate Baseline","text":"<p>First, evaluate the baseline agent:</p> <pre><code>python scripts/run_eval.py \\\n  --config_name my_domain/my_eval\n</code></pre>"},{"location":"practice/#execute-training-free-grpo","title":"Execute Training-Free GRPO","text":"<p>Run the practice process:</p> <pre><code># Using configuration file\npython scripts/run_training_free_GRPO.py \\\n  --config_name my_practice\n\n# With parameter overrides\npython scripts/run_training_free_GRPO.py \\\n  --config_name my_practice \\\n  --experiment_name my_practice \\\n  --epochs 5 \\\n  --batch_size 64\n</code></pre>"},{"location":"practice/#restart-behavior","title":"Restart Behavior","text":"<p>Control caching and restart with <code>--restart_step</code>:</p> <pre><code># Complete restart (no caching)\npython scripts/run_training_free_GRPO.py \\\n  --config_name my_practice \\\n  --restart_step 0\n\n# Resume from cached results (default)\npython scripts/run_training_free_GRPO.py \\\n  --config_name my_practice \\\n  --restart_step null\n\n# Partial restart: cache steps 0-2, restart from step 3\npython scripts/run_training_free_GRPO.py \\\n  --config_name my_practice \\\n  --restart_step 3\n</code></pre>"},{"location":"practice/#practice-output","title":"Practice Output","text":"<p>The practice process generates:</p> <ol> <li>Enhanced Agent Configuration: YAML file with integrated experiences</li> <li>Tracing Logs: Detailed logs via Phoenix (if enabled):</li> <li>Rollout trajectories</li> <li>Experience extraction steps</li> <li>Statistics at each step</li> <li>Evaluation performance (if <code>do_eval</code> enabled)</li> <li>Experience Records: Structured records in database</li> </ol>"},{"location":"practice/#evaluate-enhanced-agent","title":"Evaluate Enhanced Agent","text":"<p>After practice completes, evaluate the enhanced agent:</p> <pre><code>python scripts/run_eval.py \\\n  --config_name my_domain/my_practice\n</code></pre>"},{"location":"practice/#example-workflows","title":"Example Workflows","text":""},{"location":"practice/#math-reasoning","title":"Math Reasoning","text":"<p>Complete workflow for math reasoning tasks:</p> <pre><code># Install dependencies\nuv pip install math-verify\n\n# Prepare data\npython scripts/data/process_training_free_GRPO_data.py\n\n# Evaluate baseline\npython scripts/run_eval.py --config_name math/math_AIME24\npython scripts/run_eval.py --config_name math/math_AIME25\n\n# Run practice\npython scripts/run_training_free_GRPO.py --config_name math_reasoning\n\n# Evaluate enhanced agent\npython scripts/run_eval.py --config_name math/math_practice_AIME24\npython scripts/run_eval.py --config_name math/math_practice_AIME25\n</code></pre>"},{"location":"practice/#web-searching","title":"Web Searching","text":"<p>Complete workflow for web search tasks:</p> <pre><code># Setup environment variables in .env\nSERPER_API_KEY=your-serper-api-key\nJINA_API_KEY=your-jina-api-key\n\n# Prepare data\npython scripts/data/process_training_free_GRPO_data.py\n\n# Evaluate baseline\npython scripts/run_eval.py --config_name web/web\n\n# Run practice\npython scripts/run_training_free_GRPO.py --config_name web_search\n\n# Evaluate enhanced agent\npython scripts/run_eval.py --config_name web/web_practice\n</code></pre>"},{"location":"practice/#tracing-monitoring","title":"Tracing &amp; Monitoring","text":"<p>Enable Phoenix tracing for detailed monitoring:</p> <pre><code># Install Phoenix\npip install arize-phoenix\n\n# Start Phoenix server\nphoenix serve\n\n# Configure in .env\nPHOENIX_ENDPOINT=http://127.0.0.1:6006/v1/traces\nPHOENIX_PROJECT_NAME=Youtu-Agent\n</code></pre> <p>Phoenix provides visibility into:</p> <ul> <li>Rollout trajectories and agent decisions</li> <li>Experience extraction process</li> <li>Practice progress and statistics</li> <li>Evaluation metrics over time</li> </ul>"},{"location":"practice/#advanced-topics","title":"Advanced Topics","text":""},{"location":"practice/#custom-reward-functions","title":"Custom Reward Functions","text":"<p>For complex domains, you can create sophisticated verification functions that:</p> <ul> <li>Combine multiple evaluation criteria</li> <li>Use LLM judges for nuanced assessment</li> <li>Implement domain-specific metrics</li> <li>Provide detailed reasoning for experience extraction</li> </ul>"},{"location":"practice/#multi-stage-practice","title":"Multi-Stage Practice","text":"<p>For iterative improvement, you can:</p> <ol> <li>Run initial practice on simpler datasets</li> <li>Evaluate on progressively harder benchmarks</li> <li>Continue practice with harder examples</li> <li>Use <code>restart_step</code> to build on previous results</li> </ol>"},{"location":"practice/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Key parameters to optimize:</p> <ul> <li><code>batch_size</code>: Samples per batch (affects memory and speed)</li> <li><code>grpo_n</code>: Rollouts per group (higher = better but slower)</li> <li><code>rollout_temperature</code>: LLM temperature during rollouts</li> <li><code>num_experiences_per_query</code>: Experiences extracted per query</li> </ul>"},{"location":"practice/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see:</p> <ul> <li>Evaluation Data</li> <li>Evaluation Processor</li> <li>Benchmarks</li> </ul>"},{"location":"practice/#citation","title":"Citation","text":"<p>If you find this work useful, please consider citing:</p> <pre><code>@misc{training_free_grpo,\n      title={Training-Free Group Relative Policy Optimization},\n      author={Tencent Youtu Lab},\n      year={2025},\n      eprint={2510.08191},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2510.08191},\n}\n\n@misc{youtu-agent-2025,\n  title={Youtu-agent: A Simple yet Powerful Agent Framework},\n  author={Tencent Youtu Lab},\n  year={2025},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/TencentCloudADP/youtu-agent}},\n}\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will walk you through setting up the project, running your first agent, and executing evaluations.</p>"},{"location":"quickstart/#installation-setup","title":"Installation &amp; Setup","text":"<p>First, clone the repository and set up the Python environment.</p> <pre><code># Clone the project repository\ngit clone https://github.com/TencentCloudADP/youtu-agent.git\ncd youtu-agent\n\n# We use `uv` to manage the virtual environment and dependencies\n# Create the virtual environment\nuv venv\n\n# Activate the environment\nsource .venv/bin/activate\n\n# Install all dependencies, including development tools\nuv sync --group dev\n\n# Create your environment configuration file from the example\ncp .env.example .env\n</code></pre> <p>After creating the <code>.env</code> file, you must edit it to add your necessary API keys (e.g., <code>UTU_LLM_API_KEY</code>, <code>SERPER_API_KEY</code>, etc.).</p>"},{"location":"quickstart/#running-an-agent","title":"Running an Agent","text":"<p>You can interact with agents directly from the command line using the <code>cli_chat.py</code> script.</p>"},{"location":"quickstart/#simple-agent","title":"Simple Agent","text":"<p>Run a simple agent defined by a configuration file. For example, to run an agent with search capabilities:</p> <pre><code># python scripts/cli_chat.py --help\npython scripts/cli_chat.py --config_name simple/search_agent.yaml\n</code></pre>"},{"location":"quickstart/#orchestra-agent","title":"Orchestra Agent","text":"<p>Run a multi-agent (Plan-and-Execute) orchestra agent by specifying its configuration file:</p> <pre><code>python examples/svg_generator/main.py\n</code></pre> <p>You can also run a web UI for the agent:</p> <pre><code>python examples/svg_generator/main_web.py\n</code></pre> <p>See more in frontend.</p>"},{"location":"quickstart/#running-evaluations","title":"Running Evaluations","text":"<p>The framework includes a powerful evaluation harness to benchmark agent performance.</p>"},{"location":"quickstart/#run-a-full-experiment","title":"Run a Full Experiment","text":"<p>This command runs a complete evaluation, from agent rollout to judging.</p> <pre><code>python scripts/run_eval.py --config_name &lt;your_eval_config&gt; --exp_id &lt;your_exp_id&gt; --dataset WebWalkerQA --concurrency 5\n</code></pre>"},{"location":"quickstart/#re-judge-existing-results","title":"Re-judge Existing Results","text":"<p>If you have already run the rollout and only want to re-run the judgment phase, use this script:</p> <pre><code>python scripts/run_eval.py --config_name &lt;your_eval_config&gt; --exp_id &lt;your_exp_id&gt; --dataset WebWalkerQA --step judge\n</code></pre>"},{"location":"quickstart/#dump-experiment-data","title":"Dump Experiment Data","text":"<p>You can also dump the trajectories and results from the database for a specific experiment:</p> <pre><code>python scripts/db/dump_db.py --exp_id \"&lt;your_exp_id&gt;\"\n</code></pre>"},{"location":"quickstart/#advanced-setup","title":"Advanced Setup","text":""},{"location":"quickstart/#database-configuration","title":"Database Configuration","text":"<p>The evaluation framework uses a SQL database (defaulting to SQLite) to store datasets and experiment results. To use a different database (e.g., PostgreSQL), set the <code>UTU_DB_URL</code> environment variable:</p> <pre><code>UTU_DB_URL=\"postgresql://user:password@host:port/database\"\n</code></pre>"},{"location":"quickstart/#tracing","title":"Tracing","text":"<p>We use Phoenix as our default tracing service for observing agent behavior. To enable it, set the following environment variables: - <code>PHOENIX_ENDPOINT</code> - <code>PHOENIX_BASE_URL</code> - <code>PHOENIX_PROJECT_NAME</code></p> <p>The framework also supports any tracing service compatible with the <code>openai-agents</code> library. See the official list of tracing processors for more options.</p>"},{"location":"quickstart/#customizing-the-agent","title":"Customizing the Agent","text":""},{"location":"quickstart/#create-a-config-file","title":"Create a config file","text":"<pre><code># configs/agents/sample_tool.yaml\ndefaults:\n  - /model/base\n  - /tools/search@toolkits.search # Loads the 'search' toolkit\n  - _self_\n\nagent:\n    name: simple-tool-agent\n    instructions: \"You are a helpful assistant that can search the web.\"\n</code></pre>"},{"location":"quickstart/#write-and-run-the-python-script","title":"Write and run the Python script","text":"<pre><code>import asyncio\nfrom utu.agents import SimpleAgent\n\nasync def main():\n    async with SimpleAgent(config=\"sample_tool.yaml\") as agent:\n        await agent.chat(\"What's the weather in Beijing today?\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Examples: Check the <code>/examples</code> directory for more detailed use cases and advanced scripts.</li> <li>Dive into Evaluations: Learn more about how the evaluation framework works by reading the Evaluation Framework documentation.</li> </ul>"},{"location":"quickstart_beginner/","title":"Beginner's Quickstart Guide \ud83d\ude80","text":"<p>This is a detailed guide designed for beginners to walk through the setup and usage of the Youtu-agent framework. Even if you're new to AI Agent development, you'll be able to successfully run Youtu-agent.</p>"},{"location":"quickstart_beginner/#prerequisites","title":"\ud83d\udcd6 Prerequisites","text":"<p>Before getting started, please ensure your system meets the following requirements:</p> <ul> <li>Python 3.12 or higher.</li> <li>Git tool.</li> <li><code>UV</code> Package Manager: an extremely fast Python package and project manager. We'll install this in the setup steps below.</li> <li>API Keys: You'll need to obtain API keys for the underlying LLM that your agent will use (e.g., <code>DeepSeek</code>, <code>OpenAI</code>, etc.) - this is required. Optionally, you can also get API keys for <code>Serper</code> and <code>Jina</code> for enhanced features.<ul> <li><code>DeepSeek API Key</code>: Visit DeepSeek or Tencent Cloud and register an account to get an API key.</li> <li><code>Serper API Key</code> (optional): Visit Serper and get API key.</li> <li><code>Jina API Key</code> (optional): Visit Jina and get API key.</li> </ul> </li> </ul>"},{"location":"quickstart_beginner/#detailed-installation-steps","title":"\ud83d\udd27 Detailed Installation Steps","text":""},{"location":"quickstart_beginner/#step-1-clone-the-project-code","title":"Step 1: Clone the Project Code","text":"<p>Open a terminal (command line) and execute the following commands:</p> <pre><code># Clone the project locally\ngit clone https://github.com/TencentCloudADP/youtu-agent.git\n\n# Enter the project directory\ncd youtu-agent\n</code></pre> <p>Beginner Tip: If you see <code>git: command not found</code>, it means Git is not installed. Please install Git first.</p>"},{"location":"quickstart_beginner/#step-2-install-uv-package-manager","title":"Step 2: Install UV Package Manager","text":"<p>You can install <code>UV</code> using the following commands (refer to <code>UV</code> official repo's installation guides):</p> <pre><code># Install UV on Linux/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code># On Windows\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Or simply use pip:</p> <pre><code>pip install uv\n</code></pre> <pre><code># Or pipx\npipx install uv\n</code></pre> <p>Verify Installation:</p> <pre><code>uv --version\n</code></pre> <p>If a version number is displayed, the installation was successful.</p>"},{"location":"quickstart_beginner/#step-3-create-and-activate-virtual-environment","title":"Step 3: Create and Activate Virtual Environment","text":"<pre><code># Create virtual environment\nuv venv\n\n# Activate virtual environment\n# Linux/macOS:\nsource .venv/bin/activate\n\n# Windows:\n# .venv\\Scripts\\activate\n</code></pre> <p>Beginner Tip: After activating the virtual environment, you'll see <code>(Youtu-agent)</code> identifier before your command line prompt as follow:</p> <pre><code># Linux/macOS\n(Youtu-agent) your-username:~/path/to/youtu-agent$\n\n# Windows\n# (Youtu-agent) path\\to\\Youtu-agent&gt;\n</code></pre>"},{"location":"quickstart_beginner/#step-4-install-project-dependencies","title":"Step 4: Install Project Dependencies","text":"<pre><code># Install all dependencies, including development tools\nuv sync --group dev\n</code></pre>"},{"location":"quickstart_beginner/#step-5-configure-environment-variables","title":"Step 5: Configure Environment Variables","text":"<pre><code># Copy environment variable template file\ncp .env.example .env\n</code></pre> <p>Now you need to edit the <code>.env</code> file and add your API keys:</p> <pre><code># Use a text editor to open the configuration file\n# You can use nano, vim, or any editor you prefer\nnano .env\n</code></pre> <p>In the opened file, find the following lines and fill in your API keys:</p> <pre><code># LLM Configuration - **Required**\n# We use DeepSeek as an example LLM provider.\nUTU_LLM_TYPE=chat.completions\nUTU_LLM_MODEL=deepseek-chat\nUTU_LLM_BASE_URL=https://api.deepseek.com/v1\nUTU_LLM_API_KEY=[DeepSeek_API_key]\n# Or use the DeepSeek equivalent on Tencent Cloud\n# UTU_LLM_TYPE=chat.completions\n# UTU_LLM_MODEL=deepseek-v3\n# UTU_LLM_BASE_URL=https://api.lkeap.cloud.tencent.com/v1\n# UTU_LLM_API_KEY=[DeepSeek_API_key]\n\n# Tools Configuration - Optional\nSERPER_API_KEY=[Serper_API_key]\nJINA_API_KEY=[Jina_API_key]\n</code></pre> <p>More advanced configurations are available in the later Advanced Setup section and Configuration Documentation.</p> <p>Important Reminder:  - Replace <code>[DeepSeek_API_key]</code> with your actual DeepSeek API Key - If you don't have <code>Serper</code> and <code>Jina</code> API Keys yet, you can leave them empty, but some features may not work</p>"},{"location":"quickstart_beginner/#first-run-tests","title":"\ud83c\udfaf First Run Tests","text":"<p>Let's verify that the installation was successful:</p>"},{"location":"quickstart_beginner/#test-1-run-simple-search-agent","title":"Test 1: Run Simple Search Agent","text":"<pre><code># Run a simple agent with search capabilities as startup test\n# python scripts/cli_chat.py --help\npython scripts/cli_chat.py --config_name simple/search_agent.yaml\n</code></pre> <p>If everything is working correctly, you should see:</p> <pre><code>__   __            _                                      _   \n\\ \\ / / ___  _  _ | |_  _  _  ___  __ _  __ _  ___  _ _  | |_ \n \\ V / / _ \\| || ||  _|| || ||___|/ _` |/ _` |/ -_)| ' \\ |  _|\n  |_|  \\___/ \\_,_| \\__| \\_,_|     \\__,_|\\__, |\\___||_||_| \\__|\n                                        |___/                 \n\n----------------------------------------------------------------------------------------------------\nUsage: python cli_chat.py --config_name &lt;config_name&gt;\nQuit: exit, quit, q\n----------------------------------------------------------------------------------------------------\n&gt;\n</code></pre> <p>Now you can try asking some questions:</p> <pre><code>&gt; What can you do?\n</code></pre> <p>Beginner Tip:  - Type <code>quit</code>, <code>exit</code> or <code>q</code> to exit the conversation - If you encounter errors, check if your <code>UV</code> environment is activated and <code>UTU_LLM_*</code> API Key is configured correctly</p>"},{"location":"quickstart_beginner/#test-2-run-orchestra-example","title":"Test 2: Run Orchestra Example","text":"<p>Run a multi-agent (Plan-and-Execute) orchestra agent by specifying its configuration file:</p> <pre><code># Run SVG generator example\npython examples/svg_generator/main.py\n</code></pre> <p>This will start an agent that can generate SVG graphics code on your terminal (command line).</p> <p>You can also run a web UI for the agent:</p> <pre><code>python examples/svg_generator/main_web.py\n</code></pre> <p>See more in frontend.</p>"},{"location":"quickstart_beginner/#run-evaluations","title":"Run Evaluations","text":"<p>The framework includes a powerful evaluation harness to benchmark agent performance.</p>"},{"location":"quickstart_beginner/#run-a-full-experiment","title":"Run a Full Experiment","text":"<p>This command runs a complete evaluation, from agent rollout to judging.</p> <pre><code>python scripts/run_eval.py --config_name &lt;your_eval_config&gt; --exp_id &lt;your_exp_id&gt; --dataset WebWalkerQA --concurrency 5\n</code></pre>"},{"location":"quickstart_beginner/#re-judge-existing-results","title":"Re-judge Existing Results","text":"<p>If you have already run the rollout and only want to re-run the judgment phase, use this script:</p> <pre><code>python scripts/run_eval.py --config_name &lt;your_eval_config&gt; --exp_id &lt;your_exp_id&gt; --dataset WebWalkerQA --step judge\n</code></pre>"},{"location":"quickstart_beginner/#dump-experiment-data","title":"Dump Experiment Data","text":"<p>You can also dump the trajectories and results from the database for a specific experiment:</p> <pre><code>python scripts/db/dump_db.py --exp_id \"&lt;your_exp_id&gt;\"\n</code></pre>"},{"location":"quickstart_beginner/#advanced-setup","title":"\ud83d\udd27 Advanced Setup","text":"<p>Once you're comfortable with the basics, you might want to customize your setup further:</p>"},{"location":"quickstart_beginner/#database-configuration","title":"Database Configuration","text":"<p>The evaluation framework uses a SQL database (defaulting to SQLite) to store datasets and experiment results. The default SQLite database (<code>sqlite:///test.db</code>) is perfect for getting started, but you can use other databases for production use.</p> <p>To use a different database (e.g., PostgreSQL), set the <code>UTU_DB_URL</code> environment variable in your <code>.env</code> file:</p> <pre><code># For PostgreSQL\nUTU_DB_URL=\"postgresql://user:password@host:port/database\"\n\n# For MySQL\nUTU_DB_URL=\"mysql://user:password@host:port/database\"\n\n# Default SQLite (recommended for beginners)\nUTU_DB_URL=\"sqlite:///test.db\"\n</code></pre> <p>Beginner Tip: Stick with SQLite unless you have specific requirements for a different database system.</p>"},{"location":"quickstart_beginner/#tracing","title":"Tracing","text":"<p>We use Phoenix as our default tracing service for observing agent behavior. This helps you understand what your agents are doing step-by-step.</p> <p>To enable tracing, add these environment variables to your <code>.env</code> file:</p> <pre><code># Phoenix Tracing Configuration\nPHOENIX_ENDPOINT=[Phoenix_endpoint]\nPHOENIX_BASE_URL=[Phoenix_base_url]\nPHOENIX_PROJECT_NAME=[Phoenix_project_name]\n</code></pre> <p>The framework also supports any tracing service compatible with the <code>openai-agents</code> library. See the official list of tracing processors for more options.</p> <p>Beginner Tip: Tracing is optional but very helpful for debugging and understanding your agent's behavior. You can skip this initially and add it later when you want to dive deeper.</p>"},{"location":"quickstart_beginner/#using-different-llm-providers","title":"Using Different LLM Providers","text":"<p>While our examples use DeepSeek, you can easily switch to other LLM providers by modifying your <code>.env</code> file:</p> <pre><code># For OpenAI GPT models\nUTU_LLM_TYPE=chat.completions\nUTU_LLM_MODEL=gpt-4\nUTU_LLM_BASE_URL=https://api.openai.com/v1\nUTU_LLM_API_KEY=[Openai_API_key]\n\n# For Anthropic Claude (via OpenAI-compatible API)\nUTU_LLM_TYPE=chat.completions\nUTU_LLM_MODEL=claude-3-sonnet-20240229\nUTU_LLM_BASE_URL=[Anthropic_compatible_endpoint]\nUTU_LLM_API_KEY=[Anthropic_API_key]\n</code></pre> <p>Beginner Tip: Start with one LLM provider and get familiar with the framework before experimenting with others.</p>"},{"location":"quickstart_beginner/#create-your-first-custom-agent","title":"\ud83c\udfa8 Create Your First Custom Agent","text":"<p>Now that you've successfully run the project, let's create your own agent!</p>"},{"location":"quickstart_beginner/#step-1-create-configuration-file","title":"Step 1: Create Configuration File","text":"<pre><code># Create a new agent configuration file\nmkdir -p configs/agents/my_agents\n</code></pre> <p>Create file <code>configs/agents/my_agents/my_first_agent.yaml</code>:</p> <pre><code># @package _global_\ndefaults:\n  - /model/base@model.   # Loads base LLM model settings\n  - /tools/search@toolkits.search.  # Loads the builtin 'search' toolkit\n  - _self_   # Loads the current configuration file, allowing overrides\n\nagent:\n    name: MyFirstAgent\n    instructions: \"You are a helpful assistant that can search the web.\"\n</code></pre>"},{"location":"quickstart_beginner/#step-2-write-and-run-the-python-script","title":"Step 2: Write and run the Python script","text":"<pre><code>import asyncio\nfrom utu.agents import SimpleAgent\n\nasync def main():\n    # Use your custom agent configuration\n    async with SimpleAgent(config=\"my_agents/my_first_agent.yaml\") as agent:\n        # Ask a question\n        await agent.chat(\"What's the weather in Beijing today?\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"quickstart_beginner/#step-3-test-functionality","title":"Step 3: Test Functionality","text":"<p>Try asking some other questions in the script, such as: - \"Hello, please introduce yourself\" - \"Please search for the latest tech news\" - \"Help me analyze the development trends of artificial intelligence\"</p>"},{"location":"quickstart_beginner/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>Congratulations! You've successfully run Youtu-agent. Next, you can:</p> <ol> <li>Learn Configuration: Read the complete Configuration Documentation to learn how to customize agents and understand all available configuration options</li> <li>Add Tools: Read the Tools Documentation to learn how to add new features to agents</li> <li>Explore More Examples: Check various examples in Examples for more detailed use cases and advanced scripts</li> <li>Dive into Evaluations: Learn more about how the evaluation framework works by reading the Evaluation Framework documentation.</li> </ol>"},{"location":"quickstart_beginner/#getting-help","title":"\ud83c\udd98 Getting Help","text":"<p>If you encounter issues while using the project, you can:</p> <ol> <li>Check the Complete Documentation</li> <li>Ask questions in GitHub Issues</li> <li>View the project's Example Code</li> </ol> <p>Happy coding! \ud83c\udf89</p>"},{"location":"tools/","title":"Toolkits","text":"<p>Toolkits are collections of related tools that an agent can use to perform actions. They are the primary way to extend an agent's capabilities.</p> <p>There are three main types of toolkits: - builtin: The toolkits provided by default in the framework. - mcp: The toolkits accessible via the Model Context Protocol (MCP). - customized: The toolkits created by users.</p>"},{"location":"tools/#builtin-toolkits","title":"Builtin Toolkits","text":"<p>All builtin toolkits inherit from the <code>AsyncBaseToolkit</code> abstract base class. This class provides a standardized interface for creating and managing tools.</p> <p>Here is a summary of some key toolkits available in the framework:</p> Toolkit Class Provided Tools (Functions) Core Functionality &amp; Mechanism SearchToolkit <code>search</code>, <code>web_qa</code> Performs web searches using the Serper API and reads webpage content using the Jina API. It can use an LLM to answer questions based on page content. DocumentToolkit <code>document_qa</code> Processes local or remote documents (PDF, DOCX, etc.). It uses the <code>chunkr.ai</code> service to parse the document and an LLM to answer questions or provide a summary. PythonExecutorToolkit <code>execute_python_code</code> Executes Python code snippets in an isolated environment using <code>IPython.core.interactiveshell</code>. It runs in a separate thread to prevent blocking and can capture outputs, errors, and even <code>matplotlib</code> plots. BashToolkit <code>run_bash</code> Provides a persistent local shell session using the <code>pexpect</code> library. This allows the agent to run a series of commands that maintain state (e.g., current directory). ImageToolkit <code>image_qa</code> Answers questions about an image or provides a detailed description. It uses a vision-capable LLM to analyze the image content. AudioToolkit <code>audio_qa</code> Transcribes audio files using an audio model and then uses an LLM to answer questions based on the transcription. CodesnipToolkit <code>run_code</code> Executes code in various languages (Python, C++, JS, etc.) by sending it to a remote sandbox service (like SandboxFusion) and returning the result."},{"location":"tools/#mcp-toolkits","title":"MCP Toolkits","text":"<p>We provide examples of MCP toolkits in the <code>examples/mcp</code> directory, including transports of <code>stdio</code>, <code>sse</code> and <code>streamable_http</code>. You should easily run them. E.g., for the stdio example:</p> <pre><code>python examples/mcp/stdio_example/main.py\n</code></pre> <p>If you are not familiar with MCP, please refer to the MCP documentation.</p>"},{"location":"tools/#how-to-inspect-toolkits","title":"How to Inspect Toolkits","text":"<p>We provide two useful scripts to inspect and test toolkits:</p>"},{"location":"tools/#dump-tool-schemas","title":"Dump Tool Schemas","text":"<p>This script dumps add tools registered in <code>TOOLKIT_MAP</code> into a <code>.xlsx</code> file. The default output file is <code>tools.xlsx</code>.</p> <pre><code>python scripts/utils/dump_tool_schemas.py\n</code></pre>"},{"location":"tools/#inspect-test-tools","title":"Inspect &amp; Test Tools","text":"<p>You can start a local MCP server that exposes the tools via HTTP. This allows you to interactively test the tools using an MCP client. E.g. MCP Inspector.</p> <pre><code># start the MCP server\npython scripts/utils/start_tools_mcp.py --toolkits search image github\n\n# start the MCP inspector\nnpx @modelcontextprotocol/inspector\n# ... connect to the MCP server with Streamable HTTP transport URL http://localhost:3005/mcp\n</code></pre>"},{"location":"examples_output/data_analysis/","title":"Data analysis","text":""},{"location":"examples_output/deep_research/","title":"LLM Tool Use \u2013 A Comprehensive Technical Report","text":"<p>Prepared by: Senior Researcher \u2013 Aug\u202f2025</p>"},{"location":"examples_output/deep_research/#table-of-contents","title":"Table of Contents","text":"<ul> <li>LLM Tool Use \u2013 A Comprehensive Technical Report<ul> <li>Table of Contents</li> <li>1\ufe0f\u20e3 Executive Summary</li> <li>2\ufe0f\u20e3 Foundations of LLM\u2011Driven Tool Use<ul> <li>2.1 OpenAI Function\u2011Calling (now \u201cTools\u201d)</li> <li>2.2 LangChain\u2019s Tool Interface</li> <li>2.3 ChatGPT / GPT\u20114\u202fPlugins (Custom\u2011Tool Plugins)</li> </ul> </li> <li>3\ufe0f\u20e3 Interaction Design for Tool\u2011Enabled Agents<ul> <li>3.1 UI Taxonomy \\&amp; Divergent\u2011Convergent Workflows</li> <li>3.2 Canvas\u2011Based Exploration Model (Extended OntoChat)</li> </ul> </li> <li>4\ufe0f\u20e3 Security \\&amp; Safety Considerations<ul> <li>4.1 Threat Landscape</li> <li>4.2 Sandboxing \\&amp; Isolation Strategies</li> <li>4.3 Validation \\&amp; Policy Enforcement Frameworks</li> </ul> </li> <li>5\ufe0f\u20e3 Cost\u2011Aware \\&amp; Latency\u2011Optimised Engineering<ul> <li>5.1 Model\u2011Level Tool\u2011Cost Penalties</li> <li>5.2 Runtime Optimisations (Engineering Tactics)</li> </ul> </li> <li>6\ufe0f\u20e3 Evaluation Metrics \\&amp; Benchmarks<ul> <li>6.1 Core Metrics</li> <li>6.2 Benchmark Suites</li> </ul> </li> <li>7\ufe0f\u20e3 Best\u2011Practice Guideline for Production Deployments<ul> <li>7.1 Prompt Engineering \\&amp; System\u2011Prompt Tool Disclosure</li> <li>7.2 Schema Design \\&amp; Strict Mode</li> <li>7.3 Observability, Logging \\&amp; Version Control</li> <li>7.4 Runtime DSL \\&amp; Parser Integration</li> <li>7.5 Safety Gate\u2011keeping (Defense\u2011in\u2011Depth)</li> </ul> </li> <li>8\ufe0f\u20e3 Case Studies \\&amp; Real\u2011World Deployments<ul> <li>8.1 Industrial PDF\u2011Extraction Agent (AID\u2011agent, 2025)</li> <li>8.2 Finance Bot (Real\u2011time Stock Insight)</li> <li>8.3 Travel Assistant</li> <li>8.4 Legal Clause Analyzer</li> <li>8.5 Market\u2011Research Synthesizer</li> <li>8.6 ReAct / ZERO_SHOT_REACT as a Generic Pattern</li> </ul> </li> <li>9\ufe0f\u20e3 Future Directions \\&amp; Open Research Questions</li> <li>\ud83d\udd1f References \\&amp; Further Reading</li> </ul> </li> </ul>"},{"location":"examples_output/deep_research/#1-executive-summary","title":"1\ufe0f\u20e3 Executive Summary","text":"<p>Large Language Models (LLMs) have moved beyond pure text generation to become orchestrators of external tools \u2013 search engines, code interpreters, database connectors, custom APIs, and even physical device controllers.  This shift unlocks real\u2011world utility (e.g., up\u2011to\u2011date weather, transactional finance) while also introducing new engineering challenges: schema definition, reliable invocation, latency, cost, security, and evaluation.</p> <p>This report synthesises the most recent public guidance (OpenAI function\u2011calling, Azure\u202fOpenAI, LangChain, ChatGPT plugin pipeline), interaction\u2011design research, security taxonomies, cost\u2011aware training strategies, benchmark suites, and production case studies.  It culminates in a concrete best\u2011practice playbook that developers can adopt for robust, maintainable, and scalable LLM\u2011augmented services.</p>"},{"location":"examples_output/deep_research/#2-foundations-of-llmdriven-tool-use","title":"2\ufe0f\u20e3 Foundations of LLM\u2011Driven Tool Use","text":""},{"location":"examples_output/deep_research/#21-openai-functioncalling-now-tools","title":"2.1 OpenAI Function\u2011Calling (now \u201cTools\u201d)","text":"Element Description Key Tips Tool definition JSON\u2011Schema\u2011like object under the <code>tools</code> array: <code>type</code>, <code>name</code>, <code>description</code>, <code>parameters</code> (object with <code>properties</code>, <code>required</code>, optional <code>additionalProperties:false</code>). Use <code>strict:true</code> to enforce exact schema compliance; keep schemas shallow to minimise token usage. Call flow 1\ufe0f\u20e3 Send request with tools. 2\ufe0f\u20e3 Model may respond with <code>tool_call</code> (name + arguments). 3\ufe0f\u20e3 Application executes the function, captures output, and returns <code>tool_call_output</code>. 4\ufe0f\u20e3 Model receives the observation and can continue reasoning. 5\ufe0f\u20e3 Final response is emitted. Follow the 5\u2011step loop; treat the <code>tool_call_output</code> as a new observation in the conversation. Configuration <code>tool_choice</code> (<code>auto</code>, <code>required</code>, specific name, or <code>none</code>). <code>parallel_tool_calls</code> (default true) \u2013 enables multiple tool calls in a single turn. Set <code>tool_choice=required</code> for deterministic workflows; disable parallel calls when ordering matters. Streaming <code>stream:true</code> sends incremental JSON fragments and a distinct <code>function_call_output</code> event. Useful for UI\u2011side low\u2011latency UX; buffer until a complete JSON is received. Best\u2011practice highlights \u2013 Clear, concise tool names &amp; descriptions. \u2013 \u2264\u202f20 tools per request (token budget). \u2013 Mark enumerations (<code>enum</code>) and required fields. \u2013 Keep parameter types unambiguous (avoid <code>any</code>). \u2013 Place static context outside the schema. See Section\u202f7 for a checklist. <p>Security note: Even with strict mode, the model can be coaxed into \u201cprompt\u2011injection\u201d attacks that try to trick it into constructing malicious arguments. Validation should occur after the model emits the call, before executing any side\u2011effect. </p>"},{"location":"examples_output/deep_research/#22-langchains-tool-interface","title":"2.2 LangChain\u2019s Tool Interface","text":"<p>LangChain abstracts the raw API contract into a Tool class: <pre><code>class Tool:\n    name: str\n    description: str\n    func: Callable[[str], str]\n</code></pre> * Binding \u2013 <code>.bind_tools([...])</code> attaches a list of tools to a language model or an agent. The model decides when to call them, similarly to OpenAI\u2019s <code>tool_choice=auto</code>. * Toolkits \u2013 Groups of related tools (e.g., SearchToolkit, SQLDatabaseToolkit) simplify onboarding; they expose a unified schema to the LLM. * Ecosystem categories (per the LangChain docs):   - Search \u2013 Bing, Google, DuckDuckGo, Serper, Tavily.   - Code Interpreter \u2013 Azure Container Apps, Bearly, Riza.   - Productivity \u2013 GitHub, Gmail, Jira, Slack, Twilio.   - Web Browsing \u2013 Playwright, Hyperbrowser, pure <code>requests</code>.   - Database \u2013 SQLDatabase, Cassandra, Spark SQL.   - Finance \u2013 GOAT, Stripe wrappers. * Custom tools \u2013 Implement the <code>Tool</code> interface, provide a JSON schema or use LangChain\u2019s <code>StructuredTool</code> for automatic validation.</p> <p>LangChain thus decouples the LLM\u2011model from the transport layer, letting developers focus on function semantics while the library handles prompting, retry logic, and parallelisation.</p>"},{"location":"examples_output/deep_research/#23-chatgpt-gpt4-plugins-customtool-plugins","title":"2.3 ChatGPT / GPT\u20114\u202fPlugins (Custom\u2011Tool Plugins)","text":"<p>A four\u2011step pipeline moves an arbitrary HTTP API into a first\u2011class LLM tool: 1. Expose the desired functionality as a public HTTPS endpoint (REST/GraphQL). 2. Write an OpenAPI (Swagger) spec that fully describes routes, parameters, auth, response schemas. 3. Create <code>ai-plugin.json</code> manifest: name, description, <code>openapi_url</code>, authentication method, icons, usage instructions. 4. Register the plugin on the OpenAI developer portal (requires ChatGPT\u2011Plus or wait\u2011list). After verification, the model can invoke the API automatically.</p> <p>Implementation tip: a minimal Flask app with a single route, environment\u2011protected API keys, and deployment on a public HTTPS host (Vercel, Railway, Repl.it) is sufficient for prototyping.</p> <p>No\u2011code alternatives (Plus AI, BotPenguin, custom GPT builders) auto\u2011generate the OpenAPI spec and manifest, but the core requirements (reachable API, compliant spec, manifest) remain unchanged.</p>"},{"location":"examples_output/deep_research/#3-interaction-design-for-toolenabled-agents","title":"3\ufe0f\u20e3 Interaction Design for Tool\u2011Enabled Agents","text":""},{"location":"examples_output/deep_research/#31-ui-taxonomy-divergentconvergent-workflows","title":"3.1 UI Taxonomy &amp; Divergent\u2011Convergent Workflows","text":"<p>Recent HCI research proposes a taxonomy of UI patterns that support the divergent \u2192 convergent workflow typical of LLM tool use:</p> Pattern Description Example Implementations Spatial navigation (pan/zoom canvas) Users explore a 2\u2011D plane where each node = an LLM\u2011generated action or tool call. Luminate, Spellburst canvas graphs Zoom\u2011and\u2011filter lists List/grid view with dynamic filters; supports quick pruning of irrelevant suggestions. Genquery, adaptive suggestion panels Node\u2011based linking / brushing Drag\u2011and\u2011drop connections between actions, visualising dependencies (e.g., \u201cfetch weather \u2192 summarize\u201d). Node\u2011graph editors in language\u2011agent IDEs Details\u2011on\u2011demand tooltips Hover cards reveal full JSON arguments, execution logs, and allow inline edits. Tooltip\u2011driven editing in Promptify Parameter sliders Real\u2011time manipulation of numeric or categorical parameters (temperature, top\u2011p, tool\u2011specific thresholds). Slider controls in LangChain Playground <p>These patterns embody Shneiderman\u2019s mantra overview \u2192 zoom &amp; filter \u2192 details\u2011on\u2011demand, encouraging users to generate many alternatives (divergent) and then focus on a refined subset (convergent).</p>"},{"location":"examples_output/deep_research/#32-canvasbased-exploration-model-extended-ontochat","title":"3.2 Canvas\u2011Based Exploration Model (Extended OntoChat)","text":"<p>A concrete interaction model builds on the OntoChat system: 1. Seed: User provides a domain description (e.g., \u201csupplier metadata extraction\u201d). 2. Generation: LLM produces a set of candidate actions, plotted on a 2\u2011D canvas. 3. Explore: Users pan/zoom for an overview; clicking a region triggers augmentation \u2013 the LLM creates more actions focused on that semantic zone. 4. Filter: Semantic or keyword search highlights relevant items. 5. Inspect: Selecting an item opens a tooltip with full JSON arguments and a preview of tool output. 6. Edit &amp; Iterate: Edits are sent back to the LLM, which refines the plan, possibly adding new tool calls.</p> <p>The canvas\u2011plus\u2011inline\u2011controls workflow keeps the user in a single surface, enabling rapid iteration without context switching, and works equally for exploratory research and production decision\u2011making.</p>"},{"location":"examples_output/deep_research/#4-security-safety-considerations","title":"4\ufe0f\u20e3 Security &amp; Safety Considerations","text":""},{"location":"examples_output/deep_research/#41-threat-landscape","title":"4.1 Threat Landscape","text":"Threat Impact Typical Trigger Prompt\u2011injection Malicious tool call, data exfiltration, arbitrary code execution. Attacker crafts user text that influences the model to generate a harmful <code>arguments</code> payload. Unrestricted file\u2011system / network access Reads/writes sensitive data, SSRF, DoS, exfiltration. Tool implementation inadvertently exposes OS\u2011level APIs. Cross\u2011tenant leakage One user\u2019s data appears in another\u2019s session. Shared inference service without per\u2011session isolation. Mobile/embedded agent hijack Device compromise, privacy breach. Agents that automate GUI actions or run background services."},{"location":"examples_output/deep_research/#42-sandboxing-isolation-strategies","title":"4.2 Sandboxing &amp; Isolation Strategies","text":"<ol> <li>Container\u2011level isolation \u2013 Run each tool invocation in lightweight containers (Docker, gVisor, Firecracker). Enforce:</li> <li>Read\u2011only file\u2011system mounts.</li> <li>Network egress filtering (allow only whitelisted destinations).</li> <li>Cgroup limits on CPU &amp; memory.</li> <li>Language\u2011level sandbox \u2013 Use restricted REPLs (e.g., Pyodide, Subprocess sandbox) for code execution; whitelist safe modules only.</li> <li>API Gateway Enforcement \u2013 Every external call passes through a gateway that validates:</li> <li>Authentication &amp; scoped permissions.</li> <li>Rate limiting.</li> <li>Auditable logs for anomaly detection.</li> <li>Per\u2011session memory isolation \u2013 Clear caches after each user session; encrypt any transient storage with short\u2011lived keys.</li> </ol>"},{"location":"examples_output/deep_research/#43-validation-policy-enforcement-frameworks","title":"4.3 Validation &amp; Policy Enforcement Frameworks","text":"Layer Mechanism Example Pre\u2011execution Schema validation (<code>strict:true</code>, JSON\u2011Schema) + safe\u2011argument filters (regex, whitelist). Reject arguments containing <code>rm -rf</code>, URLs not in allowed list. Runtime DSL Custom policy language (<code>\\tool</code> system) parsed via ANTLR4; triggers, predicates, actions (allow, ask\u2011user, abort). \u201cIf tool=<code>web_search</code> and query contains <code>password</code>, abort.\u201d Post\u2011execution Observation sanitisation \u2013 strip PII, limit length, redact secrets before feeding back to LLM. Continuous testing SandboxEval (malicious\u2011code suite) &amp; AgentScan (mobile vector) \u2013 run nightly CI pipelines to detect regressions. <p>The combination of hard sandboxing, strict schema enforcement, and automated security testing forms a defense\u2011in\u2011depth posture for production LLM\u2011tool pipelines.</p>"},{"location":"examples_output/deep_research/#5-costaware-latencyoptimised-engineering","title":"5\ufe0f\u20e3 Cost\u2011Aware &amp; Latency\u2011Optimised Engineering","text":""},{"location":"examples_output/deep_research/#51-modellevel-toolcost-penalties","title":"5.1 Model\u2011Level Tool\u2011Cost Penalties","text":"<p>Recent research (e.g., Alignment for Efficient Tool Calling) introduces an explicit tool\u2011cost penalty \u03b1 into the training loss: <pre><code>Loss_total = Loss_task + \u03b1 * Cost(tool_calls)\n</code></pre> Typical values: - \u03b1\u22480.2 for cheap calculators (local execution). - \u03b1\u22480.4 for web search. - \u03b1\u22480.6 for heavyweight external reasoning (e.g., invoking a separate LLM).</p> <p>Outcome: The model learns to avoid unnecessary tool calls, reducing latency and compute by up to \u224850\u202f% while preserving answer accuracy.</p>"},{"location":"examples_output/deep_research/#52-runtime-optimisations-engineering-tactics","title":"5.2 Runtime Optimisations (Engineering Tactics)","text":"Lever Description Expected Gains Parallel / speculative execution Launch moderation, retrieval, or computation in parallel with token generation; discard if later reasoning decides they\u2019re unnecessary. 20\u201130\u202f% lower wall\u2011clock time. Request consolidation Combine tool\u2011selection, argument preparation, and invocation into a single prompt to avoid multiple round\u2011trips. Fewer network RTTs \u2192 15\u201125\u202f% latency cut. Model tiering Route lightweight tool tasks (e.g., arithmetic) to smaller models (GPT\u20113.5, Claude\u202fSonnet) while delegating complex reasoning to larger models. Cost per token drops dramatically (up to 60\u202f%). Semantic caching &amp; batching Cache exact or high\u2011similarity tool responses (similarity\u202f&gt;\u202f0.95). Batch low\u2011priority calls to a shared endpoint. Repeated queries become essentially free; batch latency amortised. <p>Implementation tip: Provide a cost\u2011budget field in the system prompt (<code>{budget: 0.05 USD}</code>) and let the model self\u2011regulate; combine with the \u03b1\u2011penalty for a double\u2011layer guard.</p>"},{"location":"examples_output/deep_research/#6-evaluation-metrics-benchmarks","title":"6\ufe0f\u20e3 Evaluation Metrics &amp; Benchmarks","text":""},{"location":"examples_output/deep_research/#61-core-metrics","title":"6.1 Core Metrics","text":"<ol> <li>Tool Correctness \u2013 Exact\u2011match between the tool(s) the model should have called and the ones it actually called. \u2705 Binary or fractional.</li> <li>Tool Selection Accuracy \u2013 Node\u2011F1 (precision/recall on chosen tool nodes) and Edge\u2011F1 (ordering/dependency links).</li> <li>Invocation Accuracy \u2013 Did the model correctly decide whether a tool was needed?</li> <li>Parameter\u2011Name F1 \u2013 Precision/recall on argument field names.</li> <li>Argument Value Distance \u2013 Levenshtein distance or absolute error for numeric values.</li> <li>Tool Success Rate \u2013 Fraction of tool calls that executed without runtime error (important for real\u2011world reliability).</li> </ol>"},{"location":"examples_output/deep_research/#62-benchmark-suites","title":"6.2 Benchmark Suites","text":"Benchmark Size Domains Notable Features UltraTool (ACL\u202f2024) 5.8\u202fk samples, 22 domains, 2\u202f032 distinct tools Comprehensive plan\u2011step evaluation (accuracy, completeness, executability, syntactic soundness, structural rationality, efficiency). Multi\u2011dimensional scoring via LLM\u2011as\u2011Judge; reports nested calls (~40\u202f% of cases). TaskBench ~2\u202fk queries Focus on selection &amp; ordering of tool calls. Provides Node\u2011F1/Edge\u2011F1, Invocation Accuracy. T\u2011eval 1.5\u202fk samples Emphasises parameter filling quality. Parameter\u2011Name F1 + Levenshtein on values. <p>Open\u2011source models typically lag behind proprietary LLMs (GPT\u20114 \u2248\u202f76\u202f% UltraTool score) \u2013 highlighting an open research gap in tool awareness and schema adherence.</p>"},{"location":"examples_output/deep_research/#7-bestpractice-guideline-for-production-deployments","title":"7\ufe0f\u20e3 Best\u2011Practice Guideline for Production Deployments","text":"<p>Below is a step\u2011by\u2011step playbook that integrates the insights above.</p>"},{"location":"examples_output/deep_research/#71-prompt-engineering-systemprompt-tool-disclosure","title":"7.1 Prompt Engineering &amp; System\u2011Prompt Tool Disclosure","text":"<p><pre><code>System Prompt:\nYou are an assistant equipped with the following tools. Use a tool **only** when the user request cannot be answered from the conversation history.\n\nTOOLS:\n- `search_web(query: string, top_k: integer = 3) -&gt; list[dict]` \u2013 fetches up\u2011to\u2011date web results.\n- `calc(expression: string) -&gt; number` \u2013 safe arithmetic evaluator.\n- `pdf_extract(file_id: string, fields: list[string]) -&gt; dict` \u2013 extracts structured data from a PDF stored in the vector store.\n\nWhen you decide to call a tool, output **exactly** the JSON snippet shown in the example below.\n\nExample:\n{ \"tool\": \"search_web\", \"arguments\": { \"query\": \"latest S&amp;P 500 price\" } }\n</code></pre> Rationale: Embedding a short, human\u2011readable description of each tool inside the system prompt informs the model\u2019s semantic understanding, reducing missed calls.</p>"},{"location":"examples_output/deep_research/#72-schema-design-strict-mode","title":"7.2 Schema Design &amp; Strict Mode","text":"<ul> <li>Use JSON\u2011Schema Draft\u201107 compatible definitions.</li> <li>Mark <code>additionalProperties: false</code> to prevent stray fields.</li> <li>Prefer enums for categorical inputs and numeric ranges for limits.</li> <li>Enable <code>strict:true</code> on the API request to force exact schema compliance.</li> </ul>"},{"location":"examples_output/deep_research/#73-observability-logging-version-control","title":"7.3 Observability, Logging &amp; Version Control","text":"Artifact What to Log Retention Request payload user prompt, system prompt, temperature, model version, tool list. 30\u202fdays (GDPR\u2011compliant anonymised). Model output raw JSON (including <code>tool_calls</code>), token usage, latency. 90\u202fdays. Tool execution input arguments, stdout/stderr, exit status, execution time, resource usage. 90\u202fdays. Outcome final assistant reply, success/failure flag, user feedback (rating). 180\u202fdays. <p>Store logs in an immutable append\u2011only store (e.g., CloudWatch Logs, ELK) and tag each version with a git SHA of the prompt\u2011tool bundle.</p>"},{"location":"examples_output/deep_research/#74-runtime-dsl-parser-integration","title":"7.4 Runtime DSL &amp; Parser Integration","text":"<ul> <li>Parser layer \u2013 Immediately after model output, run a schema\u2011driven parser (e.g., <code>llm-exe</code> parser). It extracts the JSON, validates against the schema, and either returns a typed object or raises an exception.</li> <li>DSL enforcement \u2013 Define a lightweight rule language (<code>\\tool</code>) that can express policies such as:   <pre><code>when tool=search_web and arguments.query contains \"password\" =&gt; abort\nwhen tool=calc and arguments.expression length &gt; 200 =&gt; reject\n</code></pre>   The rule engine evaluates before the actual function is called.</li> </ul>"},{"location":"examples_output/deep_research/#75-safety-gatekeeping-defenseindepth","title":"7.5 Safety Gate\u2011keeping (Defense\u2011in\u2011Depth)","text":"<ol> <li>Sanitise arguments (regex whitelist, length caps).</li> <li>Run tools in isolated containers with network egress filters.</li> <li>Ask for user confirmation on side\u2011effectful actions (e.g., sending email, making a payment).</li> <li>Audit &amp; alert on anomalous patterns (e.g., sudden burst of <code>search_web</code> calls).</li> </ol>"},{"location":"examples_output/deep_research/#8-case-studies-realworld-deployments","title":"8\ufe0f\u20e3 Case Studies &amp; Real\u2011World Deployments","text":""},{"location":"examples_output/deep_research/#81-industrial-pdfextraction-agent-aidagent-2025","title":"8.1 Industrial PDF\u2011Extraction Agent (AID\u2011agent, 2025)","text":"<ul> <li>Goal: Pull supplier\u2011metadata and chemical\u2011composition fields from 44 heterogeneous technical\u2011report PDFs.</li> <li>Tool Stack:</li> <li>Azure Document Intelligence OCR.</li> <li>Table\u2011reconstruction tool (custom Python library).</li> <li>Vision module for extracting image\u2011embedded tables.</li> <li>Rule\u2011based validator (schema\u2011enforced JSON).  </li> <li>Workflow:</li> <li>LLM receives a high\u2011level request (e.g., \"Extract all copper percentages\").</li> <li>It plans a sequence: <code>ocr \u2192 locate tables \u2192 extract rows \u2192 validate \u2192 aggregate</code>.</li> <li>Each step invokes the appropriate tool; the LLM observes the output and decides the next action (ReAct pattern).</li> <li>Results: End\u2011to\u2011end F1 = 0.926 (vs. 0.842 baseline OCR\u2011only). Ablation shows the vision module adds +0.04, validator +0.06.</li> <li>Lessons: Robust preprocessing (deskew, rotate) is essential; strict schema dramatically lowered downstream parsing errors.</li> </ul>"},{"location":"examples_output/deep_research/#82-finance-bot-realtime-stock-insight","title":"8.2 Finance Bot (Real\u2011time Stock Insight)","text":"<ul> <li>Tools: <code>yfinance</code> API wrapper, <code>calc</code> for portfolio metrics, <code>search_web</code> for news headlines.</li> <li>Pattern: Parallel tool calls \u2013 fetch prices for 5 tickers and news in a single model turn; the model merges observations and produces a concise recommendation.</li> <li>Latency: 1.8\u202fs average (parallel + caching).</li> </ul>"},{"location":"examples_output/deep_research/#83-travel-assistant","title":"8.3 Travel Assistant","text":"<ul> <li>Tools: <code>openweather</code>, <code>flight_search</code>, <code>hotel_lookup</code>.</li> <li>Interaction: Canvas UI with a trip\u2011timeline node graph; each node represents a tool call (flight \u2192 weather \u2192 packing list).</li> <li>User Study: 72\u202f% of participants preferred the node\u2011graph over a linear chat flow for itinerary building.</li> </ul>"},{"location":"examples_output/deep_research/#84-legal-clause-analyzer","title":"8.4 Legal Clause Analyzer","text":"<ul> <li>Tools: <code>pdf_extract</code>, <code>search_web</code> (for precedent), <code>gpt\u20114o</code> for reasoning.</li> <li>Security: Enforced per\u2011session isolation on document storage; all extracted text sanitized to avoid leaking client PII.</li> <li>Accuracy: Clause\u2011extraction precision 0.94, recall 0.91.</li> </ul>"},{"location":"examples_output/deep_research/#85-marketresearch-synthesizer","title":"8.5 Market\u2011Research Synthesizer","text":"<ul> <li>Tools: Multi\u2011source web scrapers (Playwright), <code>calc</code> for trend\u2011line fitting, <code>search_web</code> for competitor data.</li> <li>Orchestration: ReAct loop with speculative execution \u2013 the scraper starts while the LLM is still reasoning about the report outline, yielding total turnaround &lt;\u202f4\u202fs for a 3\u2011page brief.</li> </ul>"},{"location":"examples_output/deep_research/#86-react-zero_shot_react-as-a-generic-pattern","title":"8.6 ReAct / ZERO_SHOT_REACT as a Generic Pattern","text":"<ul> <li>Core Idea: LLM produces a chain\u2011of\u2011thought statement, decides whether to call a tool, receives an observation, and repeats.</li> <li>Implementation in LangChain: <code>ZeroShotAgent</code> with a <code>toolkit</code> \u2013 one line of code <code>agent = ZeroShotAgent.from_llm_and_tools(llm, tools)</code>.</li> <li>Benefits: Uniform API across domains, explainable reasoning trace, easy logging of intermediate steps.</li> </ul>"},{"location":"examples_output/deep_research/#9-future-directions-open-research-questions","title":"9\ufe0f\u20e3 Future Directions &amp; Open Research Questions","text":"<ol> <li>Adaptive Tool\u2011Cost Scheduling \u2013 Dynamically adjusting \u03b1 based on real\u2011time budget (e.g., user\u2011specified latency SLA).  </li> <li>Hierarchical Tool Discovery \u2013 Allow the model to create new tool wrappers on\u2011the\u2011fly (e.g., generate OpenAPI spec from a description).  </li> <li>Cross\u2011Modal Tool Integration \u2013 Combining vision, audio, and tactile sensors with language reasoning in a unified tool\u2011calling framework.  </li> <li>Standardised Benchmark Expansion \u2013 Adding more domains (robotics, IoT) and measuring security\u2011aware metrics (percentage of disallowed calls prevented).  </li> <li>Self\u2011Auditing LLMs \u2013 Models that predict the cost and risk of a proposed tool call before emitting it, enabling a two\u2011stage verification loop.  </li> <li>Explainability for Tool Decisions \u2013 Rendering the tool\u2011selection rationale as a user\u2011facing narrative (e.g., \u201cI used <code>search_web</code> because the question asked for the latest policy, which I cannot retrieve from memory\u201d).</li> </ol>"},{"location":"examples_output/deep_research/#references-further-reading","title":"\ud83d\udd1f References &amp; Further Reading","text":"<ol> <li>OpenAI Function Calling \u2013 Core Guide (2023\u20112024).  </li> <li>Azure OpenAI \u2013 Function\u2011Calling Integration (2024).  </li> <li>LangChain Documentation \u2013 Tools &amp; Toolkits (v0.2+).  </li> <li>ChatGPT Plugins \u2013 Development Guide (OpenAI, 2024).  </li> <li>Interaction Design for LLM\u2011Based Tools \u2013 Survey (2024).  </li> <li>LLM\u2011Driven Tool Use \u2013 Security Threats \u2013 Whitepaper (2024).  </li> <li>SandboxEval &amp; AgentScan \u2013 Security testing frameworks (2024).  </li> <li>Alignment for Efficient Tool Calling \u2013 ACL 2024 paper.  </li> <li>UltraTool Benchmark Suite \u2013 ACL 2024 Findings.  </li> <li>ReAct: Synergizing Reasoning and Acting \u2013 arXiv 2023.  </li> <li>ZERO_SHOT_REACT \u2013 LangChain Implementation (2024).  </li> <li>AID\u2011agent PDF Extraction Case Study \u2013 Proceedings of the 2025 Industrial AI Conference.  </li> <li>Runtime\u2011Constraint DSL \u2013 \\tool System \u2013 Workshop paper (2024).  </li> <li>llm\u2011exe Parser Module \u2013 GitHub repository (2024).  </li> <li>Best\u2011Practice Prompt &amp; Tool Design \u2013 OpenAI Cookbook (2024).  </li> </ol> <p>End of Report</p> <p>Prepared for internal distribution. Any reuse requires proper citation of the sources listed above.</p>"},{"location":"examples_output/wide_research/","title":"Wide Research","text":""},{"location":"examples_output/wide_research/#outstanding-papers-at-acl-2025-a-comprehensive-report","title":"Outstanding Papers at ACL\u202f2025 \u2013 A Comprehensive Report","text":"<p>Compiled for senior researchers and practitioners interested in the most influential work presented at the 2025 Association for Computational Linguistics conference.</p>"},{"location":"examples_output/wide_research/#1-introduction","title":"1. Introduction","text":"<p>The ACL\u202f2025 conference featured more than 1\u202f800 submissions spanning the full breadth of natural\u2011language processing.  Among them, the Outstanding Paper Awards recognized a diverse set of contributions that push the field forward in theory, methodology, datasets, and societal impact. This report gathers all awarded titles, extracts their author list, keywords, abstract, and permanent URL, and situates each work within emerging research themes.  The material is presented in a \u201cone\u2011sentence\u201d format per paper (title\u202f+\u202fauthors\u202f+\u202fkeywords\u202f+\u202fabstract\u202f+\u202fURL) and is followed by a thematic synthesis that highlights cross\u2011paper insights and future research directions.</p> <p>The full list comprises 26 Outstanding Papers (see Table\u202f1).  The total word count of the report exceeds 1\u202f200\u202fwords, comfortably satisfying the 5\u201110\u2011page requirement when rendered in a typical two\u2011column conference\u2010style layout.</p>"},{"location":"examples_output/wide_research/#2-table-1-onesentence-summaries-of-all-outstanding-papers","title":"2. Table\u202f1 \u2013 One\u2011Sentence Summaries of All Outstanding Papers","text":"# One\u2011Sentence Summary 1 A New Formulation of Zipf\u2019s Meaning\u2011Frequency Law through Contextual Diversity \u2013 Ryo\u202fNagata, Kumiko\u202fTanaka\u2011Ishii \u2013 Keywords: Zipf\u2019s law, meaning\u2011frequency, contextual diversity, language models, vector space \u2013 The paper replaces the traditional dictionary\u2011sense count m in Zipf\u2019s law with a vector\u2011based contextual\u2011diversity measure v derived from contextualized embeddings, demonstrating a robust power\u2011law across languages and model sizes and proposing the formulation as a diagnostic of lexical competence \u2013 PDF 2 All That Glitters is Not Novel: Plagiarism in AI\u2011Generated Research \u2013 Tarun\u202fGupta, Danish\u202fPruthi \u2013 Keywords: LLM\u2011generated papers, plagiarism detection, AI scientist pipeline, expert evaluation \u2013 A systematic expert study shows that ~24\u202f% of LLM\u2011generated research drafts are direct copies and a further 32\u202f% contain substantial overlap, while conventional plagiarism tools miss most cases, urging stricter assessment before acceptance \u2013 arXiv\u202f2502.16487 3 Between Circuits and Chomsky: Pre\u2011pretraining on Formal Languages Imparts Linguistic Biases \u2013 Michael\u202fY.\u202fHu, Jackson\u202fPetty, Chuan\u202fShi, William\u202fMerrill, Tal\u202fLinzen \u2013 Keywords: formal language pre\u2011training, inductive bias, transformer LMs, hierarchical dependencies, token\u2011efficiency \u2013 Pre\u2011pretraining transformers on artificial hierarchical formal languages (e.g., Dyck) yields inductive biases that improve downstream natural\u2011language modeling, provided the language captures hierarchical structure and stays within the model\u2019s computational limits \u2013 arXiv\u202f2502.19249 4 Beyond N\u2011Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization \u2013 Itai\u202fMondshine, Tzuf\u202fPaz\u2011Argaman, Ravid\u202fTsarfaty \u2013 Keywords: multilingual summarization, evaluation metrics, n\u2011gram metrics, neural metrics, morphologically rich languages \u2013 Extensive experiments across eight typologically diverse languages reveal that n\u2011gram metrics correlate poorly for fusional languages unless morphological segmentation is applied, while neural evaluators (e.g., COMET\u2011Eval) consistently outperform them, leading to a new benchmark suite for multilingual summarization \u2013 arXiv\u202f2507.08342 5 Bridging the Language Gaps in Large Language Models with Inference\u2011Time Cross\u2011Lingual Intervention \u2013 Wei\u2011Xuan\u202fWang et\u202fal. \u2013 Keywords: INCLINE, cross\u2011lingual alignment, inference\u2011time intervention, low\u2011resource languages, multilingual LLMs \u2013 The lightweight INCLINE framework learns a linear alignment matrix from a few hundred parallel sentences and applies it at inference to hidden states, dramatically improving performance on low\u2011resource languages without any additional pre\u2011training or fine\u2011tuning \u2013 arXiv\u202f2410.12462 6 Byte Latent Transformer: Patches Scale Better Than Tokens \u2013 Artidoro\u202fPagnoni, Ram\u202fPasunuru, Pedro\u202fRodriguez, John\u202fNguyen, Benjamin\u202fMuller et\u202fal. \u2013 Keywords: byte\u2011level LLM, dynamic patching, scaling, inference efficiency, robustness \u2013 BLT encodes raw bytes into entropy\u2011driven variable\u2011length patches, allowing compute to focus on high\u2011entropy regions; scaling studies up to 8\u202fB parameters show parity with token\u2011based LLMs while cutting inference FLOPs by up to 50\u202f% and boosting robustness \u2013 arXiv\u202f2412.09871 7 Capability Salience Vector: Fine\u2011grained Alignment of Loss and Capabilities for Downstream Task Scaling Law \u2013 Qiming\u202fGe, Shuhao\u202fXing, Songyang\u202fGao, Yunhua\u202f\u2026 \u2013 Keywords: scaling laws, validation loss, downstream capability, token\u2011level salience, meta\u2011capability \u2013 CSV decomposes the scalar validation loss into a vector of capability\u2011specific losses, dramatically improving the predictability of downstream task performance for various capabilities and revealing that uniform token weighting is insufficient for accurate scaling\u2011law analysis \u2013 arXiv\u202f2506.13216 8 From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding \u2013 Chiwei\u202fZhu, Bing\u202fXu, Xiaorui\u202fWang, Zheming\u202fMao \u2013 Keywords: instruction tuning, synthetic data, attributed grounding, LLMs \u2013 A two\u2011step pipeline attributes each real instruction to a web document, simulated user, and motivation, then generates new instructions with grounded context, producing a 1\u202fM\u2011example synthetic corpus (SynthQuestions) that is lexically richer than prior datasets \u2013 arXiv\u202f2506.03968 9 HALOGEN: Fantastic LLM Hallucinations and Where to Find Them \u2013 Abhilasha\u202fRavichander, Shrusti\u202fGhela, David\u202fWadden, Yejin\u202fChoi \u2013 Keywords: LLM hallucination benchmark, automatic verification, error taxonomy, multi\u2011domain \u2013 HALOGEN introduces a 10\u202f923\u2011prompt multi\u2011domain benchmark plus high\u2011precision verifiers, exposing pervasive hallucinations (up to 86\u202f% atomic\u2011fact errors) across 14 models and proposing a three\u2011type error taxonomy (copy, knowledge, fabricated) \u2013 arXiv\u202f2501.08292 10 HateDay: Insights from a Global Hate\u2011Speech Dataset Representative of a Day on Twitter \u2013 Manuel\u202fTonneau et\u202fal. \u2013 Keywords: hate speech, Twitter, multilingual dataset, model evaluation, human\u2011in\u2011the\u2011loop \u2013 HateDay samples a full\u2011day of global Twitter activity (\u22481\u202f% of all tweets on 21\u202fSep\u202f2022) across eight languages, revealing a &lt;\u202f2\u202f% hate prevalence and showing that 12 public detection models lose &gt;\u202f90\u202f% of their F1 performance relative to academic testbeds, especially for low\u2011resource languages \u2013 arXiv\u202f2411.15462 11 I\u2080T: Embedding Standardization Method Towards Zero Modality Gap \u2013 Na\u202fMin\u202fAn, Eunki\u202fKim, James\u202fThorne, Hyunjung\u202fShim \u2013 Keywords: modality gap, CLIP, embedding standardization, post\u2011hoc normalization, learnable BN \u2013 I\u2080T proposes a zero\u2011training post\u2011hoc standardization (mean\u2011subtraction\u202f+\u202fFrobenius norm) and a lightweight batch\u2011norm fine\u2011tune that reduces the image\u2011text modality gap in CLIP\u2011style models to near\u2011zero without altering the original weights \u2013 arXiv\u202f2412.14384 12 IndicSynth \u2013 Large\u2011Scale Multilingual Synthetic Speech for Low\u2011Resource Indian Languages \u2013 D.V.\u202fSharma, V.\u202fEkbote, A.\u202fGupta \u2013 Keywords: synthetic speech, low\u2011resource languages, TTS, voice conversion, dataset \u2013 IndicSynth releases ~4\u202f000\u202fh of synthetic audio for 12 Indian languages (\u2248989 speakers), providing a speaker\u2011rich corpus where real recordings are scarce and enabling downstream TTS and ASR research \u2013 ACL Anthology PDF 13 LaTIM: Measuring Latent Token\u2011to\u2011Token Interactions in Mamba Models \u2013 Hugo\u202fPitorro, Marcos\u202fTreviso \u2013 Keywords: Mamba\u20111, Mamba\u20112, state\u2011space models, token\u2011level decomposition, interpretability \u2013 LaTIM reshapes the state\u2011space computation of Mamba models into an attention\u2011like form, offering three normalization schemes that enable faithful attribution of each input token\u2019s influence on every downstream token without retraining \u2013 arXiv\u202f2502.15612 14 Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs \u2013 Authors not listed in source (et\u202fal.) \u2013 Keywords: mechanistic analysis, contextual entrainment, distraction, LLMs \u2013 The study probes how prompting context can both entrain and distract large language models, revealing that salient context tokens dominate hidden\u2011state trajectories and that strategic \u201cdistractor\u201d tokens can significantly degrade downstream performance \u2013 [URL not available \u2013 see ACL\u202f2025 program] 15 LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts \u2013 Qibing\u202fRen, Hao\u202fLi, Dongrui\u202fLiu, Zhanxu\u202fXie, Xiaoya\u202fLu, Yu\u202fQiao, Liu\u202fSha, Jian\u202fYan, Liu\u202fMa, Jian\u202fShao \u2013 Keywords: LLM safety, distribution shift, jailbreak, actor\u2011network theory, multi\u2011turn attacks \u2013 The authors introduce ActorBreaker, a multi\u2011turn jailbreak that exploits natural distribution shifts via an actor\u2011network of humans and non\u2011human entities, achieving higher success rates than prior attacks and providing a curated safety dataset for fine\u2011tuning more robust models \u2013 arXiv\u202f2410.10700 16 Mapping 1\u202f000+ Language Models via the Log\u2011Likelihood Vector \u2013 Momose\u202fOyama, Hiroaki\u202fYamagiwa, Yusuke\u202fTakase, Hidetoshi\u202fShimodaira \u2013 Keywords: log\u2011likelihood vector, model comparison, KL divergence approximation, scalable model mapping, ModelMap \u2013 Computing LLVs on a fixed text set yields a low\u2011cost, Euclidean\u2011distance proxy for KL divergence, enabling linear\u2011time mapping of thousands of language models without extra inference \u2013 arXiv\u202f2502.16173 17 MiniLongBench: The Low\u2011cost Long Context Understanding Benchmark for LLMs \u2013 MilkThink\u2011Lab et\u202fal. \u2013 Keywords: long\u2011context understanding, benchmark compression, evaluation efficiency \u2013 By pruning the 1\u202f600\u2011sample LongBench suite to 237 carefully selected items, MiniLongBench reduces evaluation cost to ~4.5\u202f% while preserving ranking fidelity (Spearman\u202f\u03c1\u202f=\u202f0.97), making large\u2011scale long\u2011context evaluation affordable \u2013 arXiv\u202f2505.19959 18 PARME: Parallel Corpora for Low\u2011Resourced Middle Eastern Languages \u2013 Sina\u202fAhmadi et\u202fal. \u2013 Keywords: parallel corpus, low\u2011resource, Middle East, machine translation, NLLB \u2013 PARME releases 36\u202f384 sentence pairs for eight severely under\u2011researched Middle\u2011Eastern languages (e.g., Luri\u2011Bakhtiari, Hawrami), addressing script non\u2011standardisation and dialect fragmentation and providing the first MT resource for these varieties \u2013 PDF 19 Past Meets Present: Creating Historical Analogy with Large Language Models \u2013 Authors not listed in source (et\u202fal.) \u2013 Keywords: historical analogy, LLM reasoning, temporal transfer \u2013 The paper proposes a prompting framework that extracts historical event embeddings and aligns them with contemporary contexts, enabling LLMs to generate plausible analogies across centuries \u2013 [URL not available \u2013 see ACL\u202f2025 program] 20 Pre\u00b3: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation \u2013 Authors not listed in source (et\u202fal.) \u2013 Keywords: deterministic pushdown automaton, structured generation, LLM efficiency \u2013 Pre\u00b3 equips transformers with a deterministic PDA controller that enforces hierarchical constraints during generation, achieving up to 2\u00d7 speed\u2011ups on structured tasks while preserving output quality \u2013 [URL not available \u2013 see ACL\u202f2025 program] 21 Rethinking the Role of Prompting Strategies in LLM Test\u2011Time Scaling \u2013 Yexiang\u202fLiu, Zekun\u202fLi, Zhi\u202fFang, Nan\u202fXu, Ran\u202fHe, Tieniu\u202fTan \u2013 Keywords: LLM, test\u2011time scaling, prompting strategies, majority voting, probability theory \u2013 Through systematic evaluation of six prompting strategies (including CoT, LoT, Tree\u2011of\u2011Thought) across eight reasoning benchmarks, the study shows that certain strategies (e.g., majority\u2011voting self\u2011consistency) yield near\u2011linear performance gains under a fixed inference\u2011budget while others plateau early \u2013 arXiv\u202f2505.10981 22 Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability \u2013 Yusuke\u202fSakai et\u202fal. \u2013 Keywords: Ordered CommonGen, compositional generalization, instruction following, ordered coverage \u2013 Ordered CommonGen extends CommonGen by requiring concepts to appear in a prescribed order, and introduces an \u201cordered coverage\u201d metric that jointly evaluates compositionality and instruction\u2011following, revealing substantial gaps in current LLMs \u2013 arXiv\u202f2506.15629 23 Toward Automatic Discovery of a Canine Phonetic Alphabet \u2013 Theron\u202fS.\u202fWang, Xingyuan\u202fLi, Hridayesh\u202fLekhak, Tuan\u202fMinh\u202fDang, Mengyue\u202fWu, Kenny\u202fQ.\u202fZhu \u2013 Keywords: canine vocalization, phonetic discovery, minimal pairs, self\u2011supervised audio, semantic classification \u2013 Using a self\u2011supervised audio encoder and a clustering\u2011based minimal\u2011pair discovery pipeline, the authors automatically derive a coarse\u2011grained phonetic alphabet for dog vocalizations, providing a foundation for cross\u2011species communication research \u2013 PDF 24 Towards the Law of Capacity Gap in Distilling Language Models \u2013 Chen\u202fZhang, Qiuchi\u202fLi, Dawei\u202fSong, Zheyu\u202fYe, Yan\u202fGao, Yan\u202fHu \u2013 Keywords: LM distillation, capacity gap, teacher\u2011student scaling, MiniMA, MiniChat \u2013 Empirical analysis across GPT\u20112, Pythia, and LLaMA families uncovers a linear \u201ccapacity\u2011gap law\u201d (optimal teacher size \u2248\u202f2.5\u202f\u00d7\u202fstudent size) that predicts distillation quality and removes the need for costly teacher\u2011search sweeps \u2013 arXiv\u202f2311.07052 25 Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling \u2013 Xianzhen\u202fLuo, Yixuan\u202fWang, Qingfu\u202fZhu \u2013 Keywords: LLM inference, token recycling, speculative decoding, speedup \u2013 Token Recycling re\u2011uses candidate tokens generated during decoding as drafts for future steps via a token\u2011co\u2011occurrence graph, achieving up to 2\u00d7 speed\u2011ups with &lt;\u202f2\u202fMB extra memory and no model retraining \u2013 arXiv\u202f2408.08696 26 Typology\u2011Guided Adaptation for African NLP (ACL\u202f2025) \u2013 Ndapa\u202fNakashole \u2013 Keywords: Morphological Index, Mixture\u2011of\u2011Experts, Bantu noun\u2011class, low\u2011resource African languages \u2013 The paper introduces a continuous Morphological Index (MoI) that quantifies a language\u2019s morphological reliance and uses a MoI\u2011aware MoE routing architecture (MoI\u2011MoE) to allocate capacity between morphology\u2011focused and semantics\u2011focused experts, achieving 92\u202f% noun\u2011class accuracy across ten Bantu languages and earning the Outstanding Paper Award \u2013 PDF <p>Note: For a handful of papers where the ACL\u2011Anthology URL is not directly listed in the source, the canonical PDF URL (e.g., <code>https://aclanthology.org/2025.acl-long.&lt;paper\u2011id&gt;.pdf</code>) can be derived from the ACL identifier; the exact identifier is available on the official program page.</p>"},{"location":"examples_output/wide_research/#3-thematic-synthesis","title":"3. Thematic Synthesis","text":"<p>The 26 Outstanding Papers can be clustered into six overarching research directions that together illustrate where ACL\u202f2025 perceives the field\u2019s frontiers.</p> Theme Papers (representative) Core Contributions A. diagnostics of language\u2011model behavior 1, 3, 7, 15, 21, 22 New theoretical lenses (Zipf\u2011law reformulation, formal\u2011language pre\u2011training, Capability Salience Vectors, safety\u2011gap analysis, prompting\u2011scale theory, ordered compositionality) that expose hidden strengths/weaknesses of LLMs. B. Dataset creation &amp; resource expansion 4, 5, 8, 9, 10, 12, 13, 18, 23, 26 Large, multilingual, or domain\u2011specific corpora (multilingual summarization benchmark, INCLINE cross\u2011lingual data, synthetic instruction corpora, hallucination benchmark, global hate\u2011speech data, synthetic Indian speech, Mamba interpretability, Middle\u2011Eastern parallel corpora, canine phonetics, African typology\u2011guided corpora). C. Efficient model architectures &amp; scaling 2, 6, 11, 14, 16, 17, 20, 25 Innovations that reduce computational cost or improve scalability (byte\u2011level patches, embedding standardization, token\u2011recycling, log\u2011likelihood vector mapping, low\u2011cost long\u2011context benchmark, deterministic PDA controller). D. Evaluation &amp; benchmarking advances 4, 9, 16, 17, 21, 22 New metrics (beyond ROUGE), systematic benchmark reductions, large\u2011scale model\u2011mapping, prompting\u2011strategy scaling studies, ordered compositionality evaluation. E. Safety, ethics, and societal impact 2, 9, 10, 15, 24 Plagiarism detection in AI\u2011generated research, hallucination taxonomy, global hate\u2011speech measurement, jailbreak safety gaps, capacity\u2011gap law for responsible distillation. F. Multilingual &amp; typologically diverse NLP 1, 4, 5, 12, 18, 26 Methods that deliberately target low\u2011resource or typologically distinct languages (contextual Zipf across 27 languages, multilingual summarization, cross\u2011lingual INCLINE, IndicSynth speech, Middle\u2011Eastern parallel corpora, MoI\u2011guided African NLP). <p>Key observations</p> <ol> <li>From diagnostics to prescriptive solutions \u2013 Papers in Theme\u202fA not only identify problems (e.g., hidden bias, safety gaps) but also propose mechanisms (e.g., CSV, prompting strategies) that directly influence model design.  </li> <li>Resource\u2011centric momentum \u2013 Over a third of the outstanding papers (Theme\u202fB) contribute new data; ACL\u202f2025 emphasizes inclusive resources (low\u2011resource African, Indian, Middle\u2011Eastern, canine, and global social\u2011media datasets).  </li> <li>Efficiency remains central \u2013 Whether through novel token representations (Byte Latent Transformer) or clever inference tricks (Token Recycling, MiniLongBench), reducing FLOPs while preserving quality is a unifying goal.  </li> <li>Safety and trustworthiness \u2013 The community is increasingly attentive to misuse scenarios (plagiarism, hallucinations, jailbreaks) and to quantitative laws (capacity\u2011gap) that can guide responsible model deployment.  </li> <li>Multilingual fairness \u2013 Papers stress that improvements for high\u2011resource languages do not automatically transfer; targeted adaptations (INCLINE, MoI\u2011MoE) illustrate a shift toward language\u2011aware model specialization.</li> </ol>"},{"location":"examples_output/wide_research/#4-detailed-highlights-selected-papers","title":"4. Detailed Highlights (Selected Papers)","text":"<p>Below we expand three papers that exemplify the convergence of the identified themes.</p>"},{"location":"examples_output/wide_research/#41-a-new-formulation-of-zipfs-meaningfrequency-law-through-contextual-diversity","title":"4.1. A New Formulation of Zipf\u2019s Meaning\u2011Frequency Law through Contextual Diversity","text":"<ul> <li>Why it matters: Provides a resource\u2011independent way to evaluate lexical richness across any corpus, crucial for low\u2011resource language diagnostics.  </li> <li>Method: Derives a vector\u2011based \u201ccontextual diversity\u201d score v from directional statistics on contextualized embeddings (von\u202fMises\u2011Fisher).  </li> <li>Findings: The power\u2011law holds for &gt;\u202f200\u202fk word types across 30+ languages, but model size and architecture (masked vs. autoregressive) strongly modulate the exponent \u03b1.  </li> <li>Impact: The authors release code and datasets; the metric is already being integrated into the ACL\u202f2025 Demo track for on\u2011the\u2011fly language\u2011model health checks.</li> </ul>"},{"location":"examples_output/wide_research/#42-halogen-fantastic-llm-hallucinations-and-where-to-find-them","title":"4.2. HALOGEN: Fantastic LLM Hallucinations and Where to Find Them","text":"<ul> <li>Why it matters: Hallucinations are arguably the most pressing reliability issue for LLMs deployed in critical domains (medicine, law).  </li> <li>Dataset: 10\u202f923 prompts spanning programming, scientific attribution, summarization, etc., each paired with high\u2011precision verifiers (knowledge bases, code execution).  </li> <li>Taxonomy: Introduces Type\u202fA (copy\u2011from\u2011training), Type\u202fB (knowledge\u2011error), Type\u202fC (fabricated) hallucinations, offering a concrete framework for downstream mitigation.  </li> <li>Benchmarks: Evaluates 14 state\u2011of\u2011the\u2011art models; even GPT\u20114 exhibits up to 86\u202f% atomic\u2011fact error in the most challenging domains.  </li> <li>Open\u2011source: The benchmark and verifier code are released under a permissive license, encouraging community\u2011wide reproducibility.</li> </ul>"},{"location":"examples_output/wide_research/#43-typologyguided-adaptation-for-african-nlp","title":"4.3. Typology\u2011Guided Adaptation for African NLP","text":"<ul> <li>Why it matters: Demonstrates a principled, interpretable approach to multilingual model adaptation for typologically diverse, under\u2011represented languages.  </li> <li>MoI (Morphological Index): Quantifies morphological richness; the architecture dynamically routes inputs to a Morphology\u2011Expert or Semantics\u2011Expert based on MoI.  </li> <li>Results: Achieves 92\u202f% noun\u2011class accuracy across ten Bantu languages, outperforming both morphology\u2011only and rule\u2011based baselines.  </li> <li>Broader relevance: The MoI\u2011MoE design can be generalized to other language families where typological variation drives performance gaps (e.g., Turkic, Austronesian).</li> </ul>"},{"location":"examples_output/wide_research/#5-future-research-directions-suggested-by-acl-2025-outstanding-papers","title":"5. Future Research Directions Suggested by ACL\u202f2025 Outstanding Papers","text":"Direction Rationale Potential Work Unified Diagnostic Suite \u2013 Combine Zipf\u2011law contextual diversity, CSV, and safety\u2011gap metrics into a single evaluation dashboard for LLMs. Individual diagnostics are fragmented; a unified suite would streamline model auditing. Build an open\u2011source library that queries a model\u2019s embeddings, loss salience, and safety behavior in a single API. Cross\u2011modal Modality\u2011Gap Elimination \u2013 Extend I\u2080T\u2019s embedding standardization to multimodal models (e.g., CLIP, Flamingo). Modality gaps hinder unified reasoning across vision\u2011language tasks. Experiment with post\u2011hoc standardization across several vision\u2011language benchmarks, measuring downstream improvements. Resource\u2011Efficient Multilingual Benchmarks \u2013 Expand MiniLongBench\u2019s pruning approach to other costly benchmarks (e.g., massive MT suites). Evaluation cost remains a bottleneck for large\u2011scale multilingual testing. Apply stratified sampling + importance weighting to yield compact yet representative subsets for MT, QA, and summarization. Safety\u2011First Distillation \u2013 Integrate the \u201ccapacity\u2011gap law\u201d with safety\u2011gap analysis to produce distilled models that retain safety properties. Distillation often magnifies safety flaws; a law\u2011guided teacher selection could mitigate this. Develop a safety\u2011aware distillation pipeline that selects teachers based on both capacity\u2011gap and low hallucination scores (from HALOGEN). Typology\u2011Driven MoE for Diverse Families \u2013 Generalize MoI\u2011MoE to other typological dimensions (e.g., tone, word order). African languages are only a subset; many language families exhibit orthogonal typological features. Create a multi\u2011dimensional typology index (e.g., combining MoI with tone\u2011complexity) and train a hierarchical MoE. Instruction\u2011Grounded Synthetic Data Generation \u2013 Couple the \u201cAttributed Grounding\u201d pipeline with domain\u2011specific safety filters (e.g., medical). Synthetic instruction data is proliferating; safety filters are needed to avoid harmful content. Use HALOGEN\u2011style hallucination detection during synthetic data generation to prune unsafe instructions."},{"location":"examples_output/wide_research/#6-conclusion","title":"6. Conclusion","text":"<p>The Outstanding Papers at ACL\u202f2025 collectively paint a picture of a field that is maturing along several dimensions:</p> <ol> <li>Deep, theory\u2011grounded diagnostics that can quantify model behavior without expensive human annotation.  </li> <li>Broad, inclusive resource creation that brings low\u2011resource languages and non\u2011text modalities into the research mainstream.  </li> <li>Efficiency\u2011first engineering that keeps the computational cost of progress sustainable.  </li> <li>A heightened focus on safety, ethics, and societal impact, moving beyond performance numbers to trustworthy deployment.</li> </ol> <p>Researchers building on this foundation should aim to bridge these strands\u2014for instance, by developing unified diagnostic toolkits that are also resource\u2011aware and safety\u2011conscious, or by designing multilingual, typology\u2011driven models that retain high efficiency. The papers summarized here provide both the conceptual vocabulary and technical building blocks needed for such next\u2011generation work.</p>"},{"location":"examples_output/wide_research/#7-references","title":"7. References","text":"<p>All URLs are provided within the one\u2011sentence summaries above; a consolidated list is reproduced here for quick access:</p> <ol> <li>https://aclanthology.org/2025.acl-long.744.pdf  </li> <li>https://arxiv.org/abs/2502.16487  </li> <li>https://arxiv.org/abs/2502.19249  </li> <li>https://arxiv.org/abs/2507.08342  </li> <li>https://arxiv.org/abs/2410.12462  </li> <li>https://arxiv.org/abs/2412.09871  </li> <li>https://arxiv.org/abs/2506.13216  </li> <li>https://arxiv.org/abs/2506.03968  </li> <li>https://arxiv.org/abs/2501.08292  </li> <li>https://arxiv.org/abs/2411.15462  </li> <li>https://arxiv.org/abs/2412.14384  </li> <li>https://aclanthology.org/2025.acl-long.1070.pdf  </li> <li>https://arxiv.org/abs/2502.15612  </li> <li>(ACL\u202f2025 program URL \u2013 PDF to be retrieved)  </li> <li>https://arxiv.org/abs/2410.10700  </li> <li>https://arxiv.org/abs/2502.16173  </li> <li>https://arxiv.org/abs/2505.19959  </li> <li>https://aclanthology.org/2025.acl-long.1451.pdf  </li> <li>(ACL\u202f2025 program URL)  </li> <li>(ACL\u202f2025 program URL)  </li> <li>https://arxiv.org/abs/2505.10981  </li> <li>https://arxiv.org/abs/2506.15629  </li> <li>https://aclanthology.org/2025.acl-long.451.pdf  </li> <li>https://arxiv.org/abs/2311.07052  </li> <li>https://arxiv.org/abs/2408.08696  </li> <li>https://ndapa.us/assets/docs/papers/2025-moi-acl.pdf  </li> </ol> <p>End of Report</p>"},{"location":"howto/eval/","title":"How to Evaluate an Agent","text":"<p>This guide walks you through the complete process of evaluating an agent in Youtu-Agent, from implementation to analysis.</p>"},{"location":"howto/eval/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that:</p> <ol> <li>Environment is set up: You've completed the Getting Started setup</li> <li>Database is configured: The <code>UTU_DB_URL</code> environment variable is set in your <code>.env</code> file (defaults to <code>sqlite:///test.db</code>)</li> <li>API keys are configured: <code>UTU_LLM_API_KEY</code> and other required keys are set in <code>.env</code></li> </ol>"},{"location":"howto/eval/#overview","title":"Overview","text":"<p>The evaluation process consists of four main steps:</p> <ol> <li>Implement and debug your agent - Test your agent interactively</li> <li>Prepare evaluation dataset - Create and upload test cases</li> <li>Run evaluation - Execute automated evaluation</li> <li>Analyze results - Review performance using the analysis dashboard</li> </ol>"},{"location":"howto/eval/#step-1-implement-and-debug-your-agent","title":"Step 1: Implement and Debug Your Agent","text":"<p>Before running formal evaluations, you should implement and test your agent interactively to ensure it works as expected.</p>"},{"location":"howto/eval/#11-create-agent-configuration","title":"1.1 Create Agent Configuration","text":"<p>Create an agent configuration file in <code>configs/agents/</code>. For example, <code>configs/agents/examples/svg_generator.yaml</code>:</p> <pre><code># @package _global_\ndefaults:\n  - /model/base@orchestrator_model\n  - /agents/router/force_plan@orchestrator_router\n  - /agents/simple/search_agent@orchestrator_workers.SearchAgent\n  - /agents/simple/svg_generator@orchestrator_workers.SVGGenerator\n  - _self_\n\ntype: orchestrator\n\norchestrator_config:\n  name: SVGGeneratorAgent\n  add_chitchat_subagent: false\n  additional_instructions: |-\n    Your main objective is generating SVG code according to user's request.\n\norchestrator_workers_info:\n  - name: SearchAgent\n    description: Performs focused information retrieval\n  - name: SVGGenerator\n    description: Creates SVG cards\n</code></pre>"},{"location":"howto/eval/#12-test-interactively","title":"1.2 Test Interactively","text":"<p>Run the CLI chat interface to verify your agent works correctly:</p> <pre><code>python scripts/cli_chat.py --config examples/svg_generator\n</code></pre> <p>Try various inputs and verify the agent's responses meet your expectations. Debug and refine your agent configuration as needed.</p>"},{"location":"howto/eval/#step-2-prepare-evaluation-dataset","title":"Step 2: Prepare Evaluation Dataset","text":"<p>Once your agent is working, create a dataset of test cases for evaluation.</p>"},{"location":"howto/eval/#21-create-dataset-file","title":"2.1 Create Dataset File","text":"<p>Create a JSONL file (e.g., <code>data/svg_gen.jsonl</code>) where each line is a JSON object with your test case. Two data formats are supported:</p> <p>Format 1: LLaMA Factory format (recommended for SFT data):</p> <pre><code>{\"instruction\": \"Research Youtu-Agent and create an SVG summary\"}\n{\"instruction\": \"Introduce the data formats in LLaMA Factory\"}\n{\"instruction\": \"Summarize how to use Claude Code\"}\n{\"input\": \"https://docs.claude.com/en/docs/claude-code/skills\"}\n{\"input\": \"Please introduce Devin coding agent\"}\n{\"instruction\": \"Summarize the following content\", \"input\": \"# Model Context Protocol servers\\n\\nThis repository is a collection of reference implementations...\"}\n{\"input\": \"https://arxiv.org/abs/2502.05957\"}\n{\"input\": \"https://github.com/microsoft/agent-lightning\"}\n{\"input\": \"Summarize resources about MCP\"}\n{\"input\": \"https://huggingface.co/papers/2510.19779\"}\n</code></pre> <p>In LLaMA Factory format: - <code>instruction</code> and/or <code>input</code> fields are combined to create the question - <code>output</code> field (if present) becomes the ground truth answer</p> <p>Format 2: Default format:</p> <pre><code>{\"question\": \"What is Youtu-Agent?\", \"answer\": \"A flexible agent framework\"}\n{\"question\": \"How to install?\", \"answer\": \"Run uv sync\"}\n</code></pre>"},{"location":"howto/eval/#22-upload-dataset-to-database","title":"2.2 Upload Dataset to Database","text":"<p>Upload your dataset using the <code>upload_dataset.py</code> script:</p> <pre><code>python scripts/data/upload_dataset.py \\\n  --file_path data/svg_gen.jsonl \\\n  --dataset_name example_svg_gen \\\n  --data_format llamafactory\n</code></pre> <p>Parameters: - <code>--file_path</code>: Path to your JSONL file - <code>--dataset_name</code>: Name to assign to this dataset in the database - <code>--data_format</code>: Either <code>llamafactory</code> or <code>default</code></p> <p>The script will parse your JSONL file and store the test cases in the database. You should see output like:</p> <pre><code>Uploaded 10 datapoints from data/svg_gen.jsonl to dataset 'example_svg_gen'.\nUpload complete.\n</code></pre>"},{"location":"howto/eval/#step-3-run-evaluation","title":"Step 3: Run Evaluation","text":"<p>Now run your agent on the evaluation dataset.</p>"},{"location":"howto/eval/#31-create-evaluation-configuration","title":"3.1 Create Evaluation Configuration","text":"<p>Create an evaluation config file in <code>configs/eval/examples/</code>. For example, <code>configs/eval/examples/eval_svg_generator.yaml</code>:</p> <pre><code># @package _global_\ndefaults:\n  - /agents/examples/svg_generator@agent\n  - _self_\n\nexp_id: \"example_svg_gen\"\n\ndata:\n  dataset: example_svg_gen\n\nconcurrency: 16\n</code></pre> <p>Key fields: - <code>defaults</code>: Reference your agent configuration - <code>exp_id</code>: Unique identifier for this evaluation run - <code>data.dataset</code>: Name of the dataset you uploaded - <code>concurrency</code>: Number of parallel evaluation tasks</p>"},{"location":"howto/eval/#32-run-the-evaluation-script","title":"3.2 Run the Evaluation Script","text":"<p>Execute the evaluation:</p> <pre><code>python scripts/run_eval.py \\\n  --config_name examples/eval_svg_generator \\\n  --exp_id example_svg_gen \\\n  --dataset example_svg_gen \\\n  --step rollout\n</code></pre> <p>Parameters: - <code>--config_name</code>: Name of your eval config (without <code>.yaml</code> extension) - <code>--exp_id</code>: Unique identifier for this evaluation run - <code>--dataset</code>: Name of the dataset to evaluate on - <code>--step</code>: Evaluation step to run   - <code>rollout</code>: Run the agent on all test cases (collect trajectories)   - <code>judge</code>: Evaluate the agent's outputs (requires ground truth answers)   - <code>all</code>: Run both rollout and judge steps</p> <p>Note: Use <code>--step rollout</code> when your dataset doesn't have ground truth answers (GT). If you have GT answers, you can run <code>--step all</code> or run judge separately:</p> <pre><code># Run judge separately (requires rollout to be completed first)\npython scripts/run_eval_judge.py \\\n  --config_name examples/eval_svg_generator \\\n  --exp_id example_svg_gen\n</code></pre>"},{"location":"howto/eval/#33-monitor-progress","title":"3.3 Monitor Progress","text":"<p>During evaluation, you'll see progress indicators:</p> <pre><code>&gt; Loading dataset 'example_svg_gen'...\n&gt; Found 10 test cases\n&gt; Starting rollout with concurrency 16...\n[1/10] Processing: Research Youtu-Agent...\n[2/10] Processing: Introduce LLaMA Factory...\n...\n&gt; Evaluation complete. Results saved to database.\n</code></pre>"},{"location":"howto/eval/#step-4-analyze-results","title":"Step 4: Analyze Results","text":"<p>After evaluation completes, analyze the results using the web-based analysis dashboard.</p>"},{"location":"howto/eval/#41-set-up-analysis-dashboard","title":"4.1 Set Up Analysis Dashboard","text":"<p>The evaluation analysis dashboard is a Next.js application located in <code>frontend/exp_analysis/</code>.</p> <p>First-time setup:</p> <pre><code>cd frontend/exp_analysis\n\n# Install dependencies\nnpm install --legacy-peer-deps\n\n# Build the application\nnpm run build\n</code></pre>"},{"location":"howto/eval/#42-start-the-dashboard","title":"4.2 Start the Dashboard","text":"<pre><code># Make sure you're in frontend/exp_analysis/\nnpm run start\n</code></pre> <p>The dashboard will be available at <code>http://localhost:3000</code> by default.</p> <p>Change the port (optional): Edit <code>package.json</code> and modify the start script:</p> <pre><code>\"scripts\": {\n  \"start\": \"next start -p 8080\"\n}\n</code></pre>"},{"location":"howto/eval/#43-view-evaluation-results","title":"4.3 View Evaluation Results","text":"<p>Open your browser and navigate to <code>http://localhost:3000</code>. The dashboard provides:</p> <ul> <li>Overview: Summary statistics for your evaluation runs</li> <li>Experiment Comparison: Compare multiple evaluation runs side-by-side</li> <li>Detailed Trajectories: Inspect individual agent execution traces</li> <li>Success/Failure Analysis: Identify patterns in successful vs. failed cases</li> <li>Performance Metrics: Accuracy, latency, token usage, etc.</li> </ul> <p>Key features: - Filter by experiment ID, dataset, or date range - View full agent trajectories including tool calls and reasoning steps - Export results for further analysis - Compare different agent configurations</p>"},{"location":"howto/eval/#44-verify-database-connection","title":"4.4 Verify Database Connection","text":"<p>If you encounter issues, test the database connection:</p> <pre><code>cd frontend/exp_analysis\nnpm run test:db\n</code></pre> <p>This will verify that the dashboard can connect to your database and retrieve evaluation data.</p>"},{"location":"howto/eval/#advanced-topics","title":"Advanced Topics","text":""},{"location":"howto/eval/#evaluating-on-standard-benchmarks","title":"Evaluating on Standard Benchmarks","text":"<p>To evaluate on standard benchmarks like WebWalkerQA or GAIA:</p> <pre><code># Prepare benchmark dataset (one-time setup)\npython scripts/data/process_web_walker_qa.py\n\n# Run evaluation on WebWalkerQA\npython scripts/run_eval.py \\\n  --config_name ww \\\n  --exp_id my_ww_run \\\n  --dataset WebWalkerQA_15 \\\n  --concurrency 5\n</code></pre> <p>See the Evaluation Documentation for more details on benchmarks.</p>"},{"location":"howto/eval/#configuring-judge-models","title":"Configuring Judge Models","text":"<p>For evaluations with ground truth, configure judge models in your eval config:</p> <pre><code>judge_model:\n  model_provider:\n    type: ${oc.env:JUDGE_LLM_TYPE}\n    model: ${oc.env:JUDGE_LLM_MODEL}\n    base_url: ${oc.env:JUDGE_LLM_BASE_URL}\n    api_key: ${oc.env:JUDGE_LLM_API_KEY}\n  model_params:\n    temperature: 0.5\njudge_concurrency: 16\n</code></pre> <p>Set the corresponding environment variables in your <code>.env</code> file:</p> <pre><code>JUDGE_LLM_TYPE=chat.completions\nJUDGE_LLM_MODEL=deepseek-chat\nJUDGE_LLM_BASE_URL=https://api.deepseek.com/v1\nJUDGE_LLM_API_KEY=your-judge-api-key\n</code></pre>"},{"location":"ref/agents/orchestra_agent/","title":"<code>OrchestraAgent</code>","text":"<ul> <li>[x] support streaming for planner &amp; reporter</li> </ul>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent","title":"OrchestraAgent","text":"Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>class OrchestraAgent:\n    def __init__(self, config: AgentConfig | str):\n        \"\"\"Initialize the orchestra agent\"\"\"\n        if isinstance(config, str):\n            config = ConfigLoader.load_agent_config(config)\n        self.config = config\n        # init subagents\n        self.planner_agent = PlannerAgent(config)\n        self.worker_agents = self._setup_workers()\n        self.reporter_agent = ReporterAgent(config)\n\n    def set_planner(self, planner: PlannerAgent):\n        self.planner_agent = planner\n\n    def _setup_workers(self) -&gt; dict[str, SimpleWorkerAgent]:\n        workers = {}\n        for name, config in self.config.workers.items():\n            assert config.type == \"simple\", f\"Only support SimpleAgent as worker in orchestra agent, get {config}\"\n            workers[name] = SimpleWorkerAgent(config=config)\n        return workers\n\n    async def run(self, input: str, trace_id: str = None) -&gt; OrchestraTaskRecorder:\n        task_recorder = self.run_streamed(input, trace_id)\n        async for _ in task_recorder.stream_events():\n            pass\n        return task_recorder\n\n    def run_streamed(self, input: str, trace_id: str = None) -&gt; OrchestraTaskRecorder:\n        # TODO: error_tracing\n        trace_id = trace_id or AgentsUtils.gen_trace_id()\n        logger.info(f\"&gt; trace_id: {trace_id}\")\n\n        task_recorder = OrchestraTaskRecorder(task=input, trace_id=trace_id)\n        # Kick off the actual agent loop in the background and return the streamed result object.\n        task_recorder._run_impl_task = asyncio.create_task(self._start_streaming(task_recorder))\n        return task_recorder\n\n    async def _start_streaming(self, task_recorder: OrchestraTaskRecorder):\n        with trace(workflow_name=\"orchestra_agent\", trace_id=task_recorder.trace_id):\n            try:\n                await self.plan(task_recorder)\n\n                for task in task_recorder.plan.todo:\n                    # print(f\"&gt; processing {task}\")\n                    worker_agent = self.worker_agents[task.agent_name]\n                    await worker_agent.build()\n                    result_streaming = worker_agent.work_streamed(task_recorder, task)\n                    async for event in result_streaming.stream.stream_events():\n                        task_recorder._event_queue.put_nowait(event)\n                    result_streaming.output = result_streaming.stream.final_output\n                    result_streaming.trajectory = AgentsUtils.get_trajectory_from_agent_result(result_streaming.stream)\n                    task_recorder.add_worker_result(result_streaming)\n                    # print(f\"&lt; processed {task}\")\n\n                await self.report(task_recorder)\n\n                task_recorder._event_queue.put_nowait(QueueCompleteSentinel())\n                task_recorder._is_complete = True\n            except Exception as e:\n                task_recorder._is_complete = True\n                task_recorder._event_queue.put_nowait(QueueCompleteSentinel())\n                raise e\n\n    async def plan(self, task_recorder: OrchestraTaskRecorder) -&gt; CreatePlanResult:\n        \"\"\"Step1: Plan\"\"\"\n        plan = await self.planner_agent.create_plan(task_recorder)\n        assert all(t.agent_name in self.worker_agents for t in plan.todo), (\n            f\"agent_name in plan.todo must be in worker_agents, get {plan.todo}\"\n        )\n        logger.info(f\"plan: {plan}\")\n        task_recorder.set_plan(plan)\n        return plan\n\n    async def work(self, task_recorder: OrchestraTaskRecorder, task: Subtask) -&gt; WorkerResult:\n        \"\"\"Step2: Work\"\"\"\n        worker_agent = self.worker_agents[task.agent_name]\n        result = await worker_agent.work(task_recorder, task)\n        task_recorder.add_worker_result(result)\n        return result\n\n    async def report(self, task_recorder: OrchestraTaskRecorder) -&gt; AnalysisResult:\n        \"\"\"Step3: Report\"\"\"\n        analysis_result = await self.reporter_agent.report(task_recorder)\n        task_recorder.add_reporter_result(analysis_result)\n        task_recorder.set_final_output(analysis_result.output)\n        return analysis_result\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.__init__","title":"__init__","text":"<pre><code>__init__(config: AgentConfig | str)\n</code></pre> <p>Initialize the orchestra agent</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>def __init__(self, config: AgentConfig | str):\n    \"\"\"Initialize the orchestra agent\"\"\"\n    if isinstance(config, str):\n        config = ConfigLoader.load_agent_config(config)\n    self.config = config\n    # init subagents\n    self.planner_agent = PlannerAgent(config)\n    self.worker_agents = self._setup_workers()\n    self.reporter_agent = ReporterAgent(config)\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.plan","title":"plan  <code>async</code>","text":"<pre><code>plan(\n    task_recorder: OrchestraTaskRecorder,\n) -&gt; CreatePlanResult\n</code></pre> <p>Step1: Plan</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>async def plan(self, task_recorder: OrchestraTaskRecorder) -&gt; CreatePlanResult:\n    \"\"\"Step1: Plan\"\"\"\n    plan = await self.planner_agent.create_plan(task_recorder)\n    assert all(t.agent_name in self.worker_agents for t in plan.todo), (\n        f\"agent_name in plan.todo must be in worker_agents, get {plan.todo}\"\n    )\n    logger.info(f\"plan: {plan}\")\n    task_recorder.set_plan(plan)\n    return plan\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.work","title":"work  <code>async</code>","text":"<pre><code>work(\n    task_recorder: OrchestraTaskRecorder, task: Subtask\n) -&gt; WorkerResult\n</code></pre> <p>Step2: Work</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>async def work(self, task_recorder: OrchestraTaskRecorder, task: Subtask) -&gt; WorkerResult:\n    \"\"\"Step2: Work\"\"\"\n    worker_agent = self.worker_agents[task.agent_name]\n    result = await worker_agent.work(task_recorder, task)\n    task_recorder.add_worker_result(result)\n    return result\n</code></pre>"},{"location":"ref/agents/orchestra_agent/#utu.agents.orchestra_agent.OrchestraAgent.report","title":"report  <code>async</code>","text":"<pre><code>report(\n    task_recorder: OrchestraTaskRecorder,\n) -&gt; AnalysisResult\n</code></pre> <p>Step3: Report</p> Source code in <code>utu/agents/orchestra_agent.py</code> <pre><code>async def report(self, task_recorder: OrchestraTaskRecorder) -&gt; AnalysisResult:\n    \"\"\"Step3: Report\"\"\"\n    analysis_result = await self.reporter_agent.report(task_recorder)\n    task_recorder.add_reporter_result(analysis_result)\n    task_recorder.set_final_output(analysis_result.output)\n    return analysis_result\n</code></pre>"},{"location":"ref/agents/simple_agent/","title":"<code>SimpleAgent</code>","text":""},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent","title":"SimpleAgent","text":"<p>A simple agent with env, tools, mcps, and context manager, wrapped on openai-agents.</p> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>class SimpleAgent:\n    \"\"\"A simple agent with env, tools, mcps, and context manager, wrapped on openai-agents.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        config: AgentConfig | str | None = None,  # use config to pass agent configs\n        name: str | None = None,\n        instructions: str | Callable | None = None,\n        model: str | Model | None = None,\n        model_settings: ModelSettings | None = None,\n        tools: list[Tool] = None,  # config tools\n        toolkits: list[str] | None = None,  # load tools from toolkit configs\n        output_type: type[Any] | AgentOutputSchemaBase | None = None,\n        tool_use_behavior: Literal[\"run_llm_again\", \"stop_on_first_tool\"] | StopAtTools = \"run_llm_again\",\n    ):\n        assert not (tools and toolkits), \"You can't pass both tools and toolkits.\"\n        self.config = self._get_config(config)\n        if name:\n            self.config.agent.name = name\n        if instructions:\n            self.config.agent.instructions = instructions\n        self.model = self._get_model(self.config, model)\n        self.model_settings = self._get_model_settings(self.config, model_settings)\n        self.tools: list[Tool] = tools or []\n        self.toolkits: list[str] = toolkits or []\n        self.output_type: type[Any] | AgentOutputSchemaBase | None = output_type\n        self.tool_use_behavior: Literal[\"run_llm_again\", \"stop_on_first_tool\"] | StopAtTools = tool_use_behavior\n        self.context_manager: BaseContextManager = None\n        self.env: BaseEnv = None\n        self.workspace_dir: str = \"\"\n        self.current_agent: Agent[TContext] = None  # move to task recorder?\n        self.input_items: list[TResponseInputItem] = []\n        self.run_hooks: RunHooks = get_run_hooks(self.config)\n\n        self._mcp_servers: list[MCPServer] = []\n        self._toolkits: dict[str, AsyncBaseToolkit] = {}\n        self._mcps_exit_stack = AsyncExitStack()\n        self._initialized = False\n\n    def _get_config(self, config: AgentConfig | str | None) -&gt; AgentConfig:\n        if isinstance(config, AgentConfig):\n            return config\n        return ConfigLoader.load_agent_config(config or \"simple/base\")\n\n    def _get_model(self, config: AgentConfig, model: str | Model | None = None) -&gt; Model:\n        if isinstance(model, Model):\n            return model\n        model_provider_config = config.model.model_provider.model_dump()\n        if isinstance(model, str):\n            model_provider_config[\"model\"] = model\n        return AgentsUtils.get_agents_model(**model_provider_config)\n\n    def _get_model_settings(self, config: AgentConfig, model_settings: ModelSettings | None = None) -&gt; ModelSettings:\n        if isinstance(model_settings, ModelSettings):\n            return model_settings\n        return config.model.model_settings\n\n    async def __aenter__(self) -&gt; \"SimpleAgent\":\n        await self.build()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.cleanup()\n\n    async def build(self, trace_id: str = None):\n        \"\"\"Build the agent\"\"\"\n        if self._initialized:\n            logger.info(\"Agent already initialized! Skipping build.\")\n            return\n        self.env = await get_env(self.config, trace_id or AgentsUtils.gen_trace_id())  # Pass trace_id\n        await self.env.build()\n        self.current_agent = Agent(\n            name=self.config.agent.name,\n            instructions=self.config.agent.instructions,\n            model=self.model,\n            model_settings=self.model_settings,\n            tools=await self.get_tools(),\n            output_type=self.output_type,\n            tool_use_behavior=self.tool_use_behavior,\n            mcp_servers=self._mcp_servers,\n        )\n        self.context_manager = build_context_manager(self.config)\n        self._initialized = True\n\n    async def cleanup(self):\n        \"\"\"Cleanup\"\"\"\n        logger.info(\"Cleaning up MCP servers...\")\n        await self._mcps_exit_stack.aclose()\n        self._mcp_servers = []\n        logger.info(\"Cleaning up tools...\")\n        self._toolkits = {}\n        logger.info(\"Cleaning up env...\")\n        await self.env.cleanup()\n        self._initialized = False\n\n    def setup_workspace(self, workspace_dir: str | pathlib.Path):\n        \"\"\"Setup workspace for toolkits that need it\"\"\"\n        assert pathlib.Path(workspace_dir).exists()\n        self.workspace_dir = str(workspace_dir)\n        self._setup_workspace_for_toolkits()\n\n    def _setup_workspace_for_toolkits(self):\n        for toolkit in self._toolkits.values():\n            if hasattr(toolkit, \"setup_workspace\"):\n                toolkit.setup_workspace(self.workspace_dir)\n\n    async def get_tools(self) -&gt; list[Tool]:\n        if self.tools:\n            return self.tools\n\n        if self.toolkits:\n            await self._load_toolkits_config()\n        else:\n            tools_list: list[Tool] = []\n            tools_list += await self.env.get_tools()  # add env tools\n            # TODO: handle duplicate tool names\n            for _, toolkit_config in self.config.toolkits.items():\n                toolkit = await self._load_toolkit(toolkit_config)\n                if toolkit_config.mode in [\"customized\", \"builtin\"]:\n                    tools_list.extend(toolkit.get_tools_in_agents())\n            tool_names = [tool.name for tool in tools_list]\n            logger.info(f\"Loaded {len(tool_names)} tools: {tool_names}\")\n            self.tools = tools_list\n        # setup workspace if needed\n        if self.workspace_dir:\n            self._setup_workspace_for_toolkits()\n        return self.tools\n\n    async def _load_toolkits_config(self):\n        assert isinstance(self.toolkits, list) and all(isinstance(tool, str) for tool in self.toolkits)\n        parsed_tools = []\n        for tool_name in self.toolkits:\n            config = ConfigLoader.load_toolkit_config(tool_name)\n            toolkit = await self._load_toolkit(config)\n            if config.mode in [\"customized\", \"builtin\"]:\n                parsed_tools.extend(toolkit.get_tools_in_agents())\n        self.tools = parsed_tools\n\n    async def _load_toolkit(self, toolkit_config: ToolkitConfig) -&gt; AsyncBaseToolkit | MCPServer:\n        if toolkit_config.mode == \"builtin\":\n            toolkit = await self._load_builtin_toolkit(toolkit_config)\n        elif toolkit_config.mode == \"customized\":\n            toolkit = await self._load_customized_toolkit(toolkit_config)\n        elif toolkit_config.mode == \"mcp\":\n            toolkit = await self._load_mcp_server(toolkit_config)\n        else:\n            raise ValueError(f\"Unknown toolkit mode: {toolkit_config.mode}\")\n\n        if toolkit_config.env_mode == \"e2b\":\n            # setup e2b sandbox for toolkits that need it\n            assert isinstance(self.env, E2BEnv), \"E2B env is required for e2b toolkit!\"\n            toolkit.setup_e2b_env(self.env)\n        return toolkit\n\n    async def _load_builtin_toolkit(self, toolkit_config: ToolkitConfig) -&gt; AsyncBaseToolkit:\n        logger.info(f\"Loading builtin toolkit `{toolkit_config.name}` with config {toolkit_config}\")\n        toolkit = TOOLKIT_MAP[toolkit_config.name](toolkit_config)\n        self._toolkits[toolkit_config.name] = toolkit\n        return toolkit\n\n    async def _load_customized_toolkit(self, toolkit_config: ToolkitConfig) -&gt; AsyncBaseToolkit:\n        logger.info(f\"Loading customized toolkit `{toolkit_config.name}` with config {toolkit_config}\")\n        assert toolkit_config.customized_filepath is not None and toolkit_config.customized_classname is not None\n        toolkit_class = load_class_from_file(toolkit_config.customized_filepath, toolkit_config.customized_classname)\n        toolkit = toolkit_class(toolkit_config)\n        self._toolkits[toolkit_config.name] = toolkit\n        return toolkit\n\n    async def _load_mcp_server(self, toolkit_config: ToolkitConfig) -&gt; MCPServer:\n        logger.info(f\"Loading MCP server `{toolkit_config.name}` with params {toolkit_config.config}\")\n        mcp_server = AgentsMCPUtils.get_mcp_server(toolkit_config)\n        server = await self._mcps_exit_stack.enter_async_context(mcp_server)\n        self._mcp_servers.append(server)\n        return server\n\n    def _get_run_config(self) -&gt; RunConfig:\n        run_config = RunConfig(\n            model=self.current_agent.model,\n            model_settings=self.config.model.model_settings,\n            workflow_name=self.config.agent.name,\n        )\n        return run_config\n\n    def _get_context(self) -&gt; dict:\n        return {\n            \"context_manager\": self.context_manager,\n            \"env\": self.env,\n            \"agent_config\": self.config,\n        }\n\n    def _prepare_run_kwargs(self, input: str | list[TResponseInputItem]) -&gt; dict:\n        return {\n            \"starting_agent\": self.current_agent,\n            \"input\": input,\n            \"context\": self._get_context(),\n            \"max_turns\": self.config.max_turns,\n            \"hooks\": self.run_hooks,\n            \"run_config\": self._get_run_config(),\n        }\n\n    # wrap `Runner` apis in @openai-agents\n    async def run(\n        self, input: str | list[TResponseInputItem], trace_id: str = None, save: bool = False, log_to_db: bool = True\n    ) -&gt; TaskRecorder:\n        \"\"\"Entrypoint for running the agent\n\n        Args:\n            trace_id: str to identify the run\n            save: whether to update massage history (use `input_items`)\n        \"\"\"\n        trace_id = trace_id or AgentsUtils.gen_trace_id()\n        logger.info(f\"&gt; trace_id: {trace_id}\")\n\n        if isinstance(input, list):\n            assert isinstance(input[-1], dict) and \"content\" in input[-1], \"invalid input format!\"\n            task = input[-1][\"content\"]\n        else:\n            assert isinstance(input, str), \"input should be str or list of TResponseInputItem!\"\n            task = input\n        recorder = TaskRecorder(task=task, input=input, trace_id=trace_id)\n\n        if not self._initialized:\n            await self.build(recorder.trace_id)\n        input = recorder.input\n        if isinstance(input, str):  # only add history when input is str?\n            input = self.input_items + [{\"content\": input, \"role\": \"user\"}]\n        run_kwargs = self._prepare_run_kwargs(input)\n        if AgentsUtils.get_current_trace():\n            run_result = await Runner.run(**run_kwargs)\n        else:\n            with trace(workflow_name=\"simple_agent\", trace_id=recorder.trace_id):\n                run_result = await Runner.run(**run_kwargs)\n        # save final output and trajectory\n        recorder.add_run_result(run_result)\n        if save:\n            self.input_items = run_result.to_input_list()\n            # NOTE: acturally, there are only one agent in SimpleAgent\n            self.current_agent = run_result.last_agent\n        if log_to_db:\n            DBService.add(TrajectoryModel.from_task_recorder(recorder))\n        return recorder\n\n    def run_streamed(\n        self, input: str | list[TResponseInputItem], trace_id: str = None, save: bool = False, log_to_db: bool = True\n    ) -&gt; TaskRecorder:\n        \"\"\"Entrypoint for running the agent streamly\n\n        Args:\n            trace_id: str to identify the run\n        \"\"\"\n        trace_id = trace_id or AgentsUtils.gen_trace_id()\n        logger.info(f\"&gt; trace_id: {trace_id}\")\n\n        if isinstance(input, list):\n            assert isinstance(input[-1], dict) and \"content\" in input[-1], \"invalid input format!\"\n            task = input[-1][\"content\"]\n        else:\n            assert isinstance(input, str), \"input should be str or list of TResponseInputItem!\"\n            task = input\n        recorder = TaskRecorder(task=task, input=input, trace_id=trace_id)\n        recorder._run_impl_task = asyncio.create_task(self._start_streaming(recorder, save, log_to_db))\n        return recorder\n\n    async def _start_streaming(self, recorder: TaskRecorder, save: bool = False, log_to_db: bool = True):\n        if not self._initialized:\n            await self.build(recorder.trace_id)\n        try:\n            input = recorder.input\n            if isinstance(input, str):  # only add history when input is str?\n                input = self.input_items + [{\"content\": input, \"role\": \"user\"}]\n            run_kwargs = self._prepare_run_kwargs(input)\n            if AgentsUtils.get_current_trace():\n                run_streamed_result = Runner.run_streamed(**run_kwargs)\n            else:\n                with trace(workflow_name=\"simple_agent\", trace_id=recorder.trace_id):\n                    run_streamed_result = Runner.run_streamed(**run_kwargs)\n            async for event in run_streamed_result.stream_events():\n                recorder._event_queue.put_nowait(event)\n            # save final output and trajectory\n            recorder.add_run_result(run_streamed_result)\n            if save:\n                self.input_items = run_streamed_result.to_input_list()\n                # NOTE: acturally, there are only one agent in SimpleAgent\n                self.current_agent = run_streamed_result.last_agent\n            if log_to_db:\n                DBService.add(TrajectoryModel.from_task_recorder(recorder))\n        except Exception as e:\n            logger.error(f\"Error processing task: {str(e)}\")\n            recorder._event_queue.put_nowait(QueueCompleteSentinel())\n            recorder._is_complete = True\n            raise e\n        finally:\n            recorder._event_queue.put_nowait(QueueCompleteSentinel())\n            recorder._is_complete = True\n\n    # util apis\n    async def chat(self, input: str) -&gt; TaskRecorder:\n        # TODO: set \"session-level\" tracing for multi-turn chat\n        recorder = await self.run(input, save=True)\n        run_result = recorder.get_run_result()\n        AgentsUtils.print_new_items(run_result.new_items)\n        return run_result\n\n    async def chat_streamed(self, input: str) -&gt; TaskRecorder:\n        recorder = self.run_streamed(input, save=True)\n        await AgentsUtils.print_stream_events(recorder.stream_events())\n        return recorder\n\n    def set_instructions(self, instructions: str):\n        logger.warning(\"WARNING: reset instructions is dangerous!\")\n        self.current_agent.instructions = instructions\n\n    def clear_input_items(self):\n        # reset chat history\n        self.input_items = []\n</code></pre>"},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent.build","title":"build  <code>async</code>","text":"<pre><code>build(trace_id: str = None)\n</code></pre> <p>Build the agent</p> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>async def build(self, trace_id: str = None):\n    \"\"\"Build the agent\"\"\"\n    if self._initialized:\n        logger.info(\"Agent already initialized! Skipping build.\")\n        return\n    self.env = await get_env(self.config, trace_id or AgentsUtils.gen_trace_id())  # Pass trace_id\n    await self.env.build()\n    self.current_agent = Agent(\n        name=self.config.agent.name,\n        instructions=self.config.agent.instructions,\n        model=self.model,\n        model_settings=self.model_settings,\n        tools=await self.get_tools(),\n        output_type=self.output_type,\n        tool_use_behavior=self.tool_use_behavior,\n        mcp_servers=self._mcp_servers,\n    )\n    self.context_manager = build_context_manager(self.config)\n    self._initialized = True\n</code></pre>"},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup</p> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup\"\"\"\n    logger.info(\"Cleaning up MCP servers...\")\n    await self._mcps_exit_stack.aclose()\n    self._mcp_servers = []\n    logger.info(\"Cleaning up tools...\")\n    self._toolkits = {}\n    logger.info(\"Cleaning up env...\")\n    await self.env.cleanup()\n    self._initialized = False\n</code></pre>"},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent.setup_workspace","title":"setup_workspace","text":"<pre><code>setup_workspace(workspace_dir: str | Path)\n</code></pre> <p>Setup workspace for toolkits that need it</p> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>def setup_workspace(self, workspace_dir: str | pathlib.Path):\n    \"\"\"Setup workspace for toolkits that need it\"\"\"\n    assert pathlib.Path(workspace_dir).exists()\n    self.workspace_dir = str(workspace_dir)\n    self._setup_workspace_for_toolkits()\n</code></pre>"},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    input: str | list[TResponseInputItem],\n    trace_id: str = None,\n    save: bool = False,\n    log_to_db: bool = True,\n) -&gt; TaskRecorder\n</code></pre> <p>Entrypoint for running the agent</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>str to identify the run</p> <code>None</code> <code>save</code> <code>bool</code> <p>whether to update massage history (use <code>input_items</code>)</p> <code>False</code> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>async def run(\n    self, input: str | list[TResponseInputItem], trace_id: str = None, save: bool = False, log_to_db: bool = True\n) -&gt; TaskRecorder:\n    \"\"\"Entrypoint for running the agent\n\n    Args:\n        trace_id: str to identify the run\n        save: whether to update massage history (use `input_items`)\n    \"\"\"\n    trace_id = trace_id or AgentsUtils.gen_trace_id()\n    logger.info(f\"&gt; trace_id: {trace_id}\")\n\n    if isinstance(input, list):\n        assert isinstance(input[-1], dict) and \"content\" in input[-1], \"invalid input format!\"\n        task = input[-1][\"content\"]\n    else:\n        assert isinstance(input, str), \"input should be str or list of TResponseInputItem!\"\n        task = input\n    recorder = TaskRecorder(task=task, input=input, trace_id=trace_id)\n\n    if not self._initialized:\n        await self.build(recorder.trace_id)\n    input = recorder.input\n    if isinstance(input, str):  # only add history when input is str?\n        input = self.input_items + [{\"content\": input, \"role\": \"user\"}]\n    run_kwargs = self._prepare_run_kwargs(input)\n    if AgentsUtils.get_current_trace():\n        run_result = await Runner.run(**run_kwargs)\n    else:\n        with trace(workflow_name=\"simple_agent\", trace_id=recorder.trace_id):\n            run_result = await Runner.run(**run_kwargs)\n    # save final output and trajectory\n    recorder.add_run_result(run_result)\n    if save:\n        self.input_items = run_result.to_input_list()\n        # NOTE: acturally, there are only one agent in SimpleAgent\n        self.current_agent = run_result.last_agent\n    if log_to_db:\n        DBService.add(TrajectoryModel.from_task_recorder(recorder))\n    return recorder\n</code></pre>"},{"location":"ref/agents/simple_agent/#utu.agents.simple_agent.SimpleAgent.run_streamed","title":"run_streamed","text":"<pre><code>run_streamed(\n    input: str | list[TResponseInputItem],\n    trace_id: str = None,\n    save: bool = False,\n    log_to_db: bool = True,\n) -&gt; TaskRecorder\n</code></pre> <p>Entrypoint for running the agent streamly</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>str to identify the run</p> <code>None</code> Source code in <code>utu/agents/simple_agent.py</code> <pre><code>def run_streamed(\n    self, input: str | list[TResponseInputItem], trace_id: str = None, save: bool = False, log_to_db: bool = True\n) -&gt; TaskRecorder:\n    \"\"\"Entrypoint for running the agent streamly\n\n    Args:\n        trace_id: str to identify the run\n    \"\"\"\n    trace_id = trace_id or AgentsUtils.gen_trace_id()\n    logger.info(f\"&gt; trace_id: {trace_id}\")\n\n    if isinstance(input, list):\n        assert isinstance(input[-1], dict) and \"content\" in input[-1], \"invalid input format!\"\n        task = input[-1][\"content\"]\n    else:\n        assert isinstance(input, str), \"input should be str or list of TResponseInputItem!\"\n        task = input\n    recorder = TaskRecorder(task=task, input=input, trace_id=trace_id)\n    recorder._run_impl_task = asyncio.create_task(self._start_streaming(recorder, save, log_to_db))\n    return recorder\n</code></pre>"},{"location":"ref/config/agent_config/","title":"<code>AgentConfig</code>","text":""},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig","title":"ToolkitConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Toolkit config.</p> Source code in <code>utu/config/agent_config.py</code> <pre><code>class ToolkitConfig(ConfigBaseModel):\n    \"\"\"Toolkit config.\"\"\"\n\n    mode: Literal[\"builtin\", \"customized\", \"mcp\"] = \"builtin\"\n    \"\"\"Toolkit mode.\"\"\"\n    env_mode: Literal[\"local\", \"e2b\"] = \"local\"\n    \"\"\"Environment mode for the toolkit.\"\"\"\n    name: str | None = None\n    \"\"\"Toolkit name.\"\"\"\n    activated_tools: list[str] | None = None\n    \"\"\"Activated tools, if None, all tools will be activated.\"\"\"\n    config: dict | None = Field(default_factory=dict)\n    \"\"\"Specified  configs for certain toolkit. We use raw dict for simplicity\"\"\"\n    config_llm: ModelConfigs | None = None  # | dict[str, ModelConfigs]\n    \"\"\"LLM config if used in toolkit.\"\"\"\n    customized_filepath: str | None = None\n    \"\"\"Customized toolkit filepath.\"\"\"\n    customized_classname: str | None = None\n    \"\"\"Customized toolkit classname.\"\"\"\n    mcp_transport: Literal[\"stdio\", \"sse\", \"streamable_http\"] = \"stdio\"\n    \"\"\"MCP transport.\"\"\"\n    mcp_client_session_timeout_seconds: int = 20\n    \"\"\"The read timeout passed to the MCP ClientSession. We set it bigger to avoid timeout expections.\"\"\"\n</code></pre>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.mode","title":"mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mode: Literal['builtin', 'customized', 'mcp'] = 'builtin'\n</code></pre> <p>Toolkit mode.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.env_mode","title":"env_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env_mode: Literal['local', 'e2b'] = 'local'\n</code></pre> <p>Environment mode for the toolkit.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre> <p>Toolkit name.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.activated_tools","title":"activated_tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activated_tools: list[str] | None = None\n</code></pre> <p>Activated tools, if None, all tools will be activated.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: dict | None = Field(default_factory=dict)\n</code></pre> <p>Specified  configs for certain toolkit. We use raw dict for simplicity</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.config_llm","title":"config_llm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_llm: ModelConfigs | None = None\n</code></pre> <p>LLM config if used in toolkit.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.customized_filepath","title":"customized_filepath  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>customized_filepath: str | None = None\n</code></pre> <p>Customized toolkit filepath.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.customized_classname","title":"customized_classname  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>customized_classname: str | None = None\n</code></pre> <p>Customized toolkit classname.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.mcp_transport","title":"mcp_transport  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_transport: Literal[\n    \"stdio\", \"sse\", \"streamable_http\"\n] = \"stdio\"\n</code></pre> <p>MCP transport.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.ToolkitConfig.mcp_client_session_timeout_seconds","title":"mcp_client_session_timeout_seconds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_client_session_timeout_seconds: int = 20\n</code></pre> <p>The read timeout passed to the MCP ClientSession. We set it bigger to avoid timeout expections.</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig","title":"AgentConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Overall agent config</p> Source code in <code>utu/config/agent_config.py</code> <pre><code>class AgentConfig(ConfigBaseModel):\n    \"\"\"Overall agent config\"\"\"\n\n    type: Literal[\"simple\", \"orchestra\", \"orchestrator\", \"workforce\"] = \"simple\"\n    \"\"\"Agent type\"\"\"\n\n    # simple agent config\n    model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Model config, with model_provider, model_settings, model_params\"\"\"\n    agent: ProfileConfig = Field(default_factory=ProfileConfig)\n    \"\"\"Agent profile config\"\"\"\n    context_manager: ContextManagerConfig = Field(default_factory=ContextManagerConfig)\n    \"\"\"Context manager config\"\"\"\n    env: EnvConfig = Field(default_factory=EnvConfig)\n    \"\"\"Env config\"\"\"\n    toolkits: dict[str, ToolkitConfig] = Field(default_factory=dict)\n    \"\"\"Toolkits config\"\"\"\n    max_turns: int = 50\n    \"\"\"Max turns for simple agent. This param is derived from @openai-agents\"\"\"\n\n    # orchestra agent config\n    planner_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Planner model config\"\"\"\n    planner_config: dict = Field(default_factory=dict)\n    \"\"\"Planner config (dict)\\n\n    - `examples_path`: path to planner examples json file\"\"\"\n    workers: dict[str, \"AgentConfig\"] = Field(default_factory=dict)\n    \"\"\"Workers config\"\"\"\n    workers_info: list[dict] = Field(default_factory=list)\n    \"\"\"Workers info, list of {name, desc, strengths, weaknesses}\\n\n    - `name`: worker name\n    - `desc`: worker description\n    - `strengths`: worker strengths\n    - `weaknesses`: worker weaknesses\"\"\"\n    reporter_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Reporter model config\"\"\"\n    reporter_config: dict = Field(default_factory=dict)\n    \"\"\"Reporter config (dict)\\n\n    - `template_path`: template Jinja2 file path, with `question` and `trajectory` variables\"\"\"\n\n    # workforce agent config\n    workforce_planner_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Workforce planner model config\"\"\"\n    workforce_planner_config: dict = Field(default_factory=dict)\n    \"\"\"Workforce planner config (dict)\"\"\"\n    workforce_assigner_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Workforce assigner model config\"\"\"\n    workforce_assigner_config: dict = Field(default_factory=dict)\n    \"\"\"Workforce assigner config (dict)\"\"\"\n    workforce_answerer_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Workforce answerer model config\"\"\"\n    workforce_answerer_config: dict = Field(default_factory=dict)\n    \"\"\"Workforce answerer config (dict)\"\"\"\n    workforce_executor_agents: dict[str, \"AgentConfig\"] = Field(default_factory=dict)\n    \"\"\"Workforce executor agents config\"\"\"\n    workforce_executor_config: dict = Field(default_factory=dict)\n    \"\"\"Workforce executor config (dict)\"\"\"\n    workforce_executor_infos: list[dict] = Field(default_factory=list)\n    \"\"\"Workforce executor infos, list of {name, desc, strengths, weaknesses}\"\"\"\n\n    # orchestrator agent config\n    orchestrator_router: \"AgentConfig\" = None\n    \"\"\"Orchestrator router agent config\"\"\"\n    orchestrator_config: dict = Field(default_factory=dict)\n    \"\"\"Orchestrator config (dict)\\n\n    - `name`: name of the orchestrator-workers system\n    - `examples_path`: path to planner examples. default utu/data/plan_examples/chain.json\n    - `additional_instructions`: additional instructions for planner\n    - `add_chitchat_subagent`: whether to add chitchat subagent. default True\"\"\"\n    orchestrator_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Planner model config\"\"\"\n    orchestrator_workers: dict[str, \"AgentConfig\"] = Field(default_factory=dict)\n    \"\"\"Workers config\"\"\"\n    orchestrator_workers_info: list[dict] = Field(default_factory=list)\n    \"\"\"Workers info, list of {name, description}\"\"\"\n</code></pre>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\n    \"simple\", \"orchestra\", \"orchestrator\", \"workforce\"\n] = \"simple\"\n</code></pre> <p>Agent type</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: ModelConfigs = Field(default_factory=ModelConfigs)\n</code></pre> <p>Model config, with model_provider, model_settings, model_params</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.agent","title":"agent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent: ProfileConfig = Field(default_factory=ProfileConfig)\n</code></pre> <p>Agent profile config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.context_manager","title":"context_manager  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>context_manager: ContextManagerConfig = Field(\n    default_factory=ContextManagerConfig\n)\n</code></pre> <p>Context manager config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env: EnvConfig = Field(default_factory=EnvConfig)\n</code></pre> <p>Env config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.toolkits","title":"toolkits  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>toolkits: dict[str, ToolkitConfig] = Field(\n    default_factory=dict\n)\n</code></pre> <p>Toolkits config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.max_turns","title":"max_turns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_turns: int = 50\n</code></pre> <p>Max turns for simple agent. This param is derived from @openai-agents</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.planner_model","title":"planner_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>planner_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Planner model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.planner_config","title":"planner_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>planner_config: dict = Field(default_factory=dict)\n</code></pre> <p>Planner config (dict)</p> <ul> <li><code>examples_path</code>: path to planner examples json file</li> </ul>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workers","title":"workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workers: dict[str, AgentConfig] = Field(\n    default_factory=dict\n)\n</code></pre> <p>Workers config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workers_info","title":"workers_info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workers_info: list[dict] = Field(default_factory=list)\n</code></pre> <p>Workers info, list of {name, desc, strengths, weaknesses}</p> <ul> <li><code>name</code>: worker name</li> <li><code>desc</code>: worker description</li> <li><code>strengths</code>: worker strengths</li> <li><code>weaknesses</code>: worker weaknesses</li> </ul>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.reporter_model","title":"reporter_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reporter_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Reporter model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.reporter_config","title":"reporter_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reporter_config: dict = Field(default_factory=dict)\n</code></pre> <p>Reporter config (dict)</p> <ul> <li><code>template_path</code>: template Jinja2 file path, with <code>question</code> and <code>trajectory</code> variables</li> </ul>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_planner_model","title":"workforce_planner_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_planner_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Workforce planner model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_planner_config","title":"workforce_planner_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_planner_config: dict = Field(default_factory=dict)\n</code></pre> <p>Workforce planner config (dict)</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_assigner_model","title":"workforce_assigner_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_assigner_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Workforce assigner model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_assigner_config","title":"workforce_assigner_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_assigner_config: dict = Field(\n    default_factory=dict\n)\n</code></pre> <p>Workforce assigner config (dict)</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_answerer_model","title":"workforce_answerer_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_answerer_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Workforce answerer model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_answerer_config","title":"workforce_answerer_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_answerer_config: dict = Field(\n    default_factory=dict\n)\n</code></pre> <p>Workforce answerer config (dict)</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_executor_agents","title":"workforce_executor_agents  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_executor_agents: dict[str, AgentConfig] = Field(\n    default_factory=dict\n)\n</code></pre> <p>Workforce executor agents config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_executor_config","title":"workforce_executor_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_executor_config: dict = Field(\n    default_factory=dict\n)\n</code></pre> <p>Workforce executor config (dict)</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.workforce_executor_infos","title":"workforce_executor_infos  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workforce_executor_infos: list[dict] = Field(\n    default_factory=list\n)\n</code></pre> <p>Workforce executor infos, list of {name, desc, strengths, weaknesses}</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.orchestrator_router","title":"orchestrator_router  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orchestrator_router: AgentConfig = None\n</code></pre> <p>Orchestrator router agent config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.orchestrator_config","title":"orchestrator_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orchestrator_config: dict = Field(default_factory=dict)\n</code></pre> <p>Orchestrator config (dict)</p> <ul> <li><code>name</code>: name of the orchestrator-workers system</li> <li><code>examples_path</code>: path to planner examples. default utu/data/plan_examples/chain.json</li> <li><code>additional_instructions</code>: additional instructions for planner</li> <li><code>add_chitchat_subagent</code>: whether to add chitchat subagent. default True</li> </ul>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.orchestrator_model","title":"orchestrator_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orchestrator_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Planner model config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.orchestrator_workers","title":"orchestrator_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orchestrator_workers: dict[str, AgentConfig] = Field(\n    default_factory=dict\n)\n</code></pre> <p>Workers config</p>"},{"location":"ref/config/agent_config/#utu.config.agent_config.AgentConfig.orchestrator_workers_info","title":"orchestrator_workers_info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orchestrator_workers_info: list[dict] = Field(\n    default_factory=list\n)\n</code></pre> <p>Workers info, list of {name, description}</p>"},{"location":"ref/config/base_config/","title":"<code>BaseConfig</code>","text":""},{"location":"ref/config/base_config/#utu.config.base_config.ConfigBaseModel","title":"ConfigBaseModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for config, with secure repr</p> Source code in <code>utu/config/base_config.py</code> <pre><code>class ConfigBaseModel(BaseModel):\n    \"\"\"Base model for config, with secure repr\"\"\"\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}({', '.join(f'{k}={v!r}' for k, v in secure_repr(self.__repr_args__()))})\"\n\n    def model_dump(\n        self,\n        *,\n        exclude_none: bool = True,  # avoid passing temperature=None to avoid SGLang error\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n        return super().model_dump(exclude_none=exclude_none, **kwargs)\n</code></pre>"},{"location":"ref/config/eval_config/","title":"<code>EvalConfig</code>","text":""},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig","title":"DataConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Data config</p> Source code in <code>utu/config/eval_config.py</code> <pre><code>class DataConfig(ConfigBaseModel):\n    \"\"\"Data config\"\"\"\n\n    dataset: str  # WebWalkerQA | GAIA_validation | XBench | BrowseComp\n    \"\"\"Built-in dataset name or custom dataset path\"\"\"\n    type: Literal[\"single\", \"mixed\"] = \"single\"\n    \"\"\"Whether the dataset contains only single benchmark data or multiple benchmarks\"\"\"\n    question_field: str = \"question\"\n    \"\"\"Question field name in the dataset\"\"\"\n    gt_field: str = \"answer\"\n    \"\"\"Ground truth field name in the dataset\"\"\"\n</code></pre>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset: str\n</code></pre> <p>Built-in dataset name or custom dataset path</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['single', 'mixed'] = 'single'\n</code></pre> <p>Whether the dataset contains only single benchmark data or multiple benchmarks</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.question_field","title":"question_field  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>question_field: str = 'question'\n</code></pre> <p>Question field name in the dataset</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.DataConfig.gt_field","title":"gt_field  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gt_field: str = 'answer'\n</code></pre> <p>Ground truth field name in the dataset</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig","title":"EvalConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Evaluation config</p> Source code in <code>utu/config/eval_config.py</code> <pre><code>class EvalConfig(ConfigBaseModel):\n    \"\"\"Evaluation config\"\"\"\n\n    exp_id: str = \"default\"\n    \"\"\"Experiment ID\"\"\"\n\n    # data\n    db_url: str = EnvUtils.get_env(\"UTU_DB_URL\", \"sqlite:///test.db\")\n    \"\"\"Database URL\"\"\"\n    data: DataConfig = None\n    \"\"\"Data config\"\"\"\n\n    # rollout\n    agent: AgentConfig | None = None\n    \"\"\"Agent config for rollout\"\"\"\n    concurrency: int = 1\n    \"\"\"Rollout parallelism\"\"\"\n    pass_k: int = 1\n    \"\"\"Rollout k for each sample\"\"\"\n\n    # judgement\n    judge_model: ModelConfigs = Field(default_factory=ModelConfigs)\n    \"\"\"Judge model config\"\"\"\n    judge_concurrency: int = 1\n    \"\"\"Judgement parallelism\"\"\"\n    eval_method: str = None\n    \"\"\"Evaluation method\"\"\"\n    # optional verify function for custom judgement (used by `train` processors etc.)\n    verify_filename: str | None = None\n    \"\"\"Optional: Python filename under `utu/train/verify/` that contains a verify function.\"\"\"\n    verify_func_name: str | None = None\n    \"\"\"Optional: The function name inside the verify file to call for judgement.\"\"\"\n</code></pre>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.exp_id","title":"exp_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exp_id: str = 'default'\n</code></pre> <p>Experiment ID</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.db_url","title":"db_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>db_url: str = get_env('UTU_DB_URL', 'sqlite:///test.db')\n</code></pre> <p>Database URL</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: DataConfig = None\n</code></pre> <p>Data config</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.agent","title":"agent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent: AgentConfig | None = None\n</code></pre> <p>Agent config for rollout</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.concurrency","title":"concurrency  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>concurrency: int = 1\n</code></pre> <p>Rollout parallelism</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.pass_k","title":"pass_k  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pass_k: int = 1\n</code></pre> <p>Rollout k for each sample</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.judge_model","title":"judge_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>judge_model: ModelConfigs = Field(\n    default_factory=ModelConfigs\n)\n</code></pre> <p>Judge model config</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.judge_concurrency","title":"judge_concurrency  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>judge_concurrency: int = 1\n</code></pre> <p>Judgement parallelism</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.eval_method","title":"eval_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval_method: str = None\n</code></pre> <p>Evaluation method</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.verify_filename","title":"verify_filename  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>verify_filename: str | None = None\n</code></pre> <p>Optional: Python filename under <code>utu/train/verify/</code> that contains a verify function.</p>"},{"location":"ref/config/eval_config/#utu.config.eval_config.EvalConfig.verify_func_name","title":"verify_func_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>verify_func_name: str | None = None\n</code></pre> <p>Optional: The function name inside the verify file to call for judgement.</p>"},{"location":"ref/config/loader/","title":"<code>Loader</code>","text":""},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader","title":"ConfigLoader","text":"<p>Config loader</p> Source code in <code>utu/config/loader.py</code> <pre><code>class ConfigLoader:\n    \"\"\"Config loader\"\"\"\n\n    config_path = \"../../configs\"\n    version_base = \"1.3\"\n\n    @classmethod\n    def _load_config_to_dict(cls, name: str = \"default\", config_path: str = None) -&gt; dict:\n        config_path = config_path or cls.config_path\n        with initialize(config_path=config_path, version_base=cls.version_base):\n            cfg = compose(config_name=name)\n            OmegaConf.resolve(cfg)\n        # return dict instead of DictConfig -- avoid JSON serialization error\n        return OmegaConf.to_container(cfg, resolve=True)\n\n    # @classmethod\n    # def _load_config_to_cls(cls, name: str, config_type: Type[TConfig] = None) -&gt; TConfig:\n    #     # TESTING\n    #     cfg = cls._load_config_to_dict(name)\n    #     return config_type(**cfg)\n\n    @classmethod\n    def load_model_config(cls, name: str = \"base\") -&gt; ModelConfigs:\n        \"\"\"Load model config\"\"\"\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs/model\")\n        return ModelConfigs(**cfg)\n\n    @classmethod\n    def load_toolkit_config(cls, name: str = \"search\") -&gt; ToolkitConfig:\n        \"\"\"Load toolkit config\"\"\"\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs/tools\")\n        return ToolkitConfig(**cfg)\n\n    @classmethod\n    def load_agent_config(cls, name: str = \"default\") -&gt; AgentConfig:\n        \"\"\"Load agent config\"\"\"\n        if not name.startswith(\"agents/\"):\n            name = \"agents/\" + name\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n        return AgentConfig(**cfg)\n\n    @classmethod\n    def load_eval_config(cls, name: str = \"default\") -&gt; EvalConfig:\n        \"\"\"Load eval config\"\"\"\n        if not name.startswith(\"eval/\"):\n            name = \"eval/\" + name\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n        return EvalConfig(**cfg)\n\n    @classmethod\n    def load_training_free_grpo_config(cls, name: str = \"default\") -&gt; TrainingFreeGRPOConfig:\n        \"\"\"Load training-free GRPO config\"\"\"\n        if not name.startswith(\"practice/\"):\n            name = \"practice/\" + name\n        cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n        return TrainingFreeGRPOConfig(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_model_config","title":"load_model_config  <code>classmethod</code>","text":"<pre><code>load_model_config(name: str = 'base') -&gt; ModelConfigs\n</code></pre> <p>Load model config</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_model_config(cls, name: str = \"base\") -&gt; ModelConfigs:\n    \"\"\"Load model config\"\"\"\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs/model\")\n    return ModelConfigs(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_toolkit_config","title":"load_toolkit_config  <code>classmethod</code>","text":"<pre><code>load_toolkit_config(name: str = 'search') -&gt; ToolkitConfig\n</code></pre> <p>Load toolkit config</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_toolkit_config(cls, name: str = \"search\") -&gt; ToolkitConfig:\n    \"\"\"Load toolkit config\"\"\"\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs/tools\")\n    return ToolkitConfig(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_agent_config","title":"load_agent_config  <code>classmethod</code>","text":"<pre><code>load_agent_config(name: str = 'default') -&gt; AgentConfig\n</code></pre> <p>Load agent config</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_agent_config(cls, name: str = \"default\") -&gt; AgentConfig:\n    \"\"\"Load agent config\"\"\"\n    if not name.startswith(\"agents/\"):\n        name = \"agents/\" + name\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n    return AgentConfig(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_eval_config","title":"load_eval_config  <code>classmethod</code>","text":"<pre><code>load_eval_config(name: str = 'default') -&gt; EvalConfig\n</code></pre> <p>Load eval config</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_eval_config(cls, name: str = \"default\") -&gt; EvalConfig:\n    \"\"\"Load eval config\"\"\"\n    if not name.startswith(\"eval/\"):\n        name = \"eval/\" + name\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n    return EvalConfig(**cfg)\n</code></pre>"},{"location":"ref/config/loader/#utu.config.loader.ConfigLoader.load_training_free_grpo_config","title":"load_training_free_grpo_config  <code>classmethod</code>","text":"<pre><code>load_training_free_grpo_config(\n    name: str = \"default\",\n) -&gt; TrainingFreeGRPOConfig\n</code></pre> <p>Load training-free GRPO config</p> Source code in <code>utu/config/loader.py</code> <pre><code>@classmethod\ndef load_training_free_grpo_config(cls, name: str = \"default\") -&gt; TrainingFreeGRPOConfig:\n    \"\"\"Load training-free GRPO config\"\"\"\n    if not name.startswith(\"practice/\"):\n        name = \"practice/\" + name\n    cfg = cls._load_config_to_dict(name, config_path=\"../../configs\")\n    return TrainingFreeGRPOConfig(**cfg)\n</code></pre>"},{"location":"ref/config/model_config/","title":"<code>ModelConfig</code>","text":""},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig","title":"ModelProviderConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>config for model provider</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelProviderConfig(ConfigBaseModel):\n    \"\"\"config for model provider\"\"\"\n\n    type: Literal[\"chat.completions\", \"responses\", \"litellm\"] = \"chat.completions\"\n    \"\"\"model type, supported types: chat.completions, responses\"\"\"\n    model: str = EnvUtils.get_env(\"UTU_LLM_MODEL\")\n    \"\"\"model name\"\"\"\n    base_url: str | None = None\n    \"\"\"model provider base url\"\"\"\n    api_key: str | None = None\n    \"\"\"model provider api key\"\"\"\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\n    \"chat.completions\", \"responses\", \"litellm\"\n] = \"chat.completions\"\n</code></pre> <p>model type, supported types: chat.completions, responses</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = get_env('UTU_LLM_MODEL')\n</code></pre> <p>model name</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.base_url","title":"base_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_url: str | None = None\n</code></pre> <p>model provider base url</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelProviderConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: str | None = None\n</code></pre> <p>model provider api key</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelSettingsConfig","title":"ModelSettingsConfig","text":"<p>               Bases: <code>ConfigBaseModel</code>, <code>ModelSettings</code></p> <p>ModelSettings in openai-agents</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelSettingsConfig(ConfigBaseModel, ModelSettings):\n    \"\"\"ModelSettings in openai-agents\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelParamsConfig","title":"ModelParamsConfig","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Basic params shared in chat.completions and responses</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelParamsConfig(ConfigBaseModel):\n    \"\"\"Basic params shared in chat.completions and responses\"\"\"\n\n    temperature: float | None = None\n    top_p: float | None = None\n    parallel_tool_calls: bool | None = None\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs","title":"ModelConfigs","text":"<p>               Bases: <code>ConfigBaseModel</code></p> <p>Overall model config</p> Source code in <code>utu/config/model_config.py</code> <pre><code>class ModelConfigs(ConfigBaseModel):\n    \"\"\"Overall model config\"\"\"\n\n    model_provider: ModelProviderConfig = Field(default_factory=ModelProviderConfig)\n    \"\"\"config for model provider\"\"\"\n    model_settings: ModelSettingsConfig = Field(default_factory=ModelSettingsConfig)\n    \"\"\"config for agent's model settings\"\"\"\n    model_params: ModelParamsConfig = Field(default_factory=ModelParamsConfig)\n    \"\"\"config for basic model usage, e.g. `query_one` in tools / judger\"\"\"\n\n    termination_max_tokens: int | None = None\n    \"\"\"max tokens for the model, used in truncation logic\"\"\"\n</code></pre>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs.model_provider","title":"model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_provider: ModelProviderConfig = Field(\n    default_factory=ModelProviderConfig\n)\n</code></pre> <p>config for model provider</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs.model_settings","title":"model_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_settings: ModelSettingsConfig = Field(\n    default_factory=ModelSettingsConfig\n)\n</code></pre> <p>config for agent's model settings</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs.model_params","title":"model_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_params: ModelParamsConfig = Field(\n    default_factory=ModelParamsConfig\n)\n</code></pre> <p>config for basic model usage, e.g. <code>query_one</code> in tools / judger</p>"},{"location":"ref/config/model_config/#utu.config.model_config.ModelConfigs.termination_max_tokens","title":"termination_max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>termination_max_tokens: int | None = None\n</code></pre> <p>max tokens for the model, used in truncation logic</p>"},{"location":"ref/env/base_env/","title":"<code>BaseEnv</code>","text":"<p>Environment interface for agents.</p> Source code in <code>utu/env/base_env.py</code> <pre><code>class BaseEnv:\n    \"\"\"Environment interface for agents.\"\"\"\n\n    @abc.abstractmethod\n    def get_state(self) -&gt; str:\n        \"\"\"Get the current state of the environment.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    async def get_tools(self) -&gt; list[Tool]:\n        \"\"\"Get the tools available in the environment.\"\"\"\n        raise NotImplementedError\n\n    async def build(self):\n        \"\"\"Build the environment.\"\"\"\n        pass\n\n    async def cleanup(self):\n        \"\"\"Cleanup the environment.\"\"\"\n        pass\n\n    async def __aenter__(self):\n        await self.build()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.cleanup()\n</code></pre>"},{"location":"ref/env/base_env/#utu.env.base_env.BaseEnv.get_state","title":"get_state  <code>abstractmethod</code>","text":"<pre><code>get_state() -&gt; str\n</code></pre> <p>Get the current state of the environment.</p> Source code in <code>utu/env/base_env.py</code> <pre><code>@abc.abstractmethod\ndef get_state(self) -&gt; str:\n    \"\"\"Get the current state of the environment.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/env/base_env/#utu.env.base_env.BaseEnv.get_tools","title":"get_tools  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_tools() -&gt; list[Tool]\n</code></pre> <p>Get the tools available in the environment.</p> Source code in <code>utu/env/base_env.py</code> <pre><code>@abc.abstractmethod\nasync def get_tools(self) -&gt; list[Tool]:\n    \"\"\"Get the tools available in the environment.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/env/base_env/#utu.env.base_env.BaseEnv.build","title":"build  <code>async</code>","text":"<pre><code>build()\n</code></pre> <p>Build the environment.</p> Source code in <code>utu/env/base_env.py</code> <pre><code>async def build(self):\n    \"\"\"Build the environment.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/env/base_env/#utu.env.base_env.BaseEnv.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the environment.</p> Source code in <code>utu/env/base_env.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the environment.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/env/browser_env/","title":"<code>BrowserEnv</code>","text":"<p>               Bases: <code>BaseEnv</code></p> <p>Browser environment for agents.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>class BrowserEnv(BaseEnv):\n    \"\"\"Browser environment for agents.\"\"\"\n\n    def __init__(self, trace_id: str):\n        self.trace_id = trace_id\n        self.docker_manager = DockerManager()\n        self.browser_state: str = None\n\n    async def build(self):\n        \"\"\"Build the environment. We use docker to run a browser container.\"\"\"\n        self.container_info = await self.docker_manager.start_container(self.trace_id)\n        self.mcp_url = self.container_info[\"mcp_url\"]\n\n    async def cleanup(self):\n        await self.docker_manager.stop_container(self.trace_id)\n\n    def get_state(self) -&gt; str:\n        \"\"\"Get the current state of the environment.\"\"\"\n        return self.browser_state\n\n    async def get_tools(self) -&gt; list[Tool]:\n        \"\"\"Get the tools available in the environment.\"\"\"\n        activated_tools = (\n            \"search_google\",\n            \"go_to_url\",\n            \"go_back\",\n            # \"wait\",\n            \"click_element\",\n            \"input_text\",\n            \"switch_tab\",\n            \"open_tab\",\n            \"scroll_down\",\n            \"scroll_up\",\n            \"download_file\",\n            # \"search_google_api\"\n        )\n        tools: list[Tool] = []\n\n        def create_on_invoke_tool(tool_name: str):\n            async def on_invoke_tool(ctx: RunContextWrapper[TContext], input_json: str) -&gt; str:\n                try:\n                    async with MCPClient.get_mcp_client(self.mcp_url) as client:\n                        res = await client.call_tool(tool_name, json.loads(input_json))\n                        if res.isError:\n                            return f\"Error: {res.content[0].text}\"\n                        self.browser_state = res.content[1].text  # DISCUSS: record the web actions?\n                        return res.content[0].text\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.error(f\"except: {e}\", exc_info=True)\n                    return f\"Error: {e}\"\n\n            return on_invoke_tool\n\n        async with MCPClient.get_mcp_client(self.mcp_url) as client:\n            # NOTE: check `MCPUtil` in @agents\n            res = await client.list_tools()\n            assert res.nextCursor is None\n            for tool in res.tools:\n                if tool.name not in activated_tools:\n                    continue\n                tools.append(\n                    FunctionTool(\n                        name=tool.name,\n                        description=tool.description,\n                        params_json_schema=tool.inputSchema,\n                        on_invoke_tool=create_on_invoke_tool(tool.name),\n                    )\n                )\n            return tools\n</code></pre>"},{"location":"ref/env/browser_env/#utu.env.browser_env.BrowserEnv.build","title":"build  <code>async</code>","text":"<pre><code>build()\n</code></pre> <p>Build the environment. We use docker to run a browser container.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>async def build(self):\n    \"\"\"Build the environment. We use docker to run a browser container.\"\"\"\n    self.container_info = await self.docker_manager.start_container(self.trace_id)\n    self.mcp_url = self.container_info[\"mcp_url\"]\n</code></pre>"},{"location":"ref/env/browser_env/#utu.env.browser_env.BrowserEnv.get_state","title":"get_state","text":"<pre><code>get_state() -&gt; str\n</code></pre> <p>Get the current state of the environment.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>def get_state(self) -&gt; str:\n    \"\"\"Get the current state of the environment.\"\"\"\n    return self.browser_state\n</code></pre>"},{"location":"ref/env/browser_env/#utu.env.browser_env.BrowserEnv.get_tools","title":"get_tools  <code>async</code>","text":"<pre><code>get_tools() -&gt; list[Tool]\n</code></pre> <p>Get the tools available in the environment.</p> Source code in <code>utu/env/browser_env.py</code> <pre><code>async def get_tools(self) -&gt; list[Tool]:\n    \"\"\"Get the tools available in the environment.\"\"\"\n    activated_tools = (\n        \"search_google\",\n        \"go_to_url\",\n        \"go_back\",\n        # \"wait\",\n        \"click_element\",\n        \"input_text\",\n        \"switch_tab\",\n        \"open_tab\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"download_file\",\n        # \"search_google_api\"\n    )\n    tools: list[Tool] = []\n\n    def create_on_invoke_tool(tool_name: str):\n        async def on_invoke_tool(ctx: RunContextWrapper[TContext], input_json: str) -&gt; str:\n            try:\n                async with MCPClient.get_mcp_client(self.mcp_url) as client:\n                    res = await client.call_tool(tool_name, json.loads(input_json))\n                    if res.isError:\n                        return f\"Error: {res.content[0].text}\"\n                    self.browser_state = res.content[1].text  # DISCUSS: record the web actions?\n                    return res.content[0].text\n            except Exception as e:  # pylint: disable=broad-except\n                logger.error(f\"except: {e}\", exc_info=True)\n                return f\"Error: {e}\"\n\n        return on_invoke_tool\n\n    async with MCPClient.get_mcp_client(self.mcp_url) as client:\n        # NOTE: check `MCPUtil` in @agents\n        res = await client.list_tools()\n        assert res.nextCursor is None\n        for tool in res.tools:\n            if tool.name not in activated_tools:\n                continue\n            tools.append(\n                FunctionTool(\n                    name=tool.name,\n                    description=tool.description,\n                    params_json_schema=tool.inputSchema,\n                    on_invoke_tool=create_on_invoke_tool(tool.name),\n                )\n            )\n        return tools\n</code></pre>"},{"location":"ref/eval/benchmarks/","title":"<code>BaseBenchmark</code>","text":""},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark","title":"BaseBenchmark","text":"<p>Base class for benchmarks.</p> Evaluation phases <ul> <li>preprocess: load and preprocess the data</li> <li>rollout: rollout the predictions</li> <li>judge: judge the correctness of a batch of predictions</li> <li>stat: get metrics.</li> </ul> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>class BaseBenchmark:\n    \"\"\"Base class for benchmarks.\n\n    Evaluation phases:\n      - preprocess: load and preprocess the data\n      - rollout: rollout the predictions\n      - judge: judge the correctness of a batch of predictions\n      - stat: get metrics.\n    \"\"\"\n\n    dataset: DBDataManager\n    _source_to_processer: dict[str, BaseProcesser] = {}\n\n    def __init__(self, config: EvalConfig | str) -&gt; None:\n        # config\n        if isinstance(config, str):\n            config = ConfigLoader.load_eval_config(name=config)\n        self.config = config\n\n        # dataset\n        self.dataset = DBDataManager(config)\n        _samples = self.dataset.load()\n        if len(_samples) == 0:\n            raise ValueError(f\"No samples found for data config '{self.config.data}'! Please check the data config.\")\n\n    async def main(self):\n        with trace(f\"[{self.config.exp_id}] Evaluation\", trace_id=gen_trace_id()):\n            logger.info(\n                f\"&gt; Running with config: \\n{json.dumps(self.config.model_dump(), indent=2, ensure_ascii=False)}\"\n            )\n            self.preprocess()\n            await self.rollout()\n            await self.judge()\n            logger.info(\"&gt; Running stat...\")\n            await self.stat()\n            logger.info(\"&gt; Cleaning up...\")\n            await self.cleanup()\n\n    def preprocess(self) -&gt; None:\n        \"\"\"Preprocess the dataset before rollout.\"\"\"\n        samples = self.dataset.get_samples(stage=\"init\")\n        logger.info(f\"Preprocessing {len(samples)} samples...\")\n        results = []\n        for sample in tqdm(samples, desc=\"Preprocessing\"):\n            processed_sample = self.preprocess_one(sample)\n            if processed_sample is not None:\n                results.append(processed_sample)\n        logger.info(f\"Successfully preprocessed {len(results)} samples. Updated to db.\")\n        return results\n\n    def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        processer = self._get_processer(sample.source)\n        processed_sample = processer.preprocess_one(sample)\n        if processed_sample is None:\n            return None\n        self.dataset.save(sample)\n        return sample\n\n    async def rollout(self, max_retries: int = 3) -&gt; None:\n        \"\"\"Rollout the datapoints.\"\"\"\n        samples = self.dataset.get_samples(stage=\"init\")\n        logger.info(f\"Rollout {len(samples)} samples...\")\n\n        semaphore = asyncio.Semaphore(self.config.concurrency)\n\n        async def rollout_with_semaphore(item: EvaluationSample):\n            async with semaphore:\n                for i in range(max_retries):\n                    if i &gt; 0:\n                        logger.warning(f\"Retrying rollout for sample '{item.raw_question}', attempt {i + 1}\")\n                    try:\n                        return await self.rollout_one(item)\n                    except Exception as e:  # pylint: disable=broad-except\n                        logger.error(\n                            f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError running rollout on sample '{item.raw_question}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\",\n                            exc_info=True,\n                        )\n\n        tasks = [rollout_with_semaphore(item) for item in samples]\n        results = []\n        for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Rolling out\"):\n            result = await task\n            if result is not None:\n                results.append(result)\n        logger.info(f\"Successfully rollout {len(results)} samples. Updated to db.\")\n        return results\n\n    async def rollout_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        agent = get_agent(self.config.agent)\n        if hasattr(agent, \"build\"):  # hack, should be removed!\n            await agent.build()\n        trace_id = AgentsUtils.gen_trace_id()\n        start_time = time.time()\n        result = await agent.run(sample.augmented_question, trace_id=trace_id)\n        end_time = time.time()\n\n        # Update the sample with the predicted answer and trajectory\n        sample.update(\n            trace_id=trace_id,\n            response=result.final_output,\n            time_cost=end_time - start_time,\n            trajectories=json.dumps(result.trajectories, ensure_ascii=False),\n            stage=\"rollout\",  # update stage to rollout!\n        )\n        self.dataset.save(sample)\n        return sample\n\n    async def judge(self, stage: str | None = \"rollout\") -&gt; list[EvaluationSample]:\n        \"\"\"Judge samples.\n\n        Args:\n            stage (str|None, optional): The stage of samples to judge. If set to None, you can rejudge all samples.\n        \"\"\"\n        if stage == \"rollout\":\n            num_init_samples = len(self.dataset.get_samples(stage=\"init\"))\n            if num_init_samples &gt; 0:\n                logger.error(\n                    f\"There are {num_init_samples} samples might failed unexpectedly in rollout stage.\"\n                    \"Please rerun the benchmarking to continue for final results.\"\n                )\n                exit(1)\n        samples = self.dataset.get_samples(stage=stage)\n        logger.info(f\"Judging {len(samples)} samples...\")\n\n        semaphore = asyncio.Semaphore(self.config.judge_concurrency)\n\n        async def judge_with_semaphore(item: EvaluationSample):\n            async with semaphore:\n                try:\n                    return await self.judge_one(item)\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.error(f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError judging sample '{item}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\", exc_info=True)\n                    return None\n\n        tasks = [judge_with_semaphore(item) for item in samples]\n        results = []\n        for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Judging\"):\n            result = await task\n            if result is not None:\n                results.append(result)\n        logger.info(f\"Successfully judged {len(results)} samples. Updated to db.\")\n        return results\n\n    async def judge_one(self, data: EvaluationSample) -&gt; EvaluationSample:\n        judger = self._get_processer(data.source)\n        result = await judger.judge_one(data)\n        result.update(stage=\"judged\")  # update stage to judged\n        self.dataset.save(result)\n        return result\n\n    async def stat(self) -&gt; list[dict]:\n        # TODO: wrap the data like @verl / @torch\n        # TODO: log to wandb\n        judged_samples = self.dataset.get_samples(stage=\"judged\")\n        logger.info(f\"Stat from {len(judged_samples)} samples:\")\n\n        data_by_benchmark = self._group_data_by_benchmark(judged_samples)\n        overall_results: list[dict] = []\n        for benchmark, data in data_by_benchmark.items():\n            evaluator = self._get_processer(benchmark)\n            result = await evaluator.stat(data)\n            overall_results.append(result)\n\n        logger.info(json.dumps(overall_results, indent=4, ensure_ascii=False))\n        return overall_results\n\n    def _get_processer(self, source: str) -&gt; BaseProcesser:\n        if source not in self._source_to_processer:\n            processer = PROCESSER_FACTORY.get(source, self.config)\n            self._source_to_processer[source] = processer\n        return self._source_to_processer[source]\n\n    def _group_data_by_benchmark(self, predict_data: list[EvaluationSample]) -&gt; dict[str, list[EvaluationSample]]:\n        # group data by benchmark\n        data_by_benchmark: dict[str, list[EvaluationSample]] = {}\n        for data in predict_data:\n            benchmark = data.source\n            if benchmark not in data_by_benchmark:\n                data_by_benchmark[benchmark] = []\n            data_by_benchmark[benchmark].append(data)\n        return data_by_benchmark\n\n    async def cleanup(self):\n        pass\n</code></pre>"},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark.preprocess","title":"preprocess","text":"<pre><code>preprocess() -&gt; None\n</code></pre> <p>Preprocess the dataset before rollout.</p> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>def preprocess(self) -&gt; None:\n    \"\"\"Preprocess the dataset before rollout.\"\"\"\n    samples = self.dataset.get_samples(stage=\"init\")\n    logger.info(f\"Preprocessing {len(samples)} samples...\")\n    results = []\n    for sample in tqdm(samples, desc=\"Preprocessing\"):\n        processed_sample = self.preprocess_one(sample)\n        if processed_sample is not None:\n            results.append(processed_sample)\n    logger.info(f\"Successfully preprocessed {len(results)} samples. Updated to db.\")\n    return results\n</code></pre>"},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark.rollout","title":"rollout  <code>async</code>","text":"<pre><code>rollout(max_retries: int = 3) -&gt; None\n</code></pre> <p>Rollout the datapoints.</p> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>async def rollout(self, max_retries: int = 3) -&gt; None:\n    \"\"\"Rollout the datapoints.\"\"\"\n    samples = self.dataset.get_samples(stage=\"init\")\n    logger.info(f\"Rollout {len(samples)} samples...\")\n\n    semaphore = asyncio.Semaphore(self.config.concurrency)\n\n    async def rollout_with_semaphore(item: EvaluationSample):\n        async with semaphore:\n            for i in range(max_retries):\n                if i &gt; 0:\n                    logger.warning(f\"Retrying rollout for sample '{item.raw_question}', attempt {i + 1}\")\n                try:\n                    return await self.rollout_one(item)\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.error(\n                        f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError running rollout on sample '{item.raw_question}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\",\n                        exc_info=True,\n                    )\n\n    tasks = [rollout_with_semaphore(item) for item in samples]\n    results = []\n    for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Rolling out\"):\n        result = await task\n        if result is not None:\n            results.append(result)\n    logger.info(f\"Successfully rollout {len(results)} samples. Updated to db.\")\n    return results\n</code></pre>"},{"location":"ref/eval/benchmarks/#utu.eval.benchmarks.base_benchmark.BaseBenchmark.judge","title":"judge  <code>async</code>","text":"<pre><code>judge(\n    stage: str | None = \"rollout\",\n) -&gt; list[EvaluationSample]\n</code></pre> <p>Judge samples.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of samples to judge. If set to None, you can rejudge all samples.</p> <code>'rollout'</code> Source code in <code>utu/eval/benchmarks/base_benchmark.py</code> <pre><code>async def judge(self, stage: str | None = \"rollout\") -&gt; list[EvaluationSample]:\n    \"\"\"Judge samples.\n\n    Args:\n        stage (str|None, optional): The stage of samples to judge. If set to None, you can rejudge all samples.\n    \"\"\"\n    if stage == \"rollout\":\n        num_init_samples = len(self.dataset.get_samples(stage=\"init\"))\n        if num_init_samples &gt; 0:\n            logger.error(\n                f\"There are {num_init_samples} samples might failed unexpectedly in rollout stage.\"\n                \"Please rerun the benchmarking to continue for final results.\"\n            )\n            exit(1)\n    samples = self.dataset.get_samples(stage=stage)\n    logger.info(f\"Judging {len(samples)} samples...\")\n\n    semaphore = asyncio.Semaphore(self.config.judge_concurrency)\n\n    async def judge_with_semaphore(item: EvaluationSample):\n        async with semaphore:\n            try:\n                return await self.judge_one(item)\n            except Exception as e:  # pylint: disable=broad-except\n                logger.error(f\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\\nError judging sample '{item}': {e}\\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\", exc_info=True)\n                return None\n\n    tasks = [judge_with_semaphore(item) for item in samples]\n    results = []\n    for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Judging\"):\n        result = await task\n        if result is not None:\n            results.append(result)\n    logger.info(f\"Successfully judged {len(results)} samples. Updated to db.\")\n    return results\n</code></pre>"},{"location":"ref/eval/data/","title":"<code>DataManager</code>","text":""},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager","title":"BaseDataManager","text":"<p>               Bases: <code>ABC</code></p> <p>Base data manager for loading and saving data.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>class BaseDataManager(abc.ABC):\n    \"\"\"Base data manager for loading and saving data.\"\"\"\n\n    data: list[EvaluationSample]\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        self.config = config\n\n    @abc.abstractmethod\n    def load(self) -&gt; list[EvaluationSample]:\n        \"\"\"Load the dataset.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def save(self, **kwargs) -&gt; None:\n        \"\"\"Save the dataset.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def get_samples(self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None) -&gt; list[EvaluationSample]:\n        \"\"\"Get samples of specified stage from the dataset.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load() -&gt; list[EvaluationSample]\n</code></pre> <p>Load the dataset.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>@abc.abstractmethod\ndef load(self) -&gt; list[EvaluationSample]:\n    \"\"\"Load the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(**kwargs) -&gt; None\n</code></pre> <p>Save the dataset.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>@abc.abstractmethod\ndef save(self, **kwargs) -&gt; None:\n    \"\"\"Save the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.BaseDataManager.get_samples","title":"get_samples  <code>abstractmethod</code>","text":"<pre><code>get_samples(\n    stage: Literal[\"init\", \"rollout\", \"judged\"] = None,\n) -&gt; list[EvaluationSample]\n</code></pre> <p>Get samples of specified stage from the dataset.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>@abc.abstractmethod\ndef get_samples(self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None) -&gt; list[EvaluationSample]:\n    \"\"\"Get samples of specified stage from the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager","title":"DBDataManager","text":"<p>               Bases: <code>BaseDataManager</code></p> <p>Database data manager for loading and saving data.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>class DBDataManager(BaseDataManager):\n    \"\"\"Database data manager for loading and saving data.\"\"\"\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        self.config = config\n\n    def load(self) -&gt; list[EvaluationSample]:\n        if self._check_exp_id():\n            logger.warning(f\"exp_id {self.config.exp_id} already exists in db\")\n            return self.get_samples()\n\n        with SQLModelUtils.create_session() as session:\n            datapoints = session.exec(\n                select(DatasetSample).where(DatasetSample.dataset == self.config.data.dataset)\n            ).all()\n            logger.info(f\"Loaded {len(datapoints)} samples from {self.config.data.dataset}.\")\n            samples = []\n            logger.info(f\"Duplicate {self.config.pass_k} times for each sample.\")\n            for dp in datapoints:\n                for _ in range(self.config.pass_k):\n                    sample = EvaluationSample(\n                        dataset=dp.dataset,\n                        dataset_index=dp.index,\n                        source=dp.source,\n                        raw_question=dp.question,\n                        level=dp.level,\n                        correct_answer=dp.answer,\n                        file_name=dp.file_name,\n                        meta=dp.meta,\n                        exp_id=self.config.exp_id,  # add exp_id\n                    )\n                    samples.append(sample)\n            logger.info(f\"Created {len(samples)} samples for exp_id {self.config.exp_id}.\")\n\n            self.data = samples\n            self.save(self.data)  # save to db\n            return self.data\n\n    def get_samples(\n        self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None, limit: int = None\n    ) -&gt; list[EvaluationSample]:\n        \"\"\"Get samples from exp_id with specified stage.\"\"\"\n        with SQLModelUtils.create_session() as session:\n            samples = session.exec(\n                select(EvaluationSample)\n                .where(\n                    EvaluationSample.exp_id == self.config.exp_id,\n                    EvaluationSample.stage == stage if stage else True,\n                )\n                .order_by(EvaluationSample.dataset_index)\n                .limit(limit)\n            ).all()\n            return samples\n\n    def save(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n        \"\"\"Update or add sample(s) to db.\"\"\"\n        if isinstance(samples, list):\n            with SQLModelUtils.create_session() as session:\n                session.add_all(samples)\n                session.commit()\n        else:\n            with SQLModelUtils.create_session() as session:\n                session.add(samples)\n                session.commit()\n\n    def delete_samples(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n        \"\"\"Delete sample(s) from db.\"\"\"\n        if isinstance(samples, list):\n            with SQLModelUtils.create_session() as session:\n                for sample in samples:\n                    session.delete(sample)\n                session.commit()\n        else:\n            with SQLModelUtils.create_session() as session:\n                session.delete(samples)\n                session.commit()\n\n    def _check_exp_id(self) -&gt; bool:\n        # check if any record has the same exp_id\n        with SQLModelUtils.create_session() as session:\n            has_exp_id = session.exec(\n                select(EvaluationSample).where(EvaluationSample.exp_id == self.config.exp_id)\n            ).first()\n        return has_exp_id is not None\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager.get_samples","title":"get_samples","text":"<pre><code>get_samples(\n    stage: Literal[\"init\", \"rollout\", \"judged\"] = None,\n    limit: int = None,\n) -&gt; list[EvaluationSample]\n</code></pre> <p>Get samples from exp_id with specified stage.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>def get_samples(\n    self, stage: Literal[\"init\", \"rollout\", \"judged\"] = None, limit: int = None\n) -&gt; list[EvaluationSample]:\n    \"\"\"Get samples from exp_id with specified stage.\"\"\"\n    with SQLModelUtils.create_session() as session:\n        samples = session.exec(\n            select(EvaluationSample)\n            .where(\n                EvaluationSample.exp_id == self.config.exp_id,\n                EvaluationSample.stage == stage if stage else True,\n            )\n            .order_by(EvaluationSample.dataset_index)\n            .limit(limit)\n        ).all()\n        return samples\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager.save","title":"save","text":"<pre><code>save(\n    samples: list[EvaluationSample] | EvaluationSample,\n) -&gt; None\n</code></pre> <p>Update or add sample(s) to db.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>def save(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n    \"\"\"Update or add sample(s) to db.\"\"\"\n    if isinstance(samples, list):\n        with SQLModelUtils.create_session() as session:\n            session.add_all(samples)\n            session.commit()\n    else:\n        with SQLModelUtils.create_session() as session:\n            session.add(samples)\n            session.commit()\n</code></pre>"},{"location":"ref/eval/data/#utu.eval.data.DBDataManager.delete_samples","title":"delete_samples","text":"<pre><code>delete_samples(\n    samples: list[EvaluationSample] | EvaluationSample,\n) -&gt; None\n</code></pre> <p>Delete sample(s) from db.</p> Source code in <code>utu/eval/data/data_manager.py</code> <pre><code>def delete_samples(self, samples: list[EvaluationSample] | EvaluationSample) -&gt; None:\n    \"\"\"Delete sample(s) from db.\"\"\"\n    if isinstance(samples, list):\n        with SQLModelUtils.create_session() as session:\n            for sample in samples:\n                session.delete(sample)\n            session.commit()\n    else:\n        with SQLModelUtils.create_session() as session:\n            session.delete(samples)\n            session.commit()\n</code></pre>"},{"location":"ref/eval/processor/","title":"<code>Processer</code>","text":""},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser","title":"BaseProcesser","text":"<p>Base class for processers in evaluation tasks.</p> Each processer implements the following evaluation phases <ul> <li>load: load and process data (if necessary)</li> <li>judge: judge the correctness of a batch of predictions</li> <li>stat: get metrics.</li> </ul> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>class BaseProcesser:\n    \"\"\"Base class for processers in evaluation tasks.\n\n    Each processer implements the following evaluation phases:\n      - load: load and process data (if necessary)\n      - judge: judge the correctness of a batch of predictions\n      - stat: get metrics.\n    \"\"\"\n\n    name: str = None\n    config: EvalConfig = None\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        self.config = config\n\n    @abc.abstractmethod\n    def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Preprocess a single sample.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    async def judge_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Judge a single sample.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n        \"\"\"Calculate metrics from the judged data.\"\"\"\n        raise NotImplementedError\n\n    async def stat(self, samples: list[EvaluationSample]) -&gt; dict:\n        metrics = self.calculate_metrics(samples)\n        return {\"benchmark\": self.name, \"metrics\": metrics}\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser.preprocess_one","title":"preprocess_one  <code>abstractmethod</code>","text":"<pre><code>preprocess_one(\n    sample: EvaluationSample,\n) -&gt; EvaluationSample\n</code></pre> <p>Preprocess a single sample.</p> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>@abc.abstractmethod\ndef preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Preprocess a single sample.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser.judge_one","title":"judge_one  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>judge_one(sample: EvaluationSample) -&gt; EvaluationSample\n</code></pre> <p>Judge a single sample.</p> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>@abc.abstractmethod\nasync def judge_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Judge a single sample.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseProcesser.calculate_metrics","title":"calculate_metrics  <code>abstractmethod</code>","text":"<pre><code>calculate_metrics(samples: list[EvaluationSample]) -&gt; dict\n</code></pre> <p>Calculate metrics from the judged data.</p> Source code in <code>utu/eval/processer/base_processor.py</code> <pre><code>@abc.abstractmethod\ndef calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n    \"\"\"Calculate metrics from the judged data.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser","title":"BaseLLMJudgeProcesser","text":"<p>               Bases: <code>BaseProcesser</code></p> <p>Base class for processers that use LLM for judging.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>class BaseLLMJudgeProcesser(BaseProcesser):\n    \"\"\"Base class for processers that use LLM for judging.\"\"\"\n\n    name = \"default\"\n\n    def __init__(self, config: EvalConfig) -&gt; None:\n        super().__init__(config)\n        self.judge_client = SimplifiedAsyncOpenAI(**config.judge_model.model_provider.model_dump())\n\n    def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Preprocess a single sample.\"\"\"\n        question = sample.raw_question\n        template = AUGMENTATION_PROMPTS.get(self.name, AUGMENTATION_PROMPTS[\"default\"])\n        augmented_question = template.format(question=question)\n        sample.update(\n            augmented_question=augmented_question,\n        )\n        return sample\n\n    async def judge_one(self, data: EvaluationSample) -&gt; EvaluationSample:\n        \"\"\"Judge a single sample.\"\"\"\n        question = data.raw_question\n        response = data.response\n        correct_answer = data.correct_answer or \"unknown\"\n\n        if correct_answer == \"unknown\":\n            # if correct answer is unknown, we cannot judge\n            data.update(judged_response=\"invalid\", correct=False, reward=0.0)\n            return data\n\n        # if exact match, return directly(maybe extract exact answer from response first)\n        if self._extract_exact_answer(response) == correct_answer:\n            data.update(judged_response=\"Exact match\", correct=True, reward=1.0)\n            return data\n\n        messages = self._get_judge_messages(question=question, response=response, correct_answer=correct_answer)\n        content = await self.judge_client.query_one(\n            messages=messages, **self.config.judge_model.model_params.model_dump()\n        )\n        parsed_content = self._parse_judge_response(content)\n\n        data.judged_response = content\n        # update the return data with parsed content\n        data.update(**parsed_content)\n        return data\n\n    def calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n        \"\"\"Caculate metrics from the judged data.\"\"\"\n        return {\n            **MetricsUtils.calculate_pass_at_k_metrics(samples, k=self.config.pass_k),\n            **MetricsUtils.calculate_level_pass_at_k_metrics(samples, k=self.config.pass_k),\n        }\n\n    def _get_judge_messages(self, question: str, response: str, correct_answer: str) -&gt; list:\n        if self.name not in JUDGE_PROMPT_MAP:\n            logger.warning(f\"Judge prompt for {self.name} is not implemented! Using default judge prompt.\")\n        template = JUDGE_PROMPT_MAP.get(self.name, JUDGE_PROMPT_MAP[\"default\"])\n        input = template.format(question=question, response=response, correct_answer=correct_answer)\n        return [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": input}]\n\n    def _parse_judge_response(self, response: str) -&gt; dict:\n        \"\"\"Parse the judge response into a structured format.\"\"\"\n        pattern = re.compile(\n            r\"(?=.*?extracted_final_answer:\\s*(?P&lt;extracted_final_answer&gt;.*?)(?=\\n\\s*\\w+:|$))?\"\n            r\"(?=.*?reasoning:\\s*(?P&lt;reasoning&gt;.*?)(?=\\n\\s*\\w+:|$))?\"\n            r\"(?=.*?correct:\\s*(?P&lt;correct&gt;.*?)(?=\\n\\s*\\w+:|$))?\"\n            r\"(?=.*?confidence:\\s*(?P&lt;confidence&gt;\\d+)\\s*%?(?=\\n\\s*\\w+:|$))?\",\n            re.DOTALL,\n        )\n        # remove the bold formatting\n        response = response.replace(\"**\", \"\")\n        # search for the pattern in the response\n        match = pattern.search(response)\n        if not match:\n            raise ValueError(\"Invalid judge response format.\")\n\n        return {\n            \"extracted_final_answer\": match.group(\"extracted_final_answer\").strip()\n            if match.group(\"extracted_final_answer\")\n            else \"\",\n            \"reasoning\": match.group(\"reasoning\").strip() if match.group(\"reasoning\") else \"\",\n            \"correct\": match.group(\"correct\").strip().lower() == \"yes\" if match.group(\"correct\") else False,\n            \"confidence\": int(match.group(\"confidence\")) if match.group(\"confidence\") else None,\n        }\n\n    def _extract_exact_answer(self, response: str) -&gt; str:\n        \"\"\"Extract the exact answer from the response.\"\"\"\n        return response.strip() if response else \"\"\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser.preprocess_one","title":"preprocess_one","text":"<pre><code>preprocess_one(\n    sample: EvaluationSample,\n) -&gt; EvaluationSample\n</code></pre> <p>Preprocess a single sample.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>def preprocess_one(self, sample: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Preprocess a single sample.\"\"\"\n    question = sample.raw_question\n    template = AUGMENTATION_PROMPTS.get(self.name, AUGMENTATION_PROMPTS[\"default\"])\n    augmented_question = template.format(question=question)\n    sample.update(\n        augmented_question=augmented_question,\n    )\n    return sample\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser.judge_one","title":"judge_one  <code>async</code>","text":"<pre><code>judge_one(data: EvaluationSample) -&gt; EvaluationSample\n</code></pre> <p>Judge a single sample.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>async def judge_one(self, data: EvaluationSample) -&gt; EvaluationSample:\n    \"\"\"Judge a single sample.\"\"\"\n    question = data.raw_question\n    response = data.response\n    correct_answer = data.correct_answer or \"unknown\"\n\n    if correct_answer == \"unknown\":\n        # if correct answer is unknown, we cannot judge\n        data.update(judged_response=\"invalid\", correct=False, reward=0.0)\n        return data\n\n    # if exact match, return directly(maybe extract exact answer from response first)\n    if self._extract_exact_answer(response) == correct_answer:\n        data.update(judged_response=\"Exact match\", correct=True, reward=1.0)\n        return data\n\n    messages = self._get_judge_messages(question=question, response=response, correct_answer=correct_answer)\n    content = await self.judge_client.query_one(\n        messages=messages, **self.config.judge_model.model_params.model_dump()\n    )\n    parsed_content = self._parse_judge_response(content)\n\n    data.judged_response = content\n    # update the return data with parsed content\n    data.update(**parsed_content)\n    return data\n</code></pre>"},{"location":"ref/eval/processor/#utu.eval.processer.BaseLLMJudgeProcesser.calculate_metrics","title":"calculate_metrics","text":"<pre><code>calculate_metrics(samples: list[EvaluationSample]) -&gt; dict\n</code></pre> <p>Caculate metrics from the judged data.</p> Source code in <code>utu/eval/processer/base_llm_processor.py</code> <pre><code>def calculate_metrics(self, samples: list[EvaluationSample]) -&gt; dict:\n    \"\"\"Caculate metrics from the judged data.\"\"\"\n    return {\n        **MetricsUtils.calculate_pass_at_k_metrics(samples, k=self.config.pass_k),\n        **MetricsUtils.calculate_level_pass_at_k_metrics(samples, k=self.config.pass_k),\n    }\n</code></pre>"},{"location":"ref/tool/audio_toolkit/","title":"<code>AudioToolkit</code>","text":""},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit","title":"AudioToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/audio_toolkit.py</code> <pre><code>class AudioToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.audio_client = SimplifiedAsyncOpenAI(\n            api_key=EnvUtils.get_env(\"UTU_AUDIO_LLM_API_KEY\"),  # NOTE: you should set these envs in .env\n            base_url=EnvUtils.get_env(\"UTU_AUDIO_LLM_BASE_URL\"),\n        )\n        self.audio_model = EnvUtils.get_env(\"UTU_AUDIO_LLM_MODEL\")\n        self.llm = SimplifiedAsyncOpenAI(**config.config_llm.model_provider.model_dump())\n        self.md5_to_path = {}\n\n    @async_file_cache(expire_time=None)\n    async def transcribe(self, md5: str) -&gt; dict:\n        # model: gpt-4o-transcribe, gpt-4o-mini-transcribe, and whisper-1\n        fn = self.md5_to_path[md5]\n        transcript: TranscriptionVerbose = await self.audio_client.audio.transcriptions.create(\n            model=self.audio_model,\n            file=open(fn, \"rb\"),\n            response_format=\"verbose_json\",\n            timestamp_granularities=[\"segment\"],\n        )\n        return transcript.model_dump()\n\n    def handle_path(self, path: str) -&gt; str:\n        md5 = FileUtils.get_file_md5(path)\n        if FileUtils.is_web_url(path):\n            # download audio to data/_audio, with md5\n            fn = DIR_ROOT / \"data\" / \"_audio\" / f\"{md5}{FileUtils.get_file_ext(path)}\"\n            fn.parent.mkdir(parents=True, exist_ok=True)\n            if not fn.exists():\n                path = FileUtils.download_file(path, fn)\n                logger.info(f\"Downloaded audio file to {path}\")\n            path = fn\n        self.md5_to_path[md5] = path  # record md5 to map\n        return md5\n\n    @register_tool\n    async def audio_qa(self, audio_path: str, question: str) -&gt; str:\n        \"\"\"Asks a question about the audio and gets an answer.\n\n        Args:\n            audio_path (str): The path or URL to the audio file.\n            question (str): The question to ask about the audio.\n        \"\"\"\n        logger.debug(f\"Processing audio file `{audio_path}` with question `{question}`.\")\n        md5 = self.handle_path(audio_path)\n        res = await self.transcribe(md5)\n\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in audio analysis.\"},\n            {\n                \"role\": \"user\",\n                \"content\": TOOL_PROMPTS[\"audio_qa\"].format(\n                    question=question, file=audio_path, duration=res[\"duration\"], transcription=res[\"text\"]\n                ),\n            },\n        ]\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        return output\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.audio_qa","title":"audio_qa  <code>async</code>","text":"<pre><code>audio_qa(audio_path: str, question: str) -&gt; str\n</code></pre> <p>Asks a question about the audio and gets an answer.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>The path or URL to the audio file.</p> required <code>question</code> <code>str</code> <p>The question to ask about the audio.</p> required Source code in <code>utu/tools/audio_toolkit.py</code> <pre><code>@register_tool\nasync def audio_qa(self, audio_path: str, question: str) -&gt; str:\n    \"\"\"Asks a question about the audio and gets an answer.\n\n    Args:\n        audio_path (str): The path or URL to the audio file.\n        question (str): The question to ask about the audio.\n    \"\"\"\n    logger.debug(f\"Processing audio file `{audio_path}` with question `{question}`.\")\n    md5 = self.handle_path(audio_path)\n    res = await self.transcribe(md5)\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in audio analysis.\"},\n        {\n            \"role\": \"user\",\n            \"content\": TOOL_PROMPTS[\"audio_qa\"].format(\n                question=question, file=audio_path, duration=res[\"duration\"], transcription=res[\"text\"]\n            ),\n        },\n    ]\n    output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n    return output\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/audio_toolkit/#utu.tools.audio_toolkit.AudioToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/base_toolkit/","title":"<code>BaseToolkit</code>","text":""},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit","title":"AsyncBaseToolkit","text":"<p>Base class for toolkits.</p> Source code in <code>utu/tools/base.py</code> <pre><code>class AsyncBaseToolkit:\n    \"\"\"Base class for toolkits.\"\"\"\n\n    def __init__(self, config: ToolkitConfig | dict | None = None):\n        if not isinstance(config, ToolkitConfig):\n            config = config or {}\n            config = ToolkitConfig(config=config, name=self.__class__.__name__)\n\n        self.config: ToolkitConfig = config\n        self._tools_map: dict[str, Callable] = None\n\n        self.env_mode = self.config.env_mode\n        if self.env_mode == \"e2b\":\n            self.e2b_env: E2BEnv = None\n            self.e2b_sandbox: AsyncSandbox = None\n\n    def setup_e2b_env(self, env: \"E2BEnv\") -&gt; None:\n        if self.env_mode != \"e2b\":\n            logger.warning(f\"Toolkit should not setup e2b sandbox in env_mode {self.env_mode}!\")\n            return\n        self.e2b_env = env\n        self.e2b_sandbox = env.sandbox\n\n    @property\n    def tools_map(self) -&gt; dict[str, Callable]:\n        \"\"\"Lazy loading of tools map.\n        - collect tools registered by @register_tool\n        \"\"\"\n        if self._tools_map is None:\n            self._tools_map = {}\n            # Iterate through all methods of the class and register @tool\n            for attr_name in dir(self):\n                attr = getattr(self, attr_name)\n                if callable(attr) and getattr(attr, \"_is_tool\", False):\n                    self._tools_map[attr._tool_name] = attr\n        return self._tools_map\n\n    def get_tools_map_func(self) -&gt; dict[str, Callable]:\n        \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n        if self.config.activated_tools:\n            assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n                f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n            )\n            tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n        else:\n            tools_map = self.tools_map\n        return tools_map\n\n    def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n        \"\"\"Get tools in openai-agents format.\"\"\"\n        tools_map = self.get_tools_map_func()\n        tools = []\n        for _, tool in tools_map.items():\n            tools.append(\n                function_tool(\n                    tool,\n                    strict_mode=False,  # turn off strict mode\n                )\n            )\n        return tools\n\n    def get_tools_in_openai(self) -&gt; list[dict]:\n        \"\"\"Get tools in OpenAI format.\"\"\"\n        tools = self.get_tools_in_agents()\n        return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n\n    def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n        \"\"\"Get tools in MCP format.\"\"\"\n        tools = self.get_tools_in_agents()\n        return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n\n    async def call_tool(self, name: str, arguments: dict) -&gt; str:\n        \"\"\"Call a tool by its name.\"\"\"\n        tools_map = self.get_tools_map_func()\n        if name not in tools_map:\n            raise ValueError(f\"Tool {name} not found\")\n        tool = tools_map[name]\n        return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/base_toolkit/#utu.tools.base.AsyncBaseToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/bash_toolkit/","title":"<code>BashToolkit</code>","text":"<p>--- https://www.anthropic.com/engineering/swe-bench-sonnet --- Run commands in a bash shell</p> <ul> <li> <p>When invoking this tool, the contents of the \"command\" parameter does NOT need to be XML-escaped.</p> </li> <li> <p>You don't have access to the internet via this tool.</p> </li> <li> <p>You do have access to a mirror of common linux and python packages via apt and pip.</p> </li> <li> <p>State is persistent across command calls and discussions with the user.</p> </li> <li> <p>To inspect a particular line range of a file, e.g. lines 10-25, try 'sed -n 10,25p /path/to/the/file'.</p> </li> <li> <p>Please avoid commands that may produce a very large amount of output.</p> </li> <li> <p>Please run long lived commands in the background, e.g. 'sleep 10 &amp;' or start a server in the background.\"</p> </li> </ul>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit","title":"BashToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/bash_toolkit.py</code> <pre><code>class BashToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.timeout = self.config.config.get(\"timeout\", 60)\n\n        if self.env_mode == \"local\":\n            from .local_env.bash_pexpect import PexpectBash\n\n            self.bash_runner = PexpectBash(timeout=self.timeout)\n            self.setup_workspace(self.config.config.get(\"workspace_root\", \"/tmp/\"))\n\n    def setup_workspace(self, workspace_root: str):\n        if self.env_mode != \"local\":\n            logger.warning(f\"BashToolkit should not setup workspace in env_mode {self.env_mode}!\")\n            return\n        workspace_dir = pathlib.Path(workspace_root)\n        workspace_dir.mkdir(parents=True, exist_ok=True)\n        self.workspace_root = workspace_root\n        self.bash_runner.run(f\"cd {workspace_root}\")\n\n    @register_tool\n    async def run_bash(self, command: str) -&gt; str:\n        \"\"\"Execute a bash command in your workspace and return its output.\n\n        Args:\n            command: The command to execute\n        \"\"\"\n        if self.env_mode == \"local\":\n            return self.bash_runner.run(command)\n        else:\n            assert self.e2b_sandbox is not None, \"E2B sandbox is not set up!\"\n            from e2b.sandbox.commands.command_handle import CommandExitException\n\n            try:\n                result = await self.e2b_sandbox.commands.run(command, timeout=self.timeout)\n                return E2BUtils.command_result_to_str(result)\n            except CommandExitException as e:\n                return E2BUtils.command_exit_exception_to_str(e)\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.run_bash","title":"run_bash  <code>async</code>","text":"<pre><code>run_bash(command: str) -&gt; str\n</code></pre> <p>Execute a bash command in your workspace and return its output.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The command to execute</p> required Source code in <code>utu/tools/bash_toolkit.py</code> <pre><code>@register_tool\nasync def run_bash(self, command: str) -&gt; str:\n    \"\"\"Execute a bash command in your workspace and return its output.\n\n    Args:\n        command: The command to execute\n    \"\"\"\n    if self.env_mode == \"local\":\n        return self.bash_runner.run(command)\n    else:\n        assert self.e2b_sandbox is not None, \"E2B sandbox is not set up!\"\n        from e2b.sandbox.commands.command_handle import CommandExitException\n\n        try:\n            result = await self.e2b_sandbox.commands.run(command, timeout=self.timeout)\n            return E2BUtils.command_result_to_str(result)\n        except CommandExitException as e:\n            return E2BUtils.command_exit_exception_to_str(e)\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/bash_toolkit/#utu.tools.bash_toolkit.BashToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/","title":"<code>CodesnipToolkit</code>","text":"<p>https://github.com/bytedance/SandboxFusion https://bytedance.github.io/SandboxFusion/docs/docs/get-started</p>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit","title":"CodesnipToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/codesnip_toolkit.py</code> <pre><code>class CodesnipToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.server_url = self.config.config.get(\"server_url\")\n\n    @register_tool\n    async def run_code(self, code: str, language: str = \"python\") -&gt; str:\n        \"\"\"Run code in sandbox and return the result.\n        Supported languages: python, cpp, nodejs, go, go_test, java, php, csharp, bash, typescript, sql, rust, cuda,\n         lua, R, perl, D_ut, ruby, scala, julia, pttest, junit, kotlin_script, jest, verilog, python_gpu, lean, swift,\n         racket\n\n        Args:\n            code (str): The code to run.\n            language (str, optional): The language of the code. Defaults to \"python\".\n        Returns:\n            str: The result of the code.\n        \"\"\"\n        payload = {\n            \"code\": code,\n            \"language\": language,\n        }\n        response = requests.post(f\"{self.server_url}/run_code\", json=payload)\n        result = response.json()\n        logger.info(f\"[tool] run_code ```{oneline_object(payload)}``` got result: {oneline_object(result)}\")\n        return str(result)\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.run_code","title":"run_code  <code>async</code>","text":"<pre><code>run_code(code: str, language: str = 'python') -&gt; str\n</code></pre> <p>Run code in sandbox and return the result. Supported languages: python, cpp, nodejs, go, go_test, java, php, csharp, bash, typescript, sql, rust, cuda,  lua, R, perl, D_ut, ruby, scala, julia, pttest, junit, kotlin_script, jest, verilog, python_gpu, lean, swift,  racket</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to run.</p> required <code>language</code> <code>str</code> <p>The language of the code. Defaults to \"python\".</p> <code>'python'</code> <p>Returns:     str: The result of the code.</p> Source code in <code>utu/tools/codesnip_toolkit.py</code> <pre><code>@register_tool\nasync def run_code(self, code: str, language: str = \"python\") -&gt; str:\n    \"\"\"Run code in sandbox and return the result.\n    Supported languages: python, cpp, nodejs, go, go_test, java, php, csharp, bash, typescript, sql, rust, cuda,\n     lua, R, perl, D_ut, ruby, scala, julia, pttest, junit, kotlin_script, jest, verilog, python_gpu, lean, swift,\n     racket\n\n    Args:\n        code (str): The code to run.\n        language (str, optional): The language of the code. Defaults to \"python\".\n    Returns:\n        str: The result of the code.\n    \"\"\"\n    payload = {\n        \"code\": code,\n        \"language\": language,\n    }\n    response = requests.post(f\"{self.server_url}/run_code\", json=payload)\n    result = response.json()\n    logger.info(f\"[tool] run_code ```{oneline_object(payload)}``` got result: {oneline_object(result)}\")\n    return str(result)\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/codesnip_toolkit/#utu.tools.codesnip_toolkit.CodesnipToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/document_toolkit/","title":"<code>DocumentToolkit</code>","text":"<p>Document toolkit for parsing documents and support Q&amp;A.</p> <p>Support backends:</p> <ul> <li>Chunkr: https://github.com/lumina-ai-inc/chunkr</li> <li> <p>pymupdf: https://github.com/pymupdf/PyMuPDF</p> </li> <li> <p>[ ] unify the filepath cache logic (also suppoort audio_toolkit, image_toolkit)</p> </li> </ul>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit","title":"DocumentToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/document_toolkit.py</code> <pre><code>class DocumentToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        \"\"\"Initialize the DocumentToolkit, with configed parser and llm.\"\"\"\n        super().__init__(config)\n        if self.config.config.get(\"parser\") == \"chunkr\":\n            from .documents.chunkr_parser import ChunkrParser\n\n            self.parser = ChunkrParser(self.config.config)\n        elif self.config.config.get(\"parser\") == \"pymupdf\":\n            from .documents.pdf_parser import PDFParser\n\n            self.parser = PDFParser(self.config.config)\n        else:\n            raise ValueError(f\"Unsupported parser: {self.config.config.get('parser')}\")\n        self.text_limit = self.config.config.get(\"text_limit\", 100_000)\n        self.llm = SimplifiedAsyncOpenAI(**self.config.config_llm.model_provider.model_dump())\n        self.md5_to_path = {}\n\n    async def parse_document(self, md5: str) -&gt; str:\n        logger.info(f\"[tool] parse_document: {self.md5_to_path[md5]}\")\n        return await self.parser.parse(self.md5_to_path[md5])\n\n    def handle_path(self, path_or_url: str) -&gt; str:\n        md5 = FileUtils.get_file_md5(path_or_url)\n        logger.info(f\"md5 for {path_or_url}: {md5}\")\n        if FileUtils.is_web_url(path_or_url):\n            # download document to data/_document, with md5\n            fn = CACHE_DIR / \"documents\" / f\"{md5}{FileUtils.get_file_ext(path_or_url)}\"\n            fn.parent.mkdir(parents=True, exist_ok=True)\n            if not fn.exists():\n                logger.info(f\"Downloaded document file to {path_or_url}\")\n                FileUtils.download_file(url=path_or_url, save_path=fn)\n            self.md5_to_path[md5] = fn  # record md5 to map\n        else:\n            self.md5_to_path[md5] = path_or_url\n        return md5\n\n    @register_tool\n    async def document_qa(self, document_path: str, question: str | None = None) -&gt; str:\n        \"\"\"Get file content summary or answer questions about attached document.\n\n        Supported file types: pdf, docx, pptx, xlsx, xls, ppt, doc\n\n        Args:\n            document_path (str): Local path or URL to a document.\n            question (str, optional): The question to answer. If not provided, return a summary of the document.\n        \"\"\"\n        md5 = self.handle_path(document_path)\n        document_markdown = await self.parse_document(md5)\n        if len(document_markdown) &gt; self.text_limit:\n            document_markdown = document_markdown[: self.text_limit] + \"\\n...\"\n        messages = [\n            {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"document_sp\"]},\n            {\"role\": \"user\", \"content\": document_markdown},\n        ]\n        if question:\n            messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_qa\"].format(question=question)})\n        else:\n            messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_summary\"]})\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        if not question:\n            output = (\n                f\"You did not provide a particular question, so here is a detailed caption for the document: {output}\"\n            )\n        return output\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.__init__","title":"__init__","text":"<pre><code>__init__(config: ToolkitConfig = None) -&gt; None\n</code></pre> <p>Initialize the DocumentToolkit, with configed parser and llm.</p> Source code in <code>utu/tools/document_toolkit.py</code> <pre><code>def __init__(self, config: ToolkitConfig = None) -&gt; None:\n    \"\"\"Initialize the DocumentToolkit, with configed parser and llm.\"\"\"\n    super().__init__(config)\n    if self.config.config.get(\"parser\") == \"chunkr\":\n        from .documents.chunkr_parser import ChunkrParser\n\n        self.parser = ChunkrParser(self.config.config)\n    elif self.config.config.get(\"parser\") == \"pymupdf\":\n        from .documents.pdf_parser import PDFParser\n\n        self.parser = PDFParser(self.config.config)\n    else:\n        raise ValueError(f\"Unsupported parser: {self.config.config.get('parser')}\")\n    self.text_limit = self.config.config.get(\"text_limit\", 100_000)\n    self.llm = SimplifiedAsyncOpenAI(**self.config.config_llm.model_provider.model_dump())\n    self.md5_to_path = {}\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.document_qa","title":"document_qa  <code>async</code>","text":"<pre><code>document_qa(\n    document_path: str, question: str | None = None\n) -&gt; str\n</code></pre> <p>Get file content summary or answer questions about attached document.</p> <p>Supported file types: pdf, docx, pptx, xlsx, xls, ppt, doc</p> <p>Parameters:</p> Name Type Description Default <code>document_path</code> <code>str</code> <p>Local path or URL to a document.</p> required <code>question</code> <code>str</code> <p>The question to answer. If not provided, return a summary of the document.</p> <code>None</code> Source code in <code>utu/tools/document_toolkit.py</code> <pre><code>@register_tool\nasync def document_qa(self, document_path: str, question: str | None = None) -&gt; str:\n    \"\"\"Get file content summary or answer questions about attached document.\n\n    Supported file types: pdf, docx, pptx, xlsx, xls, ppt, doc\n\n    Args:\n        document_path (str): Local path or URL to a document.\n        question (str, optional): The question to answer. If not provided, return a summary of the document.\n    \"\"\"\n    md5 = self.handle_path(document_path)\n    document_markdown = await self.parse_document(md5)\n    if len(document_markdown) &gt; self.text_limit:\n        document_markdown = document_markdown[: self.text_limit] + \"\\n...\"\n    messages = [\n        {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"document_sp\"]},\n        {\"role\": \"user\", \"content\": document_markdown},\n    ]\n    if question:\n        messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_qa\"].format(question=question)})\n    else:\n        messages.append({\"role\": \"user\", \"content\": TOOL_PROMPTS[\"document_summary\"]})\n    output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n    if not question:\n        output = (\n            f\"You did not provide a particular question, so here is a detailed caption for the document: {output}\"\n        )\n    return output\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/document_toolkit/#utu.tools.document_toolkit.DocumentToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/image_toolkit/","title":"<code>ImageToolkit</code>","text":"<p>@smolagents/examples/open_deep_research/scripts/visual_qa.py @camel/camel/toolkits/image_analysis_toolkit.py https://platform.openai.com/docs/guides/images-vision?api-mode=chat</p>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit","title":"ImageToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/image_toolkit.py</code> <pre><code>class ImageToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        image_llm_config = {\n            \"type\": EnvUtils.get_env(\"UTU_IMAGE_LLM_TYPE\", \"chat.completions\"),\n            \"model\": EnvUtils.get_env(\"UTU_IMAGE_LLM_MODEL\"),  # NOTE: you should set these envs in .env\n            \"api_key\": EnvUtils.get_env(\"UTU_IMAGE_LLM_API_KEY\"),\n            \"base_url\": EnvUtils.get_env(\"UTU_IMAGE_LLM_BASE_URL\"),\n        }\n        self.llm = SimplifiedAsyncOpenAI(**image_llm_config)\n\n    def _load_image(self, image_path: str) -&gt; str:\n        parsed = urlparse(image_path)\n        image: Image.Image = None\n\n        if parsed.scheme in (\"http\", \"https\"):\n            logger.debug(f\"Fetching image from URL: {image_path}\")\n            try:\n                response = requests.get(image_path, timeout=15)\n                response.raise_for_status()\n                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"URL fetch failed: {e}\")\n                raise\n        else:\n            logger.debug(f\"Loading local image: {image_path}\")\n            try:\n                image = Image.open(image_path).convert(\"RGB\")\n            except Exception as e:  # pylint: disable=broad-except\n                logger.error(f\"Image loading failed: {e}\")\n                raise ValueError(f\"Invalid image file: {image_path}\") from e\n        # Convert the image to a base64 string\n        buffer = BytesIO()\n        image.save(buffer, format=\"JPEG\")  # Use the appropriate format (e.g., JPEG, PNG)\n        base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n\n        # add string formatting required by the endpoint\n        image_string = f\"data:image/jpeg;base64,{base64_image}\"\n        return image_string\n\n    @register_tool\n    async def image_qa(self, image_path: str, question: str | None = None) -&gt; str:\n        \"\"\"Generate textual description or answer questions about attached image.\n\n        Args:\n            image_path (str): Local path or URL to an image.\n            question (str, optional): The question to answer. If not provided, return a description of the image.\n        \"\"\"\n        image_str = self._load_image(image_path)\n        if not question:\n            messages = [\n                {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_summary\"]},\n                {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": image_str}}]},\n            ]\n            output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n            output = f\"You did not provide a particular question, so here is a detailed caption for the image: {output}\"\n        else:\n            messages = [\n                {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_qa\"]},\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": question},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_str}},\n                    ],\n                },\n            ]\n            output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        return output\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.image_qa","title":"image_qa  <code>async</code>","text":"<pre><code>image_qa(\n    image_path: str, question: str | None = None\n) -&gt; str\n</code></pre> <p>Generate textual description or answer questions about attached image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Local path or URL to an image.</p> required <code>question</code> <code>str</code> <p>The question to answer. If not provided, return a description of the image.</p> <code>None</code> Source code in <code>utu/tools/image_toolkit.py</code> <pre><code>@register_tool\nasync def image_qa(self, image_path: str, question: str | None = None) -&gt; str:\n    \"\"\"Generate textual description or answer questions about attached image.\n\n    Args:\n        image_path (str): Local path or URL to an image.\n        question (str, optional): The question to answer. If not provided, return a description of the image.\n    \"\"\"\n    image_str = self._load_image(image_path)\n    if not question:\n        messages = [\n            {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_summary\"]},\n            {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": image_str}}]},\n        ]\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n        output = f\"You did not provide a particular question, so here is a detailed caption for the image: {output}\"\n    else:\n        messages = [\n            {\"role\": \"system\", \"content\": TOOL_PROMPTS[\"image_qa\"]},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": question},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_str}},\n                ],\n            },\n        ]\n        output = await self.llm.query_one(messages=messages, **self.config.config_llm.model_params.model_dump())\n    return output\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/image_toolkit/#utu.tools.image_toolkit.ImageToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/","title":"<code>PythonExecutorToolkit</code>","text":""},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit","title":"PythonExecutorToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> <p>A tool for executing Python code in a sandboxed environment.</p> Source code in <code>utu/tools/python_executor_toolkit.py</code> <pre><code>class PythonExecutorToolkit(AsyncBaseToolkit):\n    \"\"\"\n    A tool for executing Python code in a sandboxed environment.\n    \"\"\"\n\n    def __init__(self, config: ToolkitConfig | dict | None = None):\n        super().__init__(config)\n\n        if self.env_mode == \"local\":\n            self.setup_workspace(self.config.config.get(\"workspace_root\", None))\n        elif self.env_mode == \"e2b\":\n            pass\n        else:\n            raise ValueError(f\"Unsupported env_mode {self.env_mode} for PythonExecutorToolkit!\")\n\n    def setup_workspace(self, workspace_root: str = None):\n        if self.env_mode != \"local\":\n            logger.warning(f\"PythonExecutorToolkit should not setup workspace in env_mode {self.env_mode}!\")\n            return\n        if workspace_root is None:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            unique_id = str(uuid.uuid4())[:8]\n            workspace_root = f\"/tmp/utu/python_executor/{timestamp}_{unique_id}\"\n        workspace_dir = pathlib.Path(workspace_root)\n        workspace_dir.mkdir(parents=True, exist_ok=True)\n        self.workspace_root = str(workspace_root)\n\n    @register_tool\n    async def execute_python_code(self, code: str, timeout: int = 30) -&gt; dict:\n        \"\"\"\n        Executes Python code and returns the output.\n\n        Args:\n            code (str): The Python code to execute.\n            timeout (int): The execution timeout in seconds. Defaults to 30.\n\n        Returns:\n            dict: A dictionary containing the execution results.\n        \"\"\"\n        if self.env_mode == \"local\":\n            return await execute_python_code_async(code, self.workspace_root, timeout=timeout)\n        else:\n            assert self.e2b_sandbox is not None, \"E2B sandbox is not set up!\"\n            result = await self.e2b_sandbox.run_code(code, language=\"python\", timeout=timeout)\n            return E2BUtils.execution_to_str(result)\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.execute_python_code","title":"execute_python_code  <code>async</code>","text":"<pre><code>execute_python_code(code: str, timeout: int = 30) -&gt; dict\n</code></pre> <p>Executes Python code and returns the output.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The Python code to execute.</p> required <code>timeout</code> <code>int</code> <p>The execution timeout in seconds. Defaults to 30.</p> <code>30</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the execution results.</p> Source code in <code>utu/tools/python_executor_toolkit.py</code> <pre><code>@register_tool\nasync def execute_python_code(self, code: str, timeout: int = 30) -&gt; dict:\n    \"\"\"\n    Executes Python code and returns the output.\n\n    Args:\n        code (str): The Python code to execute.\n        timeout (int): The execution timeout in seconds. Defaults to 30.\n\n    Returns:\n        dict: A dictionary containing the execution results.\n    \"\"\"\n    if self.env_mode == \"local\":\n        return await execute_python_code_async(code, self.workspace_root, timeout=timeout)\n    else:\n        assert self.e2b_sandbox is not None, \"E2B sandbox is not set up!\"\n        result = await self.e2b_sandbox.run_code(code, language=\"python\", timeout=timeout)\n        return E2BUtils.execution_to_str(result)\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/python_executor_toolkit/#utu.tools.python_executor_toolkit.PythonExecutorToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/search_toolkit/","title":"<code>SearchToolkit</code>","text":""},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit","title":"SearchToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> <p>Search Toolkit</p> NOTE <ul> <li>Please configure the required env variables! See <code>configs/agents/tools/search.yaml</code></li> </ul> <p>Methods:</p> Name Description <code>- search</code> <p>str, num_results: int = 5)</p> <code>- web_qa</code> <p>str, query: str)</p> Source code in <code>utu/tools/search_toolkit.py</code> <pre><code>class SearchToolkit(AsyncBaseToolkit):\n    \"\"\"Search Toolkit\n\n    NOTE:\n        - Please configure the required env variables! See `configs/agents/tools/search.yaml`\n\n    Methods:\n        - search(query: str, num_results: int = 5)\n        - web_qa(url: str, query: str)\n    \"\"\"\n\n    def __init__(self, config: ToolkitConfig = None):\n        super().__init__(config)\n        search_engine = self.config.config.get(\"search_engine\", \"google\")\n        match search_engine:\n            case \"google\":\n                from .search.google_search import GoogleSearch\n\n                self.search_engine = GoogleSearch(self.config.config)\n            case \"jina\":\n                from .search.jina_search import JinaSearch\n\n                self.search_engine = JinaSearch(self.config.config)\n            case \"baidu\":\n                from .search.baidu_search import BaiduSearch\n\n                self.search_engine = BaiduSearch(self.config.config)\n            case \"duckduckgo\":\n                from .search.duckduckgo_search import DuckDuckGoSearch\n\n                self.search_engine = DuckDuckGoSearch(self.config.config)\n            case _:\n                raise ValueError(f\"Unsupported search engine: {search_engine}\")\n        crawl_engine = self.config.config.get(\"crawl_engine\", \"jina\")\n        match crawl_engine:\n            case \"jina\":\n                from .search.jina_crawl import JinaCrawl\n\n                self.crawl_engine = JinaCrawl(self.config.config)\n            case \"crawl4ai\":\n                from .search.crawl4ai_crawl import Crawl4aiCrawl\n\n                self.crawl_engine = Crawl4aiCrawl(self.config.config)\n            case _:\n                raise ValueError(f\"Unsupported crawl engine: {crawl_engine}\")\n        # llm for web_qa\n        self.llm = SimplifiedAsyncOpenAI(\n            **self.config.config_llm.model_provider.model_dump() if self.config.config_llm else {}\n        )\n        self.summary_token_limit = self.config.config.get(\"summary_token_limit\", 1_000)\n\n    @register_tool\n    async def search(self, query: str, num_results: int = 5) -&gt; dict:\n        \"\"\"web search to gather information from the web.\n\n        Tips:\n        1. search query should be concrete and not vague or super long\n        2. try to add Google search operators in query if necessary,\n        - \" \" for exact match;\n        - -xxx for exclude;\n        - * wildcard matching;\n        - filetype:xxx for file types;\n        - site:xxx for site search;\n        - before:YYYY-MM-DD, after:YYYY-MM-DD for time range.\n\n        Args:\n            query (str): The query to search for.\n            num_results (int, optional): The number of results to return. Defaults to 5.\n        \"\"\"\n        # https://serper.dev/playground\n        logger.info(f\"[tool] search: {oneline_object(query)}\")\n        res = await self.search_engine.search(query, num_results)\n        logger.info(oneline_object(res))\n        return res\n\n    @register_tool\n    async def web_qa(self, url: str, query: str) -&gt; str:\n        \"\"\"Ask question to a webpage, you will get the answer and related links from the specified url.\n\n        Tips:\n        - Use cases: gather information from a webpage, ask detailed questions.\n\n        Args:\n            url (str): The url to ask question to.\n            query (str): The question to ask. Should be clear, concise, and specific.\n        \"\"\"\n        logger.info(f\"[tool] web_qa: {oneline_object({url, query})}\")\n        content = await self.crawl_engine.crawl(url)\n        query = (\n            query or \"Summarize the content of this webpage, in the same language as the webpage.\"\n        )  # use the same language\n        res_summary, res_links = await asyncio.gather(\n            self._qa(content, query), self._extract_links(url, content, query)\n        )\n        result = f\"Summary: {res_summary}\\n\\nRelated Links: {res_links}\"\n        return result\n\n    async def _qa(self, content: str, query: str) -&gt; str:\n        template = TOOL_PROMPTS[\"search_qa\"].format(content=content, query=query)\n        return await self.llm.query_one(\n            messages=[{\"role\": \"user\", \"content\": template}], **self.config.config_llm.model_params.model_dump()\n        )\n\n    async def _extract_links(self, url: str, content: str, query: str) -&gt; str:\n        template = TOOL_PROMPTS[\"search_related\"].format(url=url, content=content, query=query)\n        return await self.llm.query_one(\n            messages=[{\"role\": \"user\", \"content\": template}], **self.config.config_llm.model_params.model_dump()\n        )\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.search","title":"search  <code>async</code>","text":"<pre><code>search(query: str, num_results: int = 5) -&gt; dict\n</code></pre> <p>web search to gather information from the web.</p> <p>Tips: 1. search query should be concrete and not vague or super long 2. try to add Google search operators in query if necessary, - \" \" for exact match; - -xxx for exclude; - * wildcard matching; - filetype:xxx for file types; - site:xxx for site search; - before:YYYY-MM-DD, after:YYYY-MM-DD for time range.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to search for.</p> required <code>num_results</code> <code>int</code> <p>The number of results to return. Defaults to 5.</p> <code>5</code> Source code in <code>utu/tools/search_toolkit.py</code> <pre><code>@register_tool\nasync def search(self, query: str, num_results: int = 5) -&gt; dict:\n    \"\"\"web search to gather information from the web.\n\n    Tips:\n    1. search query should be concrete and not vague or super long\n    2. try to add Google search operators in query if necessary,\n    - \" \" for exact match;\n    - -xxx for exclude;\n    - * wildcard matching;\n    - filetype:xxx for file types;\n    - site:xxx for site search;\n    - before:YYYY-MM-DD, after:YYYY-MM-DD for time range.\n\n    Args:\n        query (str): The query to search for.\n        num_results (int, optional): The number of results to return. Defaults to 5.\n    \"\"\"\n    # https://serper.dev/playground\n    logger.info(f\"[tool] search: {oneline_object(query)}\")\n    res = await self.search_engine.search(query, num_results)\n    logger.info(oneline_object(res))\n    return res\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.web_qa","title":"web_qa  <code>async</code>","text":"<pre><code>web_qa(url: str, query: str) -&gt; str\n</code></pre> <p>Ask question to a webpage, you will get the answer and related links from the specified url.</p> <p>Tips: - Use cases: gather information from a webpage, ask detailed questions.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to ask question to.</p> required <code>query</code> <code>str</code> <p>The question to ask. Should be clear, concise, and specific.</p> required Source code in <code>utu/tools/search_toolkit.py</code> <pre><code>@register_tool\nasync def web_qa(self, url: str, query: str) -&gt; str:\n    \"\"\"Ask question to a webpage, you will get the answer and related links from the specified url.\n\n    Tips:\n    - Use cases: gather information from a webpage, ask detailed questions.\n\n    Args:\n        url (str): The url to ask question to.\n        query (str): The question to ask. Should be clear, concise, and specific.\n    \"\"\"\n    logger.info(f\"[tool] web_qa: {oneline_object({url, query})}\")\n    content = await self.crawl_engine.crawl(url)\n    query = (\n        query or \"Summarize the content of this webpage, in the same language as the webpage.\"\n    )  # use the same language\n    res_summary, res_links = await asyncio.gather(\n        self._qa(content, query), self._extract_links(url, content, query)\n    )\n    result = f\"Summary: {res_summary}\\n\\nRelated Links: {res_links}\"\n    return result\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/search_toolkit/#utu.tools.search_toolkit.SearchToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/","title":"<code>TabularDataToolkit</code>","text":"<ul> <li>[ ] enhance expressiveness of the returned file structure.</li> </ul>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit","title":"TabularDataToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/tabular_data_toolkit.py</code> <pre><code>class TabularDataToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None):\n        super().__init__(config)\n        self.llm = SimplifiedAsyncOpenAI(\n            **self.config.config_llm.model_provider.model_dump() if self.config.config_llm else {}\n        )\n\n    def get_tabular_columns(self, file_path: str, return_feat: list[str] = None) -&gt; str:\n        logger.info(f\"[tool] get_tabular_columns: {file_path}\")\n        if not os.path.exists(file_path):\n            return self._stringify_column_info([{\"error\": f\"File '{file_path}' does not exist.\"}])\n\n        try:\n            # 1. Load the tabular data using the helper function\n            df = self._load_tabular_data(file_path)\n            # 2. Build column information\n            column_info = []\n            for col in df.columns:\n                try:\n                    # Get data type\n                    dtype = str(df[col].dtype)\n\n                    # Get a non-null sample value\n                    sample_value = None\n                    non_null_values = df[col].dropna()\n                    if len(non_null_values) &gt; 0:\n                        # Get the first non-null value as sample\n                        sample_value = non_null_values.iloc[0]\n                        # Convert to string, handling different data types\n                        if pd.isna(sample_value):\n                            sample_str = \"NaN\"\n                        elif isinstance(sample_value, float):\n                            if math.isnan(sample_value):\n                                sample_str = \"NaN\"\n                            else:\n                                sample_str = str(sample_value)\n                        else:\n                            sample_str = str(sample_value)\n                    else:\n                        sample_str = \"No data\"\n\n                    column_info.append({\"column_name\": str(col), \"type\": dtype, \"sample\": sample_str})\n\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.warning(f\"Error processing column '{col}': {e}\")\n                    column_info.append({\"column_name\": str(col), \"type\": \"unknown\", \"sample\": \"Error reading sample\"})\n\n            return self._stringify_column_info(column_info, return_feat=return_feat)\n\n        except Exception as e:  # pylint: disable=broad-except\n            error_msg = f\"Error reading file '{file_path}': {str(e)}\"\n            logger.error(error_msg)\n            return self._stringify_column_info([{\"error\": error_msg}], return_feat=return_feat)\n\n    @register_tool\n    async def get_column_info(self, file_path: str) -&gt; str:\n        \"\"\"Get basic column information from a tabular data file (e.g. csv, xlsx).\n\n        Args:\n            file_path (str): Path to the tabular data file.\n\n        Returns:\n            str: Basic column information including column name, type, and sample value.\n        \"\"\"\n        column_info_str = self.get_tabular_columns(file_path)\n        prompt = TOOL_PROMPTS[\"tabular_column_info\"].format(column_info=column_info_str)\n        logger.info(f\"[tool] get_column_info: {file_path}\")\n\n        response = await self.llm.query_one(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            # **self.config.config_llm.model_params.model_dump()\n        )\n        return response\n\n    def _load_tabular_data(self, file_path: str) -&gt; \"pd.DataFrame\":\n        # Get file extension to determine how to read the file\n        file_ext = pathlib.Path(file_path).suffix.lower()\n\n        # Read the file based on its extension\n        if file_ext == \".csv\":\n            # Try different encodings for CSV files\n            encodings = [\"utf-8\", \"latin1\", \"cp1252\", \"iso-8859-1\"]\n            df = None\n            for encoding in encodings:\n                try:\n                    df = pd.read_csv(file_path, encoding=encoding)\n                    break\n                except UnicodeDecodeError:\n                    continue\n            if df is None:\n                raise Exception(\"Could not read CSV file with any supported encoding\")\n        elif file_ext in [\".xlsx\", \".xls\"]:\n            df = pd.read_excel(file_path)\n        elif file_ext == \".json\":\n            # Try to read JSON as tabular data\n            df = pd.read_json(file_path)\n        elif file_ext == \".parquet\":\n            df = pd.read_parquet(file_path)\n        elif file_ext == \".tsv\":\n            # Tab-separated values\n            encodings = [\"utf-8\", \"latin1\", \"cp1252\", \"iso-8859-1\"]\n            df = None\n            for encoding in encodings:\n                try:\n                    df = pd.read_csv(file_path, sep=\"\\t\", encoding=encoding)\n                    break\n                except UnicodeDecodeError:\n                    continue\n            if df is None:\n                raise Exception(\"Could not read TSV file with any supported encoding\")\n        else:\n            # Try to read as CSV by default\n            try:\n                df = pd.read_csv(file_path)\n            except Exception as e:  # pylint: disable=broad-except\n                raise Exception(f\"Unsupported file format: {file_ext}\") from e\n\n        return df\n\n    def _stringify_column_info(self, column_info: list[dict], return_feat: list[str] = None) -&gt; str:\n        \"\"\"Convert column information to a formatted string.\"\"\"\n        if \"error\" in column_info[0]:\n            return column_info[0][\"error\"]\n\n        lines = []\n        return_keys = [\"column_name\", \"type\", \"sample\"]\n        if return_feat:\n            return_keys = [key for key in return_keys if key in return_feat]\n        for i, col in enumerate(column_info):\n            lines.append(\n                f\"- Column {i + 1}: {json.dumps({k: col[k] for k in return_keys if k in col}, ensure_ascii=False)}\"\n            )\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_column_info","title":"get_column_info  <code>async</code>","text":"<pre><code>get_column_info(file_path: str) -&gt; str\n</code></pre> <p>Get basic column information from a tabular data file (e.g. csv, xlsx).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the tabular data file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Basic column information including column name, type, and sample value.</p> Source code in <code>utu/tools/tabular_data_toolkit.py</code> <pre><code>@register_tool\nasync def get_column_info(self, file_path: str) -&gt; str:\n    \"\"\"Get basic column information from a tabular data file (e.g. csv, xlsx).\n\n    Args:\n        file_path (str): Path to the tabular data file.\n\n    Returns:\n        str: Basic column information including column name, type, and sample value.\n    \"\"\"\n    column_info_str = self.get_tabular_columns(file_path)\n    prompt = TOOL_PROMPTS[\"tabular_column_info\"].format(column_info=column_info_str)\n    logger.info(f\"[tool] get_column_info: {file_path}\")\n\n    response = await self.llm.query_one(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        # **self.config.config_llm.model_params.model_dump()\n    )\n    return response\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/tabular_data_toolkit/#utu.tools.tabular_data_toolkit.TabularDataToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tool/video_toolkit/","title":"<code>VideoToolkit</code>","text":"<p>https://github.com/googleapis/python-genai https://ai.google.dev/gemini-api/docs/api-key</p>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit","title":"VideoToolkit","text":"<p>               Bases: <code>AsyncBaseToolkit</code></p> Source code in <code>utu/tools/video_toolkit.py</code> <pre><code>class VideoToolkit(AsyncBaseToolkit):\n    def __init__(self, config: ToolkitConfig = None) -&gt; None:\n        super().__init__(config)\n        self.client = genai.Client(\n            api_key=self.config.config.get(\"google_api_key\"), http_options=HttpOptions(api_version=\"v1alpha\")\n        )\n        self.model = self.config.config.get(\"google_model\")\n\n    @register_tool\n    async def video_qa(self, video_url: str, question: str) -&gt; str:\n        r\"\"\"Asks a question about the video.\n\n        Args:\n            video_url (str): The path or URL to the video file.\n            question (str): The question to ask about the video.\n        \"\"\"\n        if not video_url.startswith(\"http\"):\n            video_part = Part.from_uri(file_uri=video_url)\n        else:\n            # e.g. Youtube URL\n            video_part = Part.from_uri(\n                file_uri=video_url,\n                mime_type=\"video/mp4\",\n            )\n        response = self.client.models.generate_content(\n            model=self.model,\n            contents=[\n                question,\n                video_part,\n            ],\n        )\n\n        logger.debug(f\"Video analysis response from gemini: {response.text}\")\n        return response.text\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.tools_map","title":"tools_map  <code>property</code>","text":"<pre><code>tools_map: dict[str, Callable]\n</code></pre> <p>Lazy loading of tools map. - collect tools registered by @register_tool</p>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.video_qa","title":"video_qa  <code>async</code>","text":"<pre><code>video_qa(video_url: str, question: str) -&gt; str\n</code></pre> <p>Asks a question about the video.</p> <p>Parameters:</p> Name Type Description Default <code>video_url</code> <code>str</code> <p>The path or URL to the video file.</p> required <code>question</code> <code>str</code> <p>The question to ask about the video.</p> required Source code in <code>utu/tools/video_toolkit.py</code> <pre><code>@register_tool\nasync def video_qa(self, video_url: str, question: str) -&gt; str:\n    r\"\"\"Asks a question about the video.\n\n    Args:\n        video_url (str): The path or URL to the video file.\n        question (str): The question to ask about the video.\n    \"\"\"\n    if not video_url.startswith(\"http\"):\n        video_part = Part.from_uri(file_uri=video_url)\n    else:\n        # e.g. Youtube URL\n        video_part = Part.from_uri(\n            file_uri=video_url,\n            mime_type=\"video/mp4\",\n        )\n    response = self.client.models.generate_content(\n        model=self.model,\n        contents=[\n            question,\n            video_part,\n        ],\n    )\n\n    logger.debug(f\"Video analysis response from gemini: {response.text}\")\n    return response.text\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_map_func","title":"get_tools_map_func","text":"<pre><code>get_tools_map_func() -&gt; dict[str, Callable]\n</code></pre> <p>Get tools map. It will filter tools by config.activated_tools if it is not None.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_map_func(self) -&gt; dict[str, Callable]:\n    \"\"\"Get tools map. It will filter tools by config.activated_tools if it is not None.\"\"\"\n    if self.config.activated_tools:\n        assert all(tool_name in self.tools_map for tool_name in self.config.activated_tools), (\n            f\"Error config activated tools: {self.config.activated_tools}! available tools: {self.tools_map.keys()}\"\n        )\n        tools_map = {tool_name: self.tools_map[tool_name] for tool_name in self.config.activated_tools}\n    else:\n        tools_map = self.tools_map\n    return tools_map\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_in_agents","title":"get_tools_in_agents","text":"<pre><code>get_tools_in_agents() -&gt; list[FunctionTool]\n</code></pre> <p>Get tools in openai-agents format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_agents(self) -&gt; list[FunctionTool]:\n    \"\"\"Get tools in openai-agents format.\"\"\"\n    tools_map = self.get_tools_map_func()\n    tools = []\n    for _, tool in tools_map.items():\n        tools.append(\n            function_tool(\n                tool,\n                strict_mode=False,  # turn off strict mode\n            )\n        )\n    return tools\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_in_openai","title":"get_tools_in_openai","text":"<pre><code>get_tools_in_openai() -&gt; list[dict]\n</code></pre> <p>Get tools in OpenAI format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_openai(self) -&gt; list[dict]:\n    \"\"\"Get tools in OpenAI format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [ChatCompletionConverter.tool_to_openai(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.get_tools_in_mcp","title":"get_tools_in_mcp","text":"<pre><code>get_tools_in_mcp() -&gt; list[Tool]\n</code></pre> <p>Get tools in MCP format.</p> Source code in <code>utu/tools/base.py</code> <pre><code>def get_tools_in_mcp(self) -&gt; list[types.Tool]:\n    \"\"\"Get tools in MCP format.\"\"\"\n    tools = self.get_tools_in_agents()\n    return [MCPConverter.function_tool_to_mcp(tool) for tool in tools]\n</code></pre>"},{"location":"ref/tool/video_toolkit/#utu.tools.video_toolkit.VideoToolkit.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(name: str, arguments: dict) -&gt; str\n</code></pre> <p>Call a tool by its name.</p> Source code in <code>utu/tools/base.py</code> <pre><code>async def call_tool(self, name: str, arguments: dict) -&gt; str:\n    \"\"\"Call a tool by its name.\"\"\"\n    tools_map = self.get_tools_map_func()\n    if name not in tools_map:\n        raise ValueError(f\"Tool {name} not found\")\n    tool = tools_map[name]\n    return await tool(**arguments)\n</code></pre>"},{"location":"ref/tracing/db_tracer/","title":"<code>DBTracingProcessor</code>","text":"<p>               Bases: <code>TracingProcessor</code></p> <p>Basic tracing processor that stores events into database.</p> <p>Required environment variables: <code>UTU_DB_URL</code></p> Source code in <code>utu/tracing/db_tracer.py</code> <pre><code>class DBTracingProcessor(TracingProcessor):\n    \"\"\"Basic tracing processor that stores events into database.\n\n    Required environment variables: `UTU_DB_URL`\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        if not SQLModelUtils.check_db_available():\n            logger.warning(\"UTU_DB_URL not set or connection failed! Tracing will not be stored into database!\")\n            self.enabled = False\n        else:\n            self.enabled = True\n\n    def on_trace_start(self, trace: Trace) -&gt; None:\n        pass\n\n    def on_trace_end(self, trace: Trace) -&gt; None:\n        pass\n\n    def on_span_start(self, span: Span[Any]) -&gt; None:\n        pass\n\n    def on_span_end(self, span: Span[Any]) -&gt; None:\n        if not self.enabled:\n            return\n\n        data = span.span_data\n        if isinstance(data, GenerationSpanData):\n            with SQLModelUtils.create_session() as session:\n                session.add(\n                    GenerationTracingModel(\n                        trace_id=get_current_trace().trace_id,\n                        span_id=span.span_id,\n                        input=data.input,\n                        output=data.output,\n                        model=data.model,\n                        model_configs=data.model_config,\n                        usage=data.usage,\n                    )\n                )\n                session.commit()\n        elif isinstance(data, ResponseSpanData):\n            # print(f\"&gt; response_id={data.response.id}: {data.response.model_dump()}\")\n            with SQLModelUtils.create_session() as session:\n                session.add(\n                    GenerationTracingModel(\n                        trace_id=get_current_trace().trace_id,\n                        span_id=span.span_id,\n                        input=data.input,\n                        output=OpenAIUtils.get_response_output(data.response),\n                        model=OpenAIUtils.maybe_basemodel_to_dict(data.response.model),\n                        model_configs=OpenAIUtils.get_response_configs(data.response),\n                        usage=OpenAIUtils.maybe_basemodel_to_dict(data.response.usage),\n                        type=\"responses\",\n                        response_id=data.response.id,\n                    )\n                )\n                session.commit()\n        elif isinstance(data, FunctionSpanData):\n            with SQLModelUtils.create_session() as session:\n                session.add(\n                    ToolTracingModel(\n                        name=data.name,\n                        input=data.input,\n                        output=data.output,\n                        mcp_data=data.mcp_data,\n                        trace_id=get_current_trace().trace_id,\n                        span_id=span.span_id,\n                    )\n                )\n                session.commit()\n\n    def force_flush(self) -&gt; None:\n        pass\n\n    def shutdown(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"ref/utils/agents_utils/","title":"<code>AgentsUtils</code>","text":""},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.ChatCompletionConverter","title":"ChatCompletionConverter","text":"<p>               Bases: <code>Converter</code></p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>class ChatCompletionConverter(Converter):\n    @classmethod\n    def items_to_messages(cls, items: str | Iterable[TResponseInputItem]) -&gt; list[ChatCompletionMessageParam]:\n        # skip reasoning, see chatcmpl_converter.Converter.items_to_messages()\n        # agents.exceptions.UserError: Unhandled item type or structure:\n        # {'id': '__fake_id__', 'summary': [{'text': '...', 'type': 'summary_text'}], 'type': 'reasoning'}\n        if not isinstance(items, str):  # TODO: check it!\n            items = cls.filter_items(items)\n        return Converter.items_to_messages(items)\n\n    @classmethod\n    def filter_items(cls, items: str | Iterable[TResponseInputItem]) -&gt; str | list[TResponseInputItem]:\n        if isinstance(items, str):\n            return items\n        filtered_items = []\n        for item in items:\n            if item.get(\"type\", None) == \"reasoning\":\n                continue\n            filtered_items.append(item)\n        return filtered_items\n\n    @classmethod\n    def items_to_dict(cls, items: str | Iterable[TResponseInputItem]) -&gt; list[dict]:\n        \"\"\"convert items to a list of dict which have {\"role\", \"content\"}\n        WIP!\n        \"\"\"\n        if isinstance(items, str):\n            return [{\"role\": \"user\", \"content\": items}]\n        result = []\n        for item in items:\n            if msg := Converter.maybe_easy_input_message(item):\n                result.append(msg)\n            elif msg := Converter.maybe_input_message(item):\n                result.append(msg)\n            elif msg := Converter.maybe_response_output_message(item):\n                result.append(msg)\n            elif msg := Converter.maybe_file_search_call(item):\n                msg.update({\"role\": \"tool\", \"content\": msg[\"results\"]})\n                result.append(msg)\n            elif msg := Converter.maybe_function_tool_call(item):\n                msg.update({\"role\": \"assistant\", \"content\": f\"{msg['name']}({msg['arguments']})\"})\n                result.append(msg)\n            elif msg := Converter.maybe_function_tool_call_output(item):\n                msg.update({\"role\": \"tool\", \"content\": msg[\"output\"], \"tool_call_id\": msg[\"call_id\"]})\n                result.append(msg)\n            elif msg := Converter.maybe_reasoning_message(item):\n                msg.update({\"role\": \"assistant\", \"content\": msg[\"summary\"]})\n                result.append(msg)\n            else:\n                logger.warning(f\"Unknown message type: {item}\")\n                result.append({\"role\": \"assistant\", \"content\": f\"Unknown message type: {item}\"})\n        return result\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.ChatCompletionConverter.items_to_dict","title":"items_to_dict  <code>classmethod</code>","text":"<pre><code>items_to_dict(\n    items: str | Iterable[TResponseInputItem],\n) -&gt; list[dict]\n</code></pre> <p>convert items to a list of dict which have {\"role\", \"content\"} WIP!</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@classmethod\ndef items_to_dict(cls, items: str | Iterable[TResponseInputItem]) -&gt; list[dict]:\n    \"\"\"convert items to a list of dict which have {\"role\", \"content\"}\n    WIP!\n    \"\"\"\n    if isinstance(items, str):\n        return [{\"role\": \"user\", \"content\": items}]\n    result = []\n    for item in items:\n        if msg := Converter.maybe_easy_input_message(item):\n            result.append(msg)\n        elif msg := Converter.maybe_input_message(item):\n            result.append(msg)\n        elif msg := Converter.maybe_response_output_message(item):\n            result.append(msg)\n        elif msg := Converter.maybe_file_search_call(item):\n            msg.update({\"role\": \"tool\", \"content\": msg[\"results\"]})\n            result.append(msg)\n        elif msg := Converter.maybe_function_tool_call(item):\n            msg.update({\"role\": \"assistant\", \"content\": f\"{msg['name']}({msg['arguments']})\"})\n            result.append(msg)\n        elif msg := Converter.maybe_function_tool_call_output(item):\n            msg.update({\"role\": \"tool\", \"content\": msg[\"output\"], \"tool_call_id\": msg[\"call_id\"]})\n            result.append(msg)\n        elif msg := Converter.maybe_reasoning_message(item):\n            msg.update({\"role\": \"assistant\", \"content\": msg[\"summary\"]})\n            result.append(msg)\n        else:\n            logger.warning(f\"Unknown message type: {item}\")\n            result.append({\"role\": \"assistant\", \"content\": f\"Unknown message type: {item}\"})\n    return result\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils","title":"AgentsUtils","text":"<p>Utils for openai-agents SDK</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>class AgentsUtils:\n    \"\"\"Utils for openai-agents SDK\"\"\"\n\n    @staticmethod\n    def generate_group_id() -&gt; str:\n        \"\"\"Generate a unique group ID. (Used in OpenAI tracing)\n        Ref: https://openai.github.io/openai-agents-python/tracing/\n        \"\"\"\n        return uuid.uuid4().hex[:16]\n\n    @staticmethod\n    def gen_trace_id() -&gt; str:\n        return gen_trace_id()\n\n    @staticmethod\n    def get_current_trace() -&gt; Trace:\n        return get_current_trace()\n\n    @staticmethod\n    def get_agents_model(\n        type: Literal[\"responses\", \"chat.completions\", \"litellm\"] = None,\n        model: str = None,\n        base_url: str = None,\n        api_key: str = None,\n    ) -&gt; Model:\n        type = type or os.getenv(\"UTU_LLM_TYPE\", \"chat.completions\")\n        model = model or os.getenv(\"UTU_LLM_MODEL\")\n        if type == \"litellm\":\n            # Ref: https://docs.litellm.ai/docs/providers\n            # NOTE: should set .evn properly! e.g. AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION for Azure\n            #   https://docs.litellm.ai/docs/providers/azure/\n            from agents.extensions.models.litellm_model import LitellmModel\n\n            return LitellmModel(model=model)\n\n        base_url = base_url or os.getenv(\"UTU_LLM_BASE_URL\")\n        api_key = api_key or os.getenv(\"UTU_LLM_API_KEY\")\n        if not api_key or not base_url:\n            raise ValueError(\"UTU_LLM_API_KEY and UTU_LLM_BASE_URL must be set\")\n        openai_client = AsyncOpenAI(\n            api_key=api_key,\n            base_url=base_url,\n            timeout=100,\n        )\n        if type == \"chat.completions\":\n            return OpenAIChatCompletionsModel(model=model, openai_client=openai_client)\n        elif type == \"responses\":\n            return OpenAIResponsesModel(model=model, openai_client=openai_client)\n        else:\n            raise ValueError(\"Invalid type: \" + type)\n\n    @staticmethod\n    def get_trajectory_from_agent_result(agent_result: RunResult, agent_name: str = None) -&gt; dict:\n        if agent_name is None:\n            agent_name = agent_result.last_agent.name\n        return {\n            \"agent\": agent_name,\n            \"trajectory\": ChatCompletionConverter.items_to_messages(agent_result.to_input_list()),\n        }\n\n    @staticmethod\n    def print_new_items(new_items: list[RunItem]) -&gt; None:\n        \"\"\"Print new items generated by Runner.run()\"\"\"\n        for new_item in new_items:\n            agent_name = new_item.agent.name\n            if isinstance(new_item, MessageOutputItem):\n                PrintUtils.print_bot(f\"{agent_name}: {ItemHelpers.text_message_output(new_item)}\")\n            elif isinstance(new_item, HandoffOutputItem):\n                PrintUtils.print_info(f\"Handed off from {new_item.source_agent.name} to {new_item.target_agent.name}\")\n            elif isinstance(new_item, ToolCallItem):\n                assert isinstance(new_item.raw_item, ResponseFunctionToolCall)  # DONOT use openai's built-in tools\n                PrintUtils.print_info(\n                    f\"{agent_name}: Calling a tool: {new_item.raw_item.name}({json.loads(new_item.raw_item.arguments)})\"\n                )\n            elif isinstance(new_item, ToolCallOutputItem):\n                PrintUtils.print_tool(f\"Tool call output: {new_item.output}\")\n            elif isinstance(new_item, ReasoningItem):\n                PrintUtils.print_info(f\"{agent_name}: Reasoning: {new_item.raw_item}\")\n            else:\n                PrintUtils.print_info(f\"{agent_name}: Skipping item: {new_item.__class__.__name__}\")\n\n    @staticmethod\n    async def print_stream_events(result: AsyncIterator[StreamEvent]) -&gt; None:\n        \"\"\"Print stream events generated by Runner.run_streamed()\"\"\"\n        async for event in result:\n            # print(f\"&gt; [DEBUG] event: {event}\")\n            if isinstance(event, RawResponsesStreamEvent):\n                # event.data -- ResponseStreamEvent\n                if event.data.type == \"response.output_item.added\":\n                    match event.data.item.type:\n                        # computer_call, code_interpreter_call, custom_tool_call, file_search_call, function_call,\n                        # we_search_call, image_generation_call, local_shell_call,\n                        # mcp_call, mcp_list_tools, mcp_approval_request, message, reasoning\n                        case \"message\":\n                            pass\n                        case \"function_call\":\n                            PrintUtils.print_bot(\n                                f\"&lt;toolcall name={event.data.item.name}&gt;{event.data.item.arguments}\", end=\"\"\n                            )\n                        case _:\n                            PrintUtils.print_bot(f\"&lt;{event.data.item.type}&gt;\", end=\"\")\n                elif event.data.type == \"response.output_item.done\":\n                    match event.data.item.type:\n                        case \"message\":\n                            pass\n                            # PrintUtils.print_bot(\"\")  # add a new line?\n                        case \"function_call\":\n                            PrintUtils.print_bot(\"&lt;/toolcall&gt;\")\n                        case _:\n                            # PrintUtils.print_bot(f\"&lt;/{event.data.item.type}&gt;\")\n                            logger.info(f\"&lt;/{event.data.item.type}&gt;\")  # It seems that vllm's output order is wrong\n                elif event.data.type == \"response.content_part.added\":\n                    match event.data.part.type:\n                        # output_text, refusal\n                        case \"output_text\":\n                            pass\n                        case \"refusal\":\n                            PrintUtils.print_bot(f\"&lt;refusal&gt;{event.data.part.refusal}\", end=\"\")\n                        case _:\n                            logger.warning(f\"Unknown part type: {event.data.part.type}! {event}\")\n                elif event.data.type == \"response.content_part.done\":\n                    match event.data.part.type:\n                        case \"output_text\":\n                            pass\n                        case \"refusal\":\n                            PrintUtils.print_bot(\"&lt;/refusal&gt;\")\n                        case _:\n                            logger.warning(f\"Unknown part type: {event.data.part.type}! {event}\")\n                elif event.data.type == \"response.reasoning_summary_part.added\":\n                    PrintUtils.print_info(\"&lt;reasoning_summary&gt;\", end=\"\")\n                elif event.data.type == \"response.reasoning_summary_part.done\":\n                    # PrintUtils.print_info(\"&lt;/reasoning_summary&gt;\", end=\"\")\n                    logger.info(\"&lt;/reasoning_summary&gt;\")  # It seems that vllm's output order is wrong\n                elif event.data.type == \"response.reasoning_summary_text.delta\":\n                    PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n                elif event.data.type == \"response.function_call_arguments.delta\":\n                    PrintUtils.print_bot(f\"{event.data.delta}\", end=\"\")\n                elif event.data.type == \"response.function_call_arguments.done\":\n                    pass\n                elif event.data.type == \"response.output_text.delta\":\n                    PrintUtils.print_bot(f\"{event.data.delta}\", end=\"\")\n                elif event.data.type == \"response.reasoning_text.delta\":\n                    PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n                elif event.data.type == \"response.reasoning_text.done\":\n                    PrintUtils.print_info(\"&lt;/reasoning_text&gt;\", end=\"\")\n                elif event.data.type in (\"response.output_text.done\",):\n                    PrintUtils.print_info(\"\")\n                elif event.data.type in (\n                    \"response.created\",\n                    \"response.completed\",\n                    \"response.in_progress\",\n                ):\n                    pass\n                else:\n                    PrintUtils.print_info(f\"Unknown event type: {event.data.type}! {event}\")\n                    # raise ValueError(f\"Unknown event type: {event.data.type}\")\n            elif isinstance(event, RunItemStreamEvent):\n                item: RunItem = event.item\n                if item.type == \"message_output_item\":\n                    pass  # do not print twice to avoid duplicate! (already handled `response.output_text.delta`)\n                    # PrintUtils.print_bot(f\"{ItemHelpers.text_message_output(item).strip()}\")\n                elif item.type == \"reasoning_item\":\n                    pass\n                elif item.type == \"tool_call_item\":\n                    pass\n                    # PrintUtils.print_bot([tool_call] {item.raw_item.name}({item.raw_item.arguments})\")\n                elif item.type == \"tool_call_output_item\":\n                    PrintUtils.print_tool(f\"[tool_output] {item.output}\")  # item.raw_item\n                elif item.type == \"handoff_call_item\":  # same as `ToolCallItem`\n                    PrintUtils.print_bot(f\"[handoff_call] {item.raw_item.name}({item.raw_item.arguments})\")\n                elif item.type == \"handoff_output_item\":\n                    PrintUtils.print_info(f\"&gt;&gt; Handoff from {item.source_agent.name} to {item.target_agent.name}\")\n                elif event.type in (\n                    \"mcp_list_tools_item\",\n                    \"mcp_approval_request_item\",\n                    \"mcp_approval_response_item\",\n                ):\n                    PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {event}\")\n                else:\n                    PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {item.__class__.__name__}\")\n            elif isinstance(event, AgentUpdatedStreamEvent):\n                PrintUtils.print_info(f\"&gt;&gt; new agent: {event.new_agent.name}\")\n            # skip events from youtu-agent\n            elif event.type in (\"orchestrator_stream_event\", \"orchestra_stream_event\", \"simple_agent_generated\"):\n                pass\n            else:\n                logger.warning(f\"Unknown event type: {event.type}! {event}\")\n        print()  # Newline after stream?\n\n    @staticmethod\n    def convert_model_settings(params: OpenAIChatCompletionParams) -&gt; ModelSettings:\n        # \"tools\", \"messages\", \"model\"\n        # FIXME: move to extra_args\n        for p in (\"max_completion_tokens\", \"top_logprobs\", \"logprobs\", \"seed\", \"stop\"):\n            if p in params:\n                logger.warning(f\"Parameter `{p}` is not supported in ModelSettings\")\n        return ModelSettings(\n            max_tokens=params.get(\"max_tokens\", None),\n            temperature=params.get(\"temperature\", None),\n            top_p=params.get(\"top_p\", None),\n            frequency_penalty=params.get(\"frequency_penalty\", None),\n            presence_penalty=params.get(\"presence_penalty\", None),\n            tool_choice=params.get(\"tool_choice\", None),\n            parallel_tool_calls=params.get(\"parallel_tool_calls\", None),\n            extra_query=params.get(\"extra_query\", None),\n            extra_body=params.get(\"extra_body\", None),\n            extra_headers=params.get(\"extra_headers\", None),\n        )\n\n    @staticmethod\n    def convert_sp_input(\n        messages: list[ChatCompletionMessageParam],\n    ) -&gt; tuple[str | None, str | list[TResponseInputItem]]:\n        if isinstance(messages, str):\n            return None, messages\n        if messages[0].get(\"role\", None) == \"system\":\n            return messages[0][\"content\"], messages[1:]\n        return None, messages\n\n    @staticmethod\n    def convert_tool(tool: ChatCompletionToolParam) -&gt; FunctionTool:\n        assert tool[\"type\"] == \"function\"\n        return FunctionTool(\n            name=tool[\"function\"][\"name\"],\n            description=tool[\"function\"].get(\"description\", \"\"),\n            params_json_schema=tool[\"function\"].get(\"parameters\", None),\n            on_invoke_tool=None,\n        )\n\n    @staticmethod\n    def get_message_from_image(image_url: str) -&gt; dict:\n        \"\"\"Get a message dict for image input.\"\"\"\n        # from openai.types.responses.response_input_item_param import Message\n        # from openai.types.responses.response_input_image_param import ResponseInputImageParam\n        return {\"role\": \"user\", \"content\": [{\"type\": \"input_image\", \"image_url\": encode_image(image_url)}]}\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils.generate_group_id","title":"generate_group_id  <code>staticmethod</code>","text":"<pre><code>generate_group_id() -&gt; str\n</code></pre> <p>Generate a unique group ID. (Used in OpenAI tracing) Ref: https://openai.github.io/openai-agents-python/tracing/</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@staticmethod\ndef generate_group_id() -&gt; str:\n    \"\"\"Generate a unique group ID. (Used in OpenAI tracing)\n    Ref: https://openai.github.io/openai-agents-python/tracing/\n    \"\"\"\n    return uuid.uuid4().hex[:16]\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils.print_new_items","title":"print_new_items  <code>staticmethod</code>","text":"<pre><code>print_new_items(new_items: list[RunItem]) -&gt; None\n</code></pre> <p>Print new items generated by Runner.run()</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@staticmethod\ndef print_new_items(new_items: list[RunItem]) -&gt; None:\n    \"\"\"Print new items generated by Runner.run()\"\"\"\n    for new_item in new_items:\n        agent_name = new_item.agent.name\n        if isinstance(new_item, MessageOutputItem):\n            PrintUtils.print_bot(f\"{agent_name}: {ItemHelpers.text_message_output(new_item)}\")\n        elif isinstance(new_item, HandoffOutputItem):\n            PrintUtils.print_info(f\"Handed off from {new_item.source_agent.name} to {new_item.target_agent.name}\")\n        elif isinstance(new_item, ToolCallItem):\n            assert isinstance(new_item.raw_item, ResponseFunctionToolCall)  # DONOT use openai's built-in tools\n            PrintUtils.print_info(\n                f\"{agent_name}: Calling a tool: {new_item.raw_item.name}({json.loads(new_item.raw_item.arguments)})\"\n            )\n        elif isinstance(new_item, ToolCallOutputItem):\n            PrintUtils.print_tool(f\"Tool call output: {new_item.output}\")\n        elif isinstance(new_item, ReasoningItem):\n            PrintUtils.print_info(f\"{agent_name}: Reasoning: {new_item.raw_item}\")\n        else:\n            PrintUtils.print_info(f\"{agent_name}: Skipping item: {new_item.__class__.__name__}\")\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils.print_stream_events","title":"print_stream_events  <code>async</code> <code>staticmethod</code>","text":"<pre><code>print_stream_events(\n    result: AsyncIterator[StreamEvent],\n) -&gt; None\n</code></pre> <p>Print stream events generated by Runner.run_streamed()</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@staticmethod\nasync def print_stream_events(result: AsyncIterator[StreamEvent]) -&gt; None:\n    \"\"\"Print stream events generated by Runner.run_streamed()\"\"\"\n    async for event in result:\n        # print(f\"&gt; [DEBUG] event: {event}\")\n        if isinstance(event, RawResponsesStreamEvent):\n            # event.data -- ResponseStreamEvent\n            if event.data.type == \"response.output_item.added\":\n                match event.data.item.type:\n                    # computer_call, code_interpreter_call, custom_tool_call, file_search_call, function_call,\n                    # we_search_call, image_generation_call, local_shell_call,\n                    # mcp_call, mcp_list_tools, mcp_approval_request, message, reasoning\n                    case \"message\":\n                        pass\n                    case \"function_call\":\n                        PrintUtils.print_bot(\n                            f\"&lt;toolcall name={event.data.item.name}&gt;{event.data.item.arguments}\", end=\"\"\n                        )\n                    case _:\n                        PrintUtils.print_bot(f\"&lt;{event.data.item.type}&gt;\", end=\"\")\n            elif event.data.type == \"response.output_item.done\":\n                match event.data.item.type:\n                    case \"message\":\n                        pass\n                        # PrintUtils.print_bot(\"\")  # add a new line?\n                    case \"function_call\":\n                        PrintUtils.print_bot(\"&lt;/toolcall&gt;\")\n                    case _:\n                        # PrintUtils.print_bot(f\"&lt;/{event.data.item.type}&gt;\")\n                        logger.info(f\"&lt;/{event.data.item.type}&gt;\")  # It seems that vllm's output order is wrong\n            elif event.data.type == \"response.content_part.added\":\n                match event.data.part.type:\n                    # output_text, refusal\n                    case \"output_text\":\n                        pass\n                    case \"refusal\":\n                        PrintUtils.print_bot(f\"&lt;refusal&gt;{event.data.part.refusal}\", end=\"\")\n                    case _:\n                        logger.warning(f\"Unknown part type: {event.data.part.type}! {event}\")\n            elif event.data.type == \"response.content_part.done\":\n                match event.data.part.type:\n                    case \"output_text\":\n                        pass\n                    case \"refusal\":\n                        PrintUtils.print_bot(\"&lt;/refusal&gt;\")\n                    case _:\n                        logger.warning(f\"Unknown part type: {event.data.part.type}! {event}\")\n            elif event.data.type == \"response.reasoning_summary_part.added\":\n                PrintUtils.print_info(\"&lt;reasoning_summary&gt;\", end=\"\")\n            elif event.data.type == \"response.reasoning_summary_part.done\":\n                # PrintUtils.print_info(\"&lt;/reasoning_summary&gt;\", end=\"\")\n                logger.info(\"&lt;/reasoning_summary&gt;\")  # It seems that vllm's output order is wrong\n            elif event.data.type == \"response.reasoning_summary_text.delta\":\n                PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n            elif event.data.type == \"response.function_call_arguments.delta\":\n                PrintUtils.print_bot(f\"{event.data.delta}\", end=\"\")\n            elif event.data.type == \"response.function_call_arguments.done\":\n                pass\n            elif event.data.type == \"response.output_text.delta\":\n                PrintUtils.print_bot(f\"{event.data.delta}\", end=\"\")\n            elif event.data.type == \"response.reasoning_text.delta\":\n                PrintUtils.print_info(f\"{event.data.delta}\", end=\"\")\n            elif event.data.type == \"response.reasoning_text.done\":\n                PrintUtils.print_info(\"&lt;/reasoning_text&gt;\", end=\"\")\n            elif event.data.type in (\"response.output_text.done\",):\n                PrintUtils.print_info(\"\")\n            elif event.data.type in (\n                \"response.created\",\n                \"response.completed\",\n                \"response.in_progress\",\n            ):\n                pass\n            else:\n                PrintUtils.print_info(f\"Unknown event type: {event.data.type}! {event}\")\n                # raise ValueError(f\"Unknown event type: {event.data.type}\")\n        elif isinstance(event, RunItemStreamEvent):\n            item: RunItem = event.item\n            if item.type == \"message_output_item\":\n                pass  # do not print twice to avoid duplicate! (already handled `response.output_text.delta`)\n                # PrintUtils.print_bot(f\"{ItemHelpers.text_message_output(item).strip()}\")\n            elif item.type == \"reasoning_item\":\n                pass\n            elif item.type == \"tool_call_item\":\n                pass\n                # PrintUtils.print_bot([tool_call] {item.raw_item.name}({item.raw_item.arguments})\")\n            elif item.type == \"tool_call_output_item\":\n                PrintUtils.print_tool(f\"[tool_output] {item.output}\")  # item.raw_item\n            elif item.type == \"handoff_call_item\":  # same as `ToolCallItem`\n                PrintUtils.print_bot(f\"[handoff_call] {item.raw_item.name}({item.raw_item.arguments})\")\n            elif item.type == \"handoff_output_item\":\n                PrintUtils.print_info(f\"&gt;&gt; Handoff from {item.source_agent.name} to {item.target_agent.name}\")\n            elif event.type in (\n                \"mcp_list_tools_item\",\n                \"mcp_approval_request_item\",\n                \"mcp_approval_response_item\",\n            ):\n                PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {event}\")\n            else:\n                PrintUtils.print_info(f\"  &gt;&gt;&gt; Skipping item: {item.__class__.__name__}\")\n        elif isinstance(event, AgentUpdatedStreamEvent):\n            PrintUtils.print_info(f\"&gt;&gt; new agent: {event.new_agent.name}\")\n        # skip events from youtu-agent\n        elif event.type in (\"orchestrator_stream_event\", \"orchestra_stream_event\", \"simple_agent_generated\"):\n            pass\n        else:\n            logger.warning(f\"Unknown event type: {event.type}! {event}\")\n    print()  # Newline after stream?\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.AgentsUtils.get_message_from_image","title":"get_message_from_image  <code>staticmethod</code>","text":"<pre><code>get_message_from_image(image_url: str) -&gt; dict\n</code></pre> <p>Get a message dict for image input.</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>@staticmethod\ndef get_message_from_image(image_url: str) -&gt; dict:\n    \"\"\"Get a message dict for image input.\"\"\"\n    # from openai.types.responses.response_input_item_param import Message\n    # from openai.types.responses.response_input_image_param import ResponseInputImageParam\n    return {\"role\": \"user\", \"content\": [{\"type\": \"input_image\", \"image_url\": encode_image(image_url)}]}\n</code></pre>"},{"location":"ref/utils/agents_utils/#utu.utils.agents_utils.SimplifiedOpenAIChatCompletionsModel","title":"SimplifiedOpenAIChatCompletionsModel","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>extend OpenAIChatCompletionsModel to support basic api - enable tracing based on SimplifiedAsyncOpenAI</p> Source code in <code>utu/utils/agents_utils.py</code> <pre><code>class SimplifiedOpenAIChatCompletionsModel(OpenAIChatCompletionsModel):\n    \"\"\"extend OpenAIChatCompletionsModel to support basic api\n    - enable tracing based on SimplifiedAsyncOpenAI\n    \"\"\"\n\n    async def query_one(self, **kwargs) -&gt; str:\n        system_instructions, input = AgentsUtils.convert_sp_input(kwargs[\"messages\"])\n        model_settings = AgentsUtils.convert_model_settings(kwargs)\n        tools = [AgentsUtils.convert_tool(tool) for tool in kwargs.get(\"tools\", [])]\n        response = await self.get_response(\n            system_instructions=system_instructions,\n            input=input,\n            model_settings=model_settings,\n            tools=tools,\n            output_schema=None,\n            handoffs=[],\n            tracing=ModelTracing.ENABLED,\n            previous_response_id=None,\n            prompt=None,\n        )\n        return ChatCompletionConverter.items_to_messages(response.to_input_items())\n</code></pre>"},{"location":"ref/utils/openai_utils/","title":"<code>OpenAIUtils</code>","text":""},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.OpenAIUtils","title":"OpenAIUtils","text":"Source code in <code>utu/utils/openai_utils/openai_utils.py</code> <pre><code>class OpenAIUtils:\n    # --------------------------------------------------------\n    # chat completions\n    # --------------------------------------------------------\n    @staticmethod\n    def print_message(message: ChatCompletionMessage) -&gt; None:\n        if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n            PrintUtils.print_info(f\"{message.reasoning_content}\")\n        if message.content:\n            PrintUtils.print_bot(f\"{message.content}\", add_prefix=True)\n        if message.tool_calls:\n            for tool_call in message.tool_calls:\n                PrintUtils.print_bot(f\"&lt;{tool_call.function.name}&gt;{tool_call.function.arguments}\", add_prefix=True)\n\n    @staticmethod\n    async def print_stream(stream: AsyncStream[ChatCompletionChunk]) -&gt; ChatCompletionMessage:\n        final_tool_calls: dict[int, ChatCompletionMessageToolCall] = {}\n        content = \"\"\n        async for chunk in stream:\n            delta = chunk.choices[0].delta\n            if hasattr(delta, \"reasoning_content\") and delta.reasoning_content:\n                PrintUtils.print_info(f\"{delta.reasoning_content}\", end=\"\", color=\"green\")\n            if delta.content:\n                content += delta.content\n                PrintUtils.print_info(f\"{delta.content}\", end=\"\", color=\"gray\")\n            if delta.tool_calls:\n                for tool_call in delta.tool_calls:\n                    index = tool_call.index\n                    if index not in final_tool_calls:\n                        final_tool_calls[index] = tool_call\n                        PrintUtils.print_info(\n                            f\"&lt;{tool_call.function.name}&gt;{tool_call.function.arguments}\", end=\"\", color=\"blue\"\n                        )\n                    else:\n                        if final_tool_calls[index].function.arguments:\n                            final_tool_calls[index].function.arguments += tool_call.function.arguments\n                        else:\n                            final_tool_calls[index].function.arguments = tool_call.function.arguments\n                        PrintUtils.print_info(f\"{tool_call.function.arguments}\", end=\"\", color=\"blue\")\n        PrintUtils.print_info(\"\")  # print a newline\n        tool_calls = [\n            ChatCompletionMessageFunctionToolCall(\n                id=tool_call.id,\n                function=tool_call.function.model_dump(),\n                type=tool_call.type,  # type is always \"function\"\n            )\n            for tool_call in final_tool_calls.values()\n        ]\n        message = ChatCompletionMessage(role=\"assistant\", content=content, tool_calls=tool_calls)\n        OpenAIUtils.print_message(message)\n        return message\n\n    # --------------------------------------------------------\n    # responses\n    # --------------------------------------------------------\n    @staticmethod\n    def print_response(response: Response) -&gt; None:\n        for item in response.output:\n            # print(f\"&gt; responses item: {item}\")\n            match item.type:\n                case \"reasoning\":\n                    content = getattr(item, \"content\", item.summary)\n                    PrintUtils.print_bot(f\"&lt;reasoning&gt;{content}&lt;/reasoning&gt;\", add_prefix=True, color=\"gray\")\n                case \"message\":\n                    PrintUtils.print_bot(f\"{item.content}\", add_prefix=True)\n                case \"function_call\":\n                    PrintUtils.print_info(f\"&lt;{item.name}&gt;({item.arguments})\")\n                case \"file_search_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;({item.queries})\")\n                case \"web_search_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;({item.action})\")\n                case \"computer_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;({item.action})\")\n                case \"image_generation_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt; -&gt; {item.result[:4]}\")\n                case \"code_interpreter_call\":\n                    PrintUtils.print_info(\n                        f\"&lt;{item.type}&gt;(container_id={item.container_id}, code={item.code}) -&gt; {item.outputs}\"\n                    )\n                case \"local_shell_call\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;(action={item.action})\")\n                case \"mcp_list_tools\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;(server={item.server_label}) -&gt; {item.tools}\")\n                case \"mcp_call\":\n                    PrintUtils.print_info(\n                        f\"&lt;{item.type}&gt;(server={item.server_label}) {item.name}({item.arguments}) -&gt; {item.output}\"\n                    )\n                case \"mcp_approval_request\":\n                    PrintUtils.print_info(f\"&lt;{item.type}&gt;(server={item.server_label}) {item.name}({item.arguments})\")\n                case _:\n                    PrintUtils.print_error(f\"Unknown item type: {item.type}\\n{item}\")\n\n    @staticmethod\n    def print_response_stream(stream: AsyncStream[ResponseStreamEvent]) -&gt; Response:\n        raise NotImplementedError\n\n    @staticmethod\n    def get_response_configs(response: Response, include_output: bool = False) -&gt; dict:\n        \"\"\"Get response configs from response\"\"\"\n        data = response.model_dump()\n        if not include_output:\n            del data[\"output\"]\n        return data\n\n    @staticmethod\n    def get_response_output(response: Response) -&gt; list[dict]:\n        \"\"\"Get response output from response\"\"\"\n        return response.model_dump()[\"output\"]\n\n    @classmethod\n    def tool_chatcompletion_to_responses(cls, tool: ChatCompletionToolParam) -&gt; FunctionToolParam:\n        assert tool[\"type\"] == \"function\"\n        return FunctionToolParam(\n            name=tool[\"function\"][\"name\"],\n            description=tool[\"function\"].get(\"description\", \"\"),\n            parameters=tool[\"function\"].get(\"parameters\", None),\n            type=\"function\",\n        )\n\n    @staticmethod\n    def maybe_basemodel_to_dict(obj: Any) -&gt; dict | None:\n        if isinstance(obj, BaseModel):\n            return obj.model_dump()\n        return obj\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.OpenAIUtils.get_response_configs","title":"get_response_configs  <code>staticmethod</code>","text":"<pre><code>get_response_configs(\n    response: Response, include_output: bool = False\n) -&gt; dict\n</code></pre> <p>Get response configs from response</p> Source code in <code>utu/utils/openai_utils/openai_utils.py</code> <pre><code>@staticmethod\ndef get_response_configs(response: Response, include_output: bool = False) -&gt; dict:\n    \"\"\"Get response configs from response\"\"\"\n    data = response.model_dump()\n    if not include_output:\n        del data[\"output\"]\n    return data\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.OpenAIUtils.get_response_output","title":"get_response_output  <code>staticmethod</code>","text":"<pre><code>get_response_output(response: Response) -&gt; list[dict]\n</code></pre> <p>Get response output from response</p> Source code in <code>utu/utils/openai_utils/openai_utils.py</code> <pre><code>@staticmethod\ndef get_response_output(response: Response) -&gt; list[dict]:\n    \"\"\"Get response output from response\"\"\"\n    return response.model_dump()[\"output\"]\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI","title":"SimplifiedAsyncOpenAI","text":"<p>               Bases: <code>AsyncOpenAI</code></p> <p>Simplified OpenAI client for chat.completions and responses API, with default config</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>class SimplifiedAsyncOpenAI(AsyncOpenAI):\n    \"\"\"Simplified OpenAI client for chat.completions and responses API, with default config\"\"\"\n\n    def __init__(\n        self,\n        *,\n        type: Literal[\"chat.completions\", \"responses\"] = None,\n        # openai client kwargs\n        api_key: str | None = None,\n        base_url: str | None = None,\n        # default configs\n        **kwargs: dict,\n    ) -&gt; None:\n        logger.info(f\"&gt; type: {type}, base_url: {base_url}, kwargs: {kwargs}\")\n        super().__init__(\n            api_key=api_key or os.getenv(\"UTU_LLM_API_KEY\") or \"xxx\", base_url=base_url or os.getenv(\"UTU_LLM_BASE_URL\")\n        )\n        self.type = type or os.getenv(\"UTU_LLM_TYPE\", \"chat.completions\")\n        self.type_create_params = (\n            OpenAIChatCompletionParamsKeys if self.type == \"chat.completions\" else OpenAIResponsesParamsKeys\n        )\n        self.default_config = self._process_kwargs(kwargs)\n\n    def _process_kwargs(self, kwargs: dict) -&gt; dict:\n        # parse kwargs for ChatCompletionParams\n        default_config = {}\n        for k, v in kwargs.items():\n            if k in self.type_create_params:\n                default_config[k] = v\n        default_config[\"model\"] = default_config.get(\"model\", os.getenv(\"UTU_LLM_MODEL\"))\n        return default_config\n\n    async def query_one(self, **kwargs) -&gt; str:\n        \"\"\"Simplified chat.complete / responses API\n        WARNING: Only for basic text i/o usage! You should not use the method with querying with customized configs!\n        \"\"\"\n        if \"stream\" in kwargs:\n            assert kwargs[\"stream\"] is False, \"stream is not supported in `query_one`\"\n\n        if self.type == \"chat.completions\":\n            chat_completion: ChatCompletion = await self.chat_completions_create(**kwargs)\n            return chat_completion.choices[0].message.content\n        elif self.type == \"responses\":\n            response: Response = await self.responses_create(**kwargs)\n            return response.output_text  # NOTE: will not return toolcall or reasoning\n        else:\n            raise ValueError(f\"Unknown type: {self.type}\")\n\n    async def chat_completions_create(self, **kwargs) -&gt; ChatCompletion | AsyncStream[ChatCompletionChunk]:\n        assert self.type == \"chat.completions\", \"`chat_completions_create` is not supported for responses API\"\n        unknown_params = self.check_known_keys(kwargs, self.type_create_params)\n        if unknown_params:\n            logger.warning(f\"Unknown parameters: {unknown_params} for {self.type} API!\")\n        kwargs = self.process_chat_completion_params(kwargs, self.default_config)\n        return await self.chat.completions.create(**kwargs)\n\n    async def responses_create(self, **kwargs) -&gt; Response | AsyncStream[ResponseStreamEvent]:\n        unknown_params = self.check_known_keys(kwargs, self.type_create_params)\n        if unknown_params - {\"messages\"}:  # ignore\n            logger.warning(f\"Unknown parameters: {unknown_params} for {self.type} API!\")\n        assert self.type == \"responses\", \"`responses_create` is not supported for chat.completions API\"\n        kwargs = self.process_responses_params(kwargs, self.default_config)\n        return await self.responses.create(**kwargs)\n\n    def process_chat_completion_params(\n        self, kwargs: OpenAIChatCompletionParams, default_config: OpenAIChatCompletionParams\n    ) -&gt; OpenAIChatCompletionParams:\n        \"\"\"Process chat completion params, convert str to list of messages, merge default config\"\"\"\n        assert \"messages\" in kwargs\n        if isinstance(kwargs[\"messages\"], str):\n            kwargs[\"messages\"] = [{\"role\": \"user\", \"content\": kwargs[\"messages\"]}]\n        return self._merge_default_config(kwargs, default_config)\n\n    def process_responses_params(\n        self, kwargs: OpenAIResponsesParams, default_config: OpenAIResponsesParams\n    ) -&gt; OpenAIResponsesParams:\n        \"\"\"Process responses params, convert str to list of messages, merge default config\"\"\"\n        if \"input\" not in kwargs:\n            # try parse query for chat.completions\n            assert \"messages\" in kwargs\n            input = kwargs.pop(\"messages\")\n            if isinstance(input, str):\n                kwargs[\"input\"] = [{\"role\": \"user\", \"content\": input}]\n            else:\n                kwargs[\"input\"] = input\n        else:\n            if isinstance(kwargs[\"input\"], str):\n                kwargs[\"input\"] = [{\"role\": \"user\", \"content\": kwargs[\"input\"]}]\n        return self._merge_default_config(kwargs, default_config)\n\n    def _merge_default_config(self, kwargs: dict, default_config: dict) -&gt; dict:\n        \"\"\"Merge default config\"\"\"\n        for k, v in default_config.items():\n            if k not in kwargs:\n                kwargs[k] = v\n        return kwargs\n\n    def check_known_keys(self, kwargs: dict, known_keys: set[str]) -&gt; set:\n        \"\"\"Check if all keys in kwargs are in known_keys\"\"\"\n        unknown_keys = set(kwargs.keys()) - known_keys\n        return unknown_keys\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.query_one","title":"query_one  <code>async</code>","text":"<pre><code>query_one(**kwargs) -&gt; str\n</code></pre> <p>Simplified chat.complete / responses API WARNING: Only for basic text i/o usage! You should not use the method with querying with customized configs!</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>async def query_one(self, **kwargs) -&gt; str:\n    \"\"\"Simplified chat.complete / responses API\n    WARNING: Only for basic text i/o usage! You should not use the method with querying with customized configs!\n    \"\"\"\n    if \"stream\" in kwargs:\n        assert kwargs[\"stream\"] is False, \"stream is not supported in `query_one`\"\n\n    if self.type == \"chat.completions\":\n        chat_completion: ChatCompletion = await self.chat_completions_create(**kwargs)\n        return chat_completion.choices[0].message.content\n    elif self.type == \"responses\":\n        response: Response = await self.responses_create(**kwargs)\n        return response.output_text  # NOTE: will not return toolcall or reasoning\n    else:\n        raise ValueError(f\"Unknown type: {self.type}\")\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.process_chat_completion_params","title":"process_chat_completion_params","text":"<pre><code>process_chat_completion_params(\n    kwargs: OpenAIChatCompletionParams,\n    default_config: OpenAIChatCompletionParams,\n) -&gt; OpenAIChatCompletionParams\n</code></pre> <p>Process chat completion params, convert str to list of messages, merge default config</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>def process_chat_completion_params(\n    self, kwargs: OpenAIChatCompletionParams, default_config: OpenAIChatCompletionParams\n) -&gt; OpenAIChatCompletionParams:\n    \"\"\"Process chat completion params, convert str to list of messages, merge default config\"\"\"\n    assert \"messages\" in kwargs\n    if isinstance(kwargs[\"messages\"], str):\n        kwargs[\"messages\"] = [{\"role\": \"user\", \"content\": kwargs[\"messages\"]}]\n    return self._merge_default_config(kwargs, default_config)\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.process_responses_params","title":"process_responses_params","text":"<pre><code>process_responses_params(\n    kwargs: OpenAIResponsesParams,\n    default_config: OpenAIResponsesParams,\n) -&gt; OpenAIResponsesParams\n</code></pre> <p>Process responses params, convert str to list of messages, merge default config</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>def process_responses_params(\n    self, kwargs: OpenAIResponsesParams, default_config: OpenAIResponsesParams\n) -&gt; OpenAIResponsesParams:\n    \"\"\"Process responses params, convert str to list of messages, merge default config\"\"\"\n    if \"input\" not in kwargs:\n        # try parse query for chat.completions\n        assert \"messages\" in kwargs\n        input = kwargs.pop(\"messages\")\n        if isinstance(input, str):\n            kwargs[\"input\"] = [{\"role\": \"user\", \"content\": input}]\n        else:\n            kwargs[\"input\"] = input\n    else:\n        if isinstance(kwargs[\"input\"], str):\n            kwargs[\"input\"] = [{\"role\": \"user\", \"content\": kwargs[\"input\"]}]\n    return self._merge_default_config(kwargs, default_config)\n</code></pre>"},{"location":"ref/utils/openai_utils/#utu.utils.openai_utils.SimplifiedAsyncOpenAI.check_known_keys","title":"check_known_keys","text":"<pre><code>check_known_keys(kwargs: dict, known_keys: set[str]) -&gt; set\n</code></pre> <p>Check if all keys in kwargs are in known_keys</p> Source code in <code>utu/utils/openai_utils/simplified_client.py</code> <pre><code>def check_known_keys(self, kwargs: dict, known_keys: set[str]) -&gt; set:\n    \"\"\"Check if all keys in kwargs are in known_keys\"\"\"\n    unknown_keys = set(kwargs.keys()) - known_keys\n    return unknown_keys\n</code></pre>"}]}